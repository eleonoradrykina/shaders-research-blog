[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.0.3","config-digest","8d1b7566f4d3c391","blog",["Map",9,10,20,21,30,31,40,41,50,51,60,61,70,71,80,81,90,91],"01-06-start",{"id":9,"data":11,"body":16,"filePath":17,"digest":18,"deferredRender":19},{"title":12,"description":13,"pubDate":14,"heroImage":15},"Starting with resources","What we going to read and watch",["Date","2025-01-05T23:00:00.000Z"],"blog-placeholder-3.jpg","import Takeaway from \"../../components/Takeaway.astro\";\n\nFirst things first: before starting this project, I have several links and tabs hanging open in my Google for weeks.\nToday I sorted them into more or less subtopics of my research resources:\n\n| SHADING LANGUAGES    | Links |\n| -------- | ------- |\n| TSL  | [Specification](https://github.com/mrdoob/three.js/wiki/Three.js-Shading-Language)   |\n| TSL | [Medium Article and video from @gianluca.lomarco](https://medium.com/@gianluca.lomarco/three-js-shading-language-a-new-era-for-shaders-cd48de8b22b0)   |\n| WGSL    | [Damien Seguin: comparing WGSL and GLSL](https://dmnsgn.me/blog/from-glsl-to-wgsl-the-future-of-shaders-on-the-web/)   |\n| WGSL    | [WebGPU best Practices pdf from Khronos Group](https://www.khronos.org/assets/uploads/developers/presentations/WebGPU_Best_Practices_Google.pdf)|\n| WGSL    | [Podcast with Gregg Tavares (author of WebGL/WebGPU Fundamentals)](https://changelog.com/jsparty/304)|\n| GLSL    | [All shaders lessons of Bruno Simon](https://threejs-journey.com/lessons/shader-patterns) |\n| GLSL    | [Non-Figurativ \"Entanglement\" piece source code](https://github.com/bgstaal/gpuparticles)|\n\n**Extra:** \n[Introduction to Signed Distance Fields](https://www.youtube.com/watch?v=pEdlZ9W2Xs0)\n\n\n| More on WebGPU   | Links |\n| -------- | ------- |\n| WebGPU Fundamentals    | [Github Repo](https://github.com/webgpu/webgpufundamentals) |\n| WebGPU Fundamentals    | [Webgpufundamentals.org Lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html)|\n| WebGL & WebGPU Meetup - November 2024  | [Youtube video](https://www.youtube.com/watch?v=koY-kDb50VI) |\n\n| Working with Shaders and Materials | Links |\n| -------- | ------- |\n| Nodetoy   | https://nodetoy.co/|\n| Material X    | https://materialx.org/ |\n\n\nI started with reading Medium Article and watching Youtube video from @gianluca.lomarco about what Threejs Shading Language is (finally italian skills come in use!).\nTo plan what I am going to do, I wanted to get a better understanding of what **TSL** is.\nMain takeaways are:\n\u003CTakeaway header=\"Nodes\">\nTSL facilitates shader development by breaking down the shaders into a series of nodes, each applying a specific effect. These nodes can be combined to generate the final shader.\n\u003C/Takeaway>\n\n\u003CTakeaway header=\"WEBGL & WEBGPU\">\nTSL will automatically handle the adaptation for the appropriate API, whether GLSL for WebGL or WGSL for WebGPU.\n\u003C/Takeaway>\n\nNext steps for me will be to get more comofrtable with writing shading langugaes and plan out the roadmap.","src/content/blog/01-06-start.mdx","7619c5767a952c82",true,"01-07-writing-shaders",{"id":20,"data":22,"body":27,"filePath":28,"digest":29,"deferredRender":19},{"title":23,"description":24,"pubDate":25,"heroImage":26},"GLSL tutorials","Remembering how to write shaders",["Date","2025-01-06T23:00:00.000Z"],"shader-1.png","I started with Bruno Simon's tutorials to remember basics of GLSL, and already found out interesting combinations:\n``` javascript\n   //BLINGS MEETING EACH OTHER\n    vec2 rotatedUv = rotate(vUV, PI/4.0, vec2(0.5, 0.5));\n\n    vec2 lightUv = vec2(\n        rotatedUv.x*0.2 + 0.45,\n        rotatedUv.y \n    );\n\n    float strengthX = 0.125 / distance(lightUv, vec2(0.5, 0.5));\n\n    vec2 lightUv2 = vec2(\n        rotatedUv.x,\n        rotatedUv.y*0.01 + 0.35\n    );\n\n     float strengthY = 0.125 / distance(lightUv2, vec2(0.5, 0.5));\n     float strength = strengthX * strengthY;\n\n     //CLAMP STRNGTH\n    strength = clamp (strength, 0.0, 1.0);\n\n    //COLORED\n    vec3 firstColor = vec3(0.0345, 0.07, 0.11);\n    vec3 secondColor = vec3(vUV, 1.0);\n\n    vec3 color = mix(firstColor, secondColor, strength);\n\n\n    gl_FragColor = vec4(vec3(color), 1.0);\n    ```\n\n    This produces gradients that look like light flare like at the cover of this post.","src/content/blog/01-07-writing-shaders.mdx","f9e144d10fd9d0a3","01-08-first-wgsl-shader",{"id":30,"data":32,"body":37,"filePath":38,"digest":39,"deferredRender":19},{"title":33,"description":34,"pubDate":35,"heroImage":36},"My first WGSL shader","Understanding WGSL semantics",["Date","2025-01-07T23:00:00.000Z"],"shader-2.png","#### Meeting\n\nToday started with the first coach meeting. Wouter suggested to start with [WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html#toc), where I accidentally opened a lesson from the middle instead of the beginning.\n\n#### WebGPU Fundamentals && Tour of WGSL\nSeeing snippets of WGSL code I was quite lost, but then I saw [Tour of WGSL](https://google.github.io/tour-of-wgsl/).\nTo understand the context better, I have decided to try out WGSL directly, so I went through the half of the tour and saved some memroy anchors to my figma conspect:\n![WGSL Figma concept](/shaders-research-blog/images/conspect.png)\n\nTo get more comfortable with WGSL, I decided to use a turn a little example from **Book of Shaders into WGSL:**\n```javascript\n@binding(0) @group(0) var\u003Cuniform> frame : u32;\n\n@vertex\nfn vtx_main(@builtin(vertex_index) vertex_index : u32) -> @builtin(position) vec4f {\n  const pos = array(\n    vec2( 0.0,  1.0),\n    vec2( -1.0, -1.0),\n    vec2( 1.0, -1.0),\n  );\n\n  return vec4f(pos[vertex_index], 0, 1);\n}\n//https://thebookofshaders.com/08/\n// YUV to RGB matrix\nconst yuv2rgb = mat3x3f(1.0, 0.0, 1.13983,\n                    1.0, -0.39465, -0.58060,\n                    1.0, 2.03211, 0.0);\n\n@fragment\nfn frag_main() -> @location(0) vec4f {\n  var color : vec3f = yuv2rgb * vec3f(0.5, sin(f32(frame) / 128), 0.5);\n  return vec4(color, 1);\n}\n```\n\n**Multiplying colors by YUV matrix** turned from yellowish-greenish triangle into cyan, although it's not quite clear on the hero Image of the post, we all need to start somewhere=).\nOn my way there I also got a bit distracted with a [reminder how multiplying matrices works](https://mathinsight.org/matrix_vector_multiplication#:~:text=Matrix%2Dvector%20product&text=If%20we%20let%20Ax,na21a22%E2%80%A6), I had a vague memory from high school but wanted to understand the outcome better.\n\nThen I re-read Fundamentals articles again and I feel how it starts to layer in my head.\n\n#### Three.js Shading Language\n\nWhile trying to understand how WebGPU works, I wanted to make sure to understand the **difference between WGSL and TSL.** After scanning documentation and examples, I wasn't quite sure I understood it correctly. \nThe examples on three js website do contain TSL already used with WebGPU, but not shaders written in WGSl. Lately when I need to summarise new things I learned I ask ChatGpt to explain me concepts as an example of pizzeria:\n![TSL-pizza](/shaders-research-blog/images/TSL-pizza.png)\n\n\n#### Planning\nFor tomorrow I am planning to:\n- map out the roadmap;\n- start from [proper beginning of WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-fundamentals.html) again;\n- re-visit [this article about differences between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html), as it brings more clarity to me:\n![conspect-3](/shaders-research-blog/images/conspect-3.png)\nI also saved this comparison\n![conspect-2](/shaders-research-blog/images/conspect-2.png)\n\nBy the way, I just learned that actually its best practice **not to use ternary opeartors, or any flow controls,** when writing GLSL code as it's not good for performance, it might apply to WGSL as well.\n\n\n*My research might seem not linear right now, but I need to get the gist of the context and try out things to better understand the basics.*","src/content/blog/01-08-first-wgsl-shader.mdx","7e3330e00d2b632b","01-09-roadmap",{"id":40,"data":42,"body":47,"filePath":48,"digest":49,"deferredRender":19},{"title":43,"description":44,"pubDate":45,"heroImage":46},"Timeline","Timeplanning",["Date","2025-01-08T23:00:00.000Z"],"timeline.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Roadmap\n\nI tried to come up with a roadmap for my research, but for now I was able only to have very rough timeline:\n![timeline](/shaders-research-blog/images/timeline.png)\n\nI think of my features more of like happy accidents during research, but I will plan them more precisely this week for sure.\nMeanwhile, I also went back to the very beginning of WebGPU Fundamentals.\n\n#### WebGPU Fundamentals\n\nThe first thing that caught my eye was that **WebGPU is an asynchronous API** and is used inside of an **async function**.\nAnother thing was **requesting an adaper**:\n```javascript\nasync function main() {\n  const adapter = await navigator.gpu?.requestAdapter();\n  const device = await adapter?.requestDevice();\n  if (!device) {\n    fail('need a browser that supports WebGPU');\n    return;\n  }\n}\nmain();\n```\nThe adapter represents a specific GPU. Some devices have multiple GPUs.\n\nOkay, now an important takeaway:\n\u003CTakeaway>\n\"Shaders are written in a language called WebGPU Shading Language (WGSL) which is often pronounced wig-sil.\"\nI need to train it more: ***WIG-SIL***\n\u003C/Takeaway>\n\nBy the time I got to a red triangle on canvas, I had this conspect in Figma:\n\n![conspect](/shaders-research-blog/images/WebGPU-setup.png)\n\nNow it makes more sense, so we can move further to **running computations on the GPU.**","src/content/blog/01-09-roadmap.mdx","589fd2ba10444241","01-13-textures",{"id":50,"data":52,"body":57,"filePath":58,"digest":59,"deferredRender":19},{"title":53,"description":54,"pubDate":55,"heroImage":56},"Textures in WebGPU","Slowly going further",["Date","2025-01-12T23:00:00.000Z"],"textures-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Vertex and Storage buffers\nBefore going further to textures, I've went through storage & vertex buffers lessons.\nBuffer data is a new concept for me to use in coding, but it's one of essentials used in WGSL if you want to go further than triangles.\n\nI already used **Uniform Buffers** in previous lesson, now we're looking into **Storage** and **Vertex** buffers.\n\nOne of the main differences of Vertex buffer is how shaders access it. We need to explain to WebGPU what it is and how it's organized:\n\n![tell WebGPU how to supply data ](/shaders-research-blog/images/vertex-data-supply.png)\n\nTo the vertex entry of the pipeline descriptor we added a buffers array which is used to describe how to pull data out of 1 or more vertex buffers. For our first and only buffer, we set an arrayStride in number of bytes. A stride in this case is how many bytes to get from the data for one vertex in the buffer, to the next vertex in the buffer.\n\nAfter several examples I also understood how to use the stride offset when passing attributes to shader:\n```javascript\nbuffers: [\n                {\n                    arrayStride: 5 * 4, // 2 floats, 4 bytes each\n                    attributes: [\n                        { shaderLocation: 0, offset: 0, format: 'float32x2' },  // position\n                        { shaderLocation: 4, offset: 8, format: 'float32x3' },  // perVertexColor\n                    ],\n                },\n                ...\n]\n```\n![stride](/shaders-research-blog/images/stride.png)\n#### Index buffers\nFor optimizing vertices computaion, we can use index buffers to re-use existing vertices:\n![index buffers](/shaders-research-blog/images/index-buffers.png)\n\nThen the code turns into this, creating vertices and also **Index Data**:\n```javascript\nfunction createCircleVertices({\n    radius = 1,\n    numSubdivisions = 24,\n    innerRadius = 0,\n    startAngle = 0,\n    endAngle = Math.PI * 2,\n} = {}) {\n    // 2 vertices at each subdivision, + 1 to wrap around the circle.\n    const numVertices = (numSubdivisions + 1) * 2;\n    // const vertexData = new Float32Array(numSubdivisions * 2 * 3 * 2);\n    const vertexData = new Float32Array(numVertices * (2 + 3));\n\n    let offset = 0;\n    const addVertex = (x, y, r, g, b) => {\n        vertexData[offset++] = x;\n        vertexData[offset++] = y;\n        vertexData[offset++] = r;\n        vertexData[offset++] = g;\n        vertexData[offset++] = b;\n    };\n\n    const innerColor = [0.3, 0.3, 0.9];\n    const outerColor = [0.9, 0.9, 0.9];\n\n    // 2 triangles per subdivision\n    //\n    // 0  2  4  6  8 ...\n    //\n    // 1  3  5  7  9 ...\n    for (let i = 0; i \u003C= numSubdivisions; ++i) {\n        const angle = startAngle + (i + 0) * (endAngle - startAngle) / numSubdivisions;\n\n        const c1 = Math.cos(angle);\n        const s1 = Math.sin(angle);\n\n        addVertex(c1 * radius, s1 * radius, ...outerColor);\n        addVertex(c1 * innerRadius, s1 * innerRadius, ...innerColor);\n    }\n\n    const indexData = new Uint32Array(numSubdivisions * 6);\n    let ndx = 0;\n\n    // 1st tri  2nd tri  3rd tri  4th tri\n    // 0 1 2    2 1 3    2 3 4    4 3 5\n    //\n    // 0--2        2     2--4        4  .....\n    // | /        /|     | /        /|\n    // |/        / |     |/        / |\n    // 1        1--3     3        3--5  .....\n    for (let i = 0; i \u003C numSubdivisions; ++i) {\n        const ndxOffset = i * 2;\n\n        // first triangle\n        indexData[ndx++] = ndxOffset;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 2;\n\n        // second triangle\n        indexData[ndx++] = ndxOffset + 2;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 3;\n    }\n\n    return {\n        vertexData,\n        indexData,\n        numVertices: indexData.length,\n    };\n}\n```\nWe then set Index Buffer as well to pass our index data.\nWe also need to call different draw function:\n```javascript\n        pass.drawIndexed(numVertices, kNumObjects);\n```\nAnd we **saved 33% vertices**, slay!\nThis is what's we're getting by the way:\n![indexBuffer](/shaders-research-blog/images/indexBuffer.png)\n\nNow we're ready to go to **textures**.\n\n#### Textures\n\nOkay, we're finally at the last part of passing data into the WebGPU shaders!\n\nThe major difference for textures is that they are accessed by **sampler**, which can blend up to 16 values together in a texture.\nAs if there was not enough new information before yay....\n\nThe first thing I noticed when started following the lesson is **flipped textures,** which we faced a lot during Bachelor Thesis, and here was the eplanation for that:\n\n![indexBuffer](/shaders-research-blog/images/flipped-textures.png)\n\nIt was also nice finally to see familiar things about UV explanation and mixing, this is something I've encountered before and is easier for comprehension=)\nI think I finally understood meaning of **magFilter** and clamping textures to edge / repeating, which I saw in three js before very briefly:\n![magFilter](/shaders-research-blog/images/magFilter.png)\n\nIt's nice knowing that even if I am not writing shaders low-level daily, I understand how graphics api works in general better!","src/content/blog/01-13-textures.mdx","0cbdb7bcff64b522","01-10-computing-shaders",{"id":60,"data":62,"body":67,"filePath":68,"digest":69,"deferredRender":19},{"title":63,"description":64,"pubDate":65,"heroImage":66},"Computing shaders","Continiuing with WGSL",["Date","2025-01-09T23:00:00.000Z"],"vert-frag.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Computing Shaders\nToday I started with WebGPU Fundamentals again, going further to Computing shaders.\n\nThe first thing that is different from yesterdays **fragment** and **vertex** shaders is that we need **storage variable**:\n```javascript\n      @group(0) @binding(0) var\u003Cstorage, read_write> data: array\u003Cf32>;\n```\nThe thing that still confuses me a bit is *locations* in WGSL shaders:\n\"We tell it weâ€™re going to specify this array on binding location 0 (the binding(0)) in bindGroup 0 (the @group(0)).\"\n\nThings get more interesting when we need to create separate buffers to **store** the data in and to **read** the data from:\n```javascript\n  //data for compute shader\n  const input = new Float32Array([1, 3, 5]);\n\n  //For WebGPU to use it, we need to make a buffer that exists on the GPU and copy the data to the buffer.\n\n  // create a buffer on the GPU to hold our computation\n  // input and output\n  const workBuffer = device.createBuffer({\n    label: 'work buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST,\n  });\n  // Copy our input data to that buffer\n  device.queue.writeBuffer(workBuffer, 0, input);\n\n  // create a buffer on the GPU to get a copy of the results\n  const resultBuffer = device.createBuffer({\n    label: 'result buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST\n  });\n\n  // Setup a bindGroup to tell the shader which\n  // buffer to use for the computation\n  const bindGroup = device.createBindGroup({\n    label: 'bindGroup for work buffer',\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [\n      { binding: 0, resource: { buffer: workBuffer } },\n    ],\n  });\n  ```\n\n  #### WebGPU Inter-stage Variables\n\n  Moving on to WebGPU Inter-stage Variables, in next chapter I learned how to pass structures (of which I think kind of like js classes) between 2 shaders:\n\n  ```javascript\n        struct OurVertexShaderOutput {\n        @builtin(position) position: vec4f,\n        @location(0) color: vec4f,\n      };\n      @vertex fn vs(\n        @builtin(vertex_index) vertexIndex : u32\n     ) -> OurVertexShaderOutput {\n        let pos = array(\n          vec2f( 0.0,  0.5),  // top center\n          vec2f(-0.5, -0.5),  // bottom left\n          vec2f( 0.5, -0.5)   // bottom right\n        );\n\n        let color = array(\n          vec4f(1, 0, 0, 1), // red\n          vec4f(0, 1, 0, 1), // green\n          vec4f(0, 0, 1, 1), // blue\n        );\n \n        var vsOutput: OurVertexShaderOutput;\n        vsOutput.position = vec4f(pos[vertexIndex], 0.0, 1.0);\n        vsOutput.color = color[vertexIndex];\n        return vsOutput;\n      }\n \n       @fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {\n        let red = vec4f(1, 0, 0, 1);\n        let colored = fsInput.color;\n\n        let grid = vec2u(fsInput.position.xy) / 16;\n        let checker = (grid.x + grid.y) % 2 == 1;\n\n        return select(red, colored, checker);\n      }\n  ```\nHere I combined 2 examples to both **pass color from vertex shader to frag shader**, and to add **condition** to use the **passed color or red** in frag shader.\nThis is the outcome:\n![vertex-to-frag](/shaders-research-blog/images/vert-frag.png)\n\n#### Uniforms\nUniforms look just like in GLSL with some differences:\nto create a Uniform, first we need to descripe its' \"class\" aka struct:\n\n```javascript\n    struct OurStruct {\n        color: vec4f,\n        scale: vec2f,\n        offset: vec2f,\n      };\n```\n\nThen, we need to declare a uniform with type of our struct:\n\n```javascript\n@group(0) @binding(0) var\u003Cuniform> ourStruct: OurStruct;\n```\nand after this we can use uniforms in our shader code.\nTo be able to se the from Javascript, we also need to create a buffer first, and to calculate it's size:\n```javascript  \nconst uniformBufferSize =\n    4 * 4 + // color is 4 32bit floats (4bytes each)\n    2 * 4 + // scale is 2 32bit floats (4bytes each)\n    2 * 4;  // offset is 2 32bit floats (4bytes each)\n  const uniformBuffer = device.createBuffer({\n    label: 'uniforms for triangle',\n    size: uniformBufferSize,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n  ```\n  after playing with setting uniforms a bit, I got this beautiful rotating triangle:\n\n![rotating-trinagle](/shaders-research-blog/images/wgsl-uniforms.gif)","src/content/blog/01-10-computing-shaders.mdx","78a51b5f49614566","01-14-planning",{"id":70,"data":72,"body":77,"filePath":78,"digest":79,"deferredRender":19},{"title":73,"description":74,"pubDate":75,"heroImage":76},"Planning","What can I use from what I learnt?",["Date","2025-01-13T23:00:00.000Z"],"Roadmap.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### WebGPU Fundamentals\n\nI am finishing going through textures lessons, including using pictures and videos as textures. I don't think I am going to use loading images on this low level, as I am limited in time, I need to be able to think of the **best usage for shaders**, something you would **not be able** to do on **Three js / React Three fiber** level.\nPer my planning I need to start working on the set up and the scene tomorrow, so there are some things I will go through / revisit while working on my experience.\n\nDuring this passion project I will not look into creating 3d Math on shading languages level, as Three js can do nearly everything much easier.\nSo my plan for WebGPU fundamentals is:\n- Finish **textures** lessons (today)\n- Deeper than fundamental **[Compute shader lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html)**\n- Check **[bind group layouts](https://webgpufundamentals.org/webgpu/lessons/webgpu-bind-group-layouts.html),** I have a feeling I might need them\n- **Particles** lessons â€“ [WebGPU Points](https://webgpufundamentals.org/webgpu/lessons/webgpu-points.html)\n- Revsisting [How WebGPU works](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html) and [difference between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html)\n\n\nWhile doing so, there are things I know I will revisit anyway, such as [Memory layout](https://webgpufundamentals.org/webgpu/lessons/webgpu-memory-layout.html) and [Transparency and blending](https://webgpufundamentals.org/webgpu/lessons/webgpu-transparency.html)\n\n#### Planning usage\n\nEvery day that I read another article from WebGPU fundamentals, I kind of scroll through the whole website one more time to see what became clearer and if new knowledge filled some spots that were blank before=)\nIt's time to make decisions now! Based on what I learnt about WGSL and TSL, I mapped out my *features,* put short *explanation about each context* and specified which *shading language* I am thinking to use for that:\n\n![Roadmap](/shaders-research-blog/images/Roadmap.png)\n\nThe reason why I have **GLSL / WGSL** 3 times is because I am not sure what will be the better soultion at the moment and if I will be able to work with WGSL at the same level as GLSL. I am at least sure to highlight **compute shaders of WGSL** for my **room no. 3,** and to make use of **TSL** as more **textures / materials** for my **room no. 4.** As this whole project is a research proccess, these choices might change based on what I am going to learn on my way.\n\nThen I also added visual references for each location:\n\n**Intro and Corridor**\n![Corridor mood](/shaders-research-blog/images/corridor-mood.png)\n\n**Listening sphere**\n![Wobbly sphere mood](/shaders-research-blog/images/wobbly-sphere-mood.png)\n\n**Room with a conspiracy theory**\n![Conspiracy theory mood](/shaders-research-blog/images/conspiracy-theory-mood.png)\n\n**Distorted memories and outside outro**\n![Outro mood](/shaders-research-blog/images/memories-and-outro-mood.png)\n\n\n#### Back to today's programme\n\nI am finishing with textures, even though I don't think ia m goimng to load videos in WGSL, I went over the article, and the CubeMap article actually helped me to understand how normals abd environment map works.\nI also have better understanding how lighting works together with normals, this is quite helpful picture:\n\n![Cube normals](/shaders-research-blog/images/cube-normals.png)\n\nFollowing the lessons, I have created [several demos in my demo folder in Liminal Russia repo](https://github.com/eleonoradrykina/liminal-russia/tree/demos/demos).\nNow my brain is boiling with theory, and I want to switch to practice with some faster beautiful results:)","src/content/blog/01-14-planning.mdx","f853796bede9cc6b","01-15-gpgpu",{"id":80,"data":82,"body":87,"filePath":88,"digest":89,"deferredRender":19},{"title":83,"description":84,"pubDate":85,"heroImage":86},"GPGPU","Ping-pong shaders",["Date","2025-01-14T23:00:00.000Z"],"ping-pong.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### GPGPU\n\nSo to pick up where I left off, I want to understand how **Frame Buffers** and **Ping-ponging** works to be able:\n- compare it with WGSL compute shaders\n- work on my Memories Room no. 2\n\n\u003CTakeaway>\nSo, we have a texture in our **Frame Buffer Object** which we can use for our vertices position instead of simply rendering it, like we would do normally.\nBecause we **can not read and write** to the same Frame Buffer Object at the same time, we will update it and then exchange them to each other (*ping-ponging*):\n\u003C/Takeaway>\n\n![ping-pong](/shaders-research-blog/images/ping-pong.png)\n\nI feel that the most challenging thing will be to do the setup myself, as Bruno Simon's tutorial provided quite a ready solution with **GPUComputationRenderer**:\n```javascript\n/**\n * GPU Compute\n */\n//Setup\nconst gpgpu = {}\ngpgpu.size = Math.ceil(Math.sqrt(baseGeometry.count))\ngpgpu.computation = new GPUComputationRenderer(gpgpu.size, gpgpu.size, renderer) //square: width , height\n\n//Base particles\nconst baseParticlesTexture = gpgpu.computation.createTexture()\n\nfor (let i = 0; i \u003C baseGeometry.count; i++) {\n    const i3 = i * 3\n    const i4 = i * 4\n\n    // Position based on geometry\n    baseParticlesTexture.image.data[i4 + 0] = baseGeometry.instance.attributes.position.array[i3 + 0]\n    baseParticlesTexture.image.data[i4 + 1] = baseGeometry.instance.attributes.position.array[i3 + 1]\n    baseParticlesTexture.image.data[i4 + 2] = baseGeometry.instance.attributes.position.array[i3 + 2]\n    baseParticlesTexture.image.data[i4 + 3] = Math.random()\n\n}\n\nconsole.log(baseParticlesTexture)\n\n//Particles variable\ngpgpu.particlesVariable = gpgpu.computation.addVariable('uParticles', gpgpuParticlesShader, baseParticlesTexture)\ngpgpu.computation.setVariableDependencies(gpgpu.particlesVariable, [gpgpu.particlesVariable]) //you can pass more dependencies here\n\n// Uniforms\ngpgpu.particlesVariable.material.uniforms.uTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uDeltaTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uBase = new THREE.Uniform(baseParticlesTexture)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldInfluence = new THREE.Uniform(0.5)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldStrength = new THREE.Uniform(2.0)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldFrequency = new THREE.Uniform(0.5)\n\n\n//Init\ngpgpu.computation.init()\n\n```\nI will look into its [code](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js) more closely tomorrow, as Bruno mentioned, it's a good thing that it's [well-documented](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js).\n\nFollowing the tutorial, we saved all vertices for our particles positions in a texture which was update using shader, and then retrieved this texture in our *actual* vertex shader.\nAnother thing for me to keep in mind that I am kind of used to having a lot of built-in things in shaders such as *modelMatrix* and *viewMatrix* and I am not sure you have them in WGSL (will see soon!).\n\n#### Tweaks\n\nOkay, we have a nice result, let's play with it just a little bit for now, and then also get back to it later!\n\nFirst, I used an AI-generated model from [Trellis](https://trellis3d.github.io/), which was extremely cool, I definetely want to try it out for my final experience result. \nThe model is a **bit uneven,** which **does not matter at all** when you use it as a geometry base for **points.**\nAfter some hassle I managed to get the vertex colors inside of my model (apparently it can be hidden in different attributes, not only *color*, but sometimes *color_1* etc)\n\nIt's also nice, because I will be able to add my own lighting in Blender and then bake it.\nThis is the outcome:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-flowfield-overview.mp4\">\u003C/video>\n\nOkay, but because my main vibe is quite depressing, I want this \"memory\" of the building to decay at some point, so the points will be *falling down*.\nAt first I was a bit silly and just put minus on the whole FlowField:\n\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (-1.0)* abs(normalize(flowField));\n\n```\nAs you can see, this makes points fly away down, but also towards negative X:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-wrong.mp4\">\u003C/video>\n\nSo actually we need to make sure that we only have negative Y in the flowfield:\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            (-1.0)*abs(simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time))),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (normalize(flowField));\n```\n\nNow it seems right:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-right.mp4\">\u003C/video>\n\nAt this point I need to stop before I spend too much time tweaking this, and get back to it later when I will be working on the final result!\nNext step will switch to compute shaders abd particles in WGSL.","src/content/blog/01-15-gpgpu.mdx","a9a2000bfdd80432","01-16-compute-20",{"id":90,"data":92,"body":97,"filePath":98,"digest":99,"deferredRender":19},{"title":93,"description":94,"pubDate":95,"heroImage":96},"Compute shaders","More in-depth article about compute shaders in WGSL",["Date","2025-01-15T23:00:00.000Z"],"histogram-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Workgroups\n\nToday I am continuing on Compute Shaders more in depth.\nThe first thing I encountered was **workgroup**:\n\n![workgroup](/shaders-research-blog/images/workgroup.png)\n\nIf we then call pass.dispatchWorkgroups(4, 3, 2) weâ€™re saying, execute a workgroup of 24 threads, 4 * 3 * 2 times (24)\n for a total of 576 threads.\n\n Compute shaders are better fit for GPGPU rather than GLSL with ping-pong buffers, but direct access \n to data buffers and organizing the computation that are used for compute shaders seems more complex at the first sight. (and at my second sight as well)\n\nMy first thought was to compare these **workgroups** to the **textures** that are used for data storage in ping-pong buffers (mostly because of the visual representation), but this is **wrong** comparison.\n\u003CTakeaway>\n    **Workgroups** is a way of organizing parallel computation. Data is stored in buffers, which we bind together and set to the pipeline. \n    **Textures** however, actually store data.\n\u003C/Takeaway>\n\nAnother important concept is **decoupling from the graphics pipeline**\n\u003CTakeaway>\n    WGSL execution model is independent of rendering. Compute shaders can work on **any kind of data**, not just pixels or vertices. Its outputs are stored in **storage buffers** or **textures** on GPU.\n\u003C/Takeaway>\n\nThese textures and storage buffers can be then fed to fragment or vertex shaders.\n\nThe next part of the article was coding the histogram using compute shader:\n![histogram](/shaders-research-blog/images/histogram.png)\n\nHere we loaded the image texture pixel by pixel in compute shader, counted its luminance and then saved into the histogram array, to be able to show it on another canvas:\n```javascript\n  @group(0) @binding(0) var\u003Cstorage, read_write> bins: array\u003Cu32>;\n      @group(0) @binding(1) var ourTexture: texture_2d\u003Cf32>;\n\n      // from: https://www.w3.org/WAI/GL/wiki/Relative_luminance\n      const kSRGBLuminanceFactors = vec3f(0.2126, 0.7152, 0.0722);\n      fn srgbLuminance(color: vec3f) -> f32 {\n        return saturate(dot(color, kSRGBLuminanceFactors));\n      }\n\n      @compute @workgroup_size(1) fn cs() {\n        let size = textureDimensions(ourTexture, 0);\n        let numBins = f32(arrayLength(&bins));\n        let lastBinIndex = u32(numBins - 1);\n        for (var y = 0u; y \u003C size.y; y++) {\n          for (var x = 0u; x \u003C size.x; x++) {\n            let position = vec2u(x, y);\n            let color = textureLoad(ourTexture, position, 0);\n            let v = srgbLuminance(color.rgb);\n            let bin = min(u32(v * numBins), lastBinIndex);\n            bins[bin] += 1;\n          }\n        }\n      }\n```\n\nInteresting thing is following:\n*\"Timing the results I found this is about 30x slower than the JavaScript version!!! ðŸ˜±ðŸ˜±ðŸ˜± (YMMV).\"*\nIt happens because we do not use *the full power* of GPU in this example as we compute this pixels one by one in 1 loop.\nBasically just like CPU, but seven slower as one GPU is slower than CPU.\n\nSo what we can do here is to use more GPU invocations, and this is very example of using workgroups.\nWe changed 1 workgroup per pixel, used position as ``global_invocation_id.xy`` instead of calculating it, and passed the amount of workgroups equal to amount of pixels:\n``pass.dispatchWorkgroups(texture.width, texture.height)``.\n\nThe result was that now histograms are always a bit different every time you re-calculate them...\nThis is actually very interesting because it's a very clear example of **race condition**, and finally I am starting to understand it yay=)\n\u003CTakeaway>\nBecause there are so many calculations happening in parallel, some of them are taking the same value in our bins array and try to write their values into the same position.\nDepending on which invocation \"wins\", the histogram changes all the time.\n\u003C/Takeaway>\n\nTo avoid this, we can use **atomicAdd**.\n\n*NOTE:* Atomic functions have the requirement that they only work on i32 or u32 and they require to data itself to be of type atomic.\n\nWhile previous ``bins[bin] += 1;`` was calling load->add->store while executing, **atomic** function will execute all 3 operations at once (probably like an atom consisting of smaller particles thus the naming?).\nSo I've changed my bins to type atomic and in the shader changed adding function to this:\n``atomicAdd(&bins[bin], 1u);``\n\n*every time it actually works I sigh in relief*\n\nBut now there's another problem â€“â€“ apparently this atomicAdd **blocks** other invocations from adding their data to the bins.\nHere is the image to see this more clear:\n![locking-bin](/shaders-research-blog/images/locking-bin.png)\nThe code works fine, but it could be faster.\nTo achieve that, we need to break up our storage into workgroups' storage. This way we can make our bins to be shared only with invocations in the same workgroup.\nWhen each workgroup has chunks of their own data, we need to write a new shader to collect all this data into one place.\n\nIt then got interestin to the place where the setup changes to 2 pipelines for 2 compute shaders:\n\n```javascript\n  const histogramChunkPipeline = device.createComputePipeline({\n    label: 'histogram',\n    layout: 'auto',\n    compute: {\n      module: histogramChunkModule,\n    },\n  });\n \n  const chunkSumPipeline = device.createComputePipeline({\n    label: 'chunk sum',\n    layout: 'auto',\n    compute: {\n      module: chunkSumModule,\n    },\n  });\n```\n\nThen I have set to BindGroups:\n```javascript\n //BINDING ALL STORAGE BUFFERS AND TEXTURE\n // ONE BINDGROUP PER ONE PASS\n    const histogramBindGroup = device.createBindGroup({\n        label: 'histogram bindGroup',\n        layout: histogramChunkPipeline.getBindGroupLayout(0),\n        entries: [\n            { binding: 0, resource: { buffer: chunksBuffer } },\n            { binding: 1, resource: texture.createView() },\n        ],\n    });\n\n    const sumBindGroups = [];\n    const numSteps = Math.ceil(Math.log2(numChunks));\n    for (let i = 0; i \u003C numSteps; ++i) {\n        const stride = 2 ** i;\n        const uniformBuffer = device.createBuffer({\n            size: 4,\n            usage: GPUBufferUsage.UNIFORM,\n            mappedAtCreation: true,\n        });\n        new Uint32Array(uniformBuffer.getMappedRange()).set([stride]);\n        uniformBuffer.unmap();\n\n        const chunkSumBindGroup = device.createBindGroup({\n            layout: chunkSumPipeline.getBindGroupLayout(0),\n            entries: [\n                { binding: 0, resource: { buffer: chunksBuffer } },\n                { binding: 1, resource: { buffer: uniformBuffer } },\n            ],\n        });\n        sumBindGroups.push(chunkSumBindGroup);\n    }\n```\n\nand then we make 2 passes to the encoder:\n\n```javascript\n    //FIRST PASS\n    pass.setPipeline(histogramChunkPipeline);\n    pass.setBindGroup(0, histogramBindGroup);\n    pass.dispatchWorkgroups(chunksAcross, chunksDown);\n\n    //SECOND PASS\n    pass.setPipeline(chunkSumPipeline);\n    let chunksLeft = numChunks;\n    sumBindGroups.forEach(bindGroup => {\n        pass.setBindGroup(0, bindGroup);\n        const dispatchCount = Math.floor(chunksLeft / 2);\n        chunksLeft -= dispatchCount;\n        pass.dispatchWorkgroups(dispatchCount);\n    });\n    pass.end();\n```\nAnd it works!\n\nNow the speed is the most optimized, here's a table from the article conparing all options we tried:\n![speed-table](/shaders-research-blog/images/speed-table.png)\n\nThe thing is that while I understood pipelines and buffers much better now, there's a new thing with breaking chunks into workgroups and also calculating the optimal subdivisions...\nThere's the last part to this article left, and then I will try to use it in different scenarios.","src/content/blog/01-16-compute-20.mdx","000852924ec954e7"]