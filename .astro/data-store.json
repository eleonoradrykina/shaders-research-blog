[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.0.3","config-digest","8d1b7566f4d3c391","blog",["Map",9,10,20,21,30,31,40,41,50,51,60,61,70,71,80,81,90,91,100,101,110,111,120,121,130,131,140,141,149,150,159,160,169,170,179,180,189,190,199,200,209,210,218,219,228,229,238,239],"01-06-start",{"id":9,"data":11,"body":16,"filePath":17,"digest":18,"deferredRender":19},{"title":12,"description":13,"pubDate":14,"heroImage":15},"Starting with resources","What we going to read and watch",["Date","2025-01-05T23:00:00.000Z"],"blog-placeholder-3.jpg","import Takeaway from \"../../components/Takeaway.astro\";\n\nFirst things first: before starting this project, I have several links and tabs hanging open in my Google for weeks.\nToday I sorted them into more or less subtopics of my research resources:\n\n| SHADING LANGUAGES    | Links |\n| -------- | ------- |\n| TSL  | [Specification](https://github.com/mrdoob/three.js/wiki/Three.js-Shading-Language)   |\n| TSL | [Medium Article and video from @gianluca.lomarco](https://medium.com/@gianluca.lomarco/three-js-shading-language-a-new-era-for-shaders-cd48de8b22b0)   |\n| WGSL    | [Damien Seguin: comparing WGSL and GLSL](https://dmnsgn.me/blog/from-glsl-to-wgsl-the-future-of-shaders-on-the-web/)   |\n| WGSL    | [WebGPU best Practices pdf from Khronos Group](https://www.khronos.org/assets/uploads/developers/presentations/WebGPU_Best_Practices_Google.pdf)|\n| WGSL    | [Podcast with Gregg Tavares (author of WebGL/WebGPU Fundamentals)](https://changelog.com/jsparty/304)|\n| GLSL    | [All shaders lessons of Bruno Simon](https://threejs-journey.com/lessons/shader-patterns) |\n| GLSL    | [Non-Figurativ \"Entanglement\" piece source code](https://github.com/bgstaal/gpuparticles)|\n\n**Extra:** \n[Introduction to Signed Distance Fields](https://www.youtube.com/watch?v=pEdlZ9W2Xs0)\n\n\n| More on WebGPU   | Links |\n| -------- | ------- |\n| WebGPU Fundamentals    | [Github Repo](https://github.com/webgpu/webgpufundamentals) |\n| WebGPU Fundamentals    | [Webgpufundamentals.org Lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html)|\n| WebGL & WebGPU Meetup - November 2024  | [Youtube video](https://www.youtube.com/watch?v=koY-kDb50VI) |\n\n| Working with Shaders and Materials | Links |\n| -------- | ------- |\n| Nodetoy   | https://nodetoy.co/|\n| Material X    | https://materialx.org/ |\n\n\nI started with reading Medium Article and watching Youtube video from @gianluca.lomarco about what Threejs Shading Language is (finally italian skills come in use!).\nTo plan what I am going to do, I wanted to get a better understanding of what **TSL** is.\nMain takeaways are:\n\u003CTakeaway header=\"Nodes\">\nTSL facilitates shader development by breaking down the shaders into a series of nodes, each applying a specific effect. These nodes can be combined to generate the final shader.\n\u003C/Takeaway>\n\n\u003CTakeaway header=\"WEBGL & WEBGPU\">\nTSL will automatically handle the adaptation for the appropriate API, whether GLSL for WebGL or WGSL for WebGPU.\n\u003C/Takeaway>\n\nNext steps for me will be to get more comofrtable with writing shading langugaes and plan out the roadmap.","src/content/blog/01-06-start.mdx","7619c5767a952c82",true,"01-07-writing-shaders",{"id":20,"data":22,"body":27,"filePath":28,"digest":29,"deferredRender":19},{"title":23,"description":24,"pubDate":25,"heroImage":26},"GLSL tutorials","Remembering how to write shaders",["Date","2025-01-06T23:00:00.000Z"],"shader-1.png","I started with Bruno Simon's tutorials to remember basics of GLSL, and already found out interesting combinations:\n``` javascript\n   //BLINGS MEETING EACH OTHER\n    vec2 rotatedUv = rotate(vUV, PI/4.0, vec2(0.5, 0.5));\n\n    vec2 lightUv = vec2(\n        rotatedUv.x*0.2 + 0.45,\n        rotatedUv.y \n    );\n\n    float strengthX = 0.125 / distance(lightUv, vec2(0.5, 0.5));\n\n    vec2 lightUv2 = vec2(\n        rotatedUv.x,\n        rotatedUv.y*0.01 + 0.35\n    );\n\n     float strengthY = 0.125 / distance(lightUv2, vec2(0.5, 0.5));\n     float strength = strengthX * strengthY;\n\n     //CLAMP STRNGTH\n    strength = clamp (strength, 0.0, 1.0);\n\n    //COLORED\n    vec3 firstColor = vec3(0.0345, 0.07, 0.11);\n    vec3 secondColor = vec3(vUV, 1.0);\n\n    vec3 color = mix(firstColor, secondColor, strength);\n\n\n    gl_FragColor = vec4(vec3(color), 1.0);\n    ```\n\n    This produces gradients that look like light flare like at the cover of this post.","src/content/blog/01-07-writing-shaders.mdx","f9e144d10fd9d0a3","01-08-first-wgsl-shader",{"id":30,"data":32,"body":37,"filePath":38,"digest":39,"deferredRender":19},{"title":33,"description":34,"pubDate":35,"heroImage":36},"My first WGSL shader","Understanding WGSL semantics",["Date","2025-01-07T23:00:00.000Z"],"shader-2.png","#### Meeting\n\nToday started with the first coach meeting. Wouter suggested to start with [WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html#toc), where I accidentally opened a lesson from the middle instead of the beginning.\n\n#### WebGPU Fundamentals && Tour of WGSL\nSeeing snippets of WGSL code I was quite lost, but then I saw [Tour of WGSL](https://google.github.io/tour-of-wgsl/).\nTo understand the context better, I have decided to try out WGSL directly, so I went through the half of the tour and saved some memroy anchors to my figma conspect:\n![WGSL Figma concept](/shaders-research-blog/images/conspect.png)\n\nTo get more comfortable with WGSL, I decided to use a turn a little example from **Book of Shaders into WGSL:**\n```javascript\n@binding(0) @group(0) var\u003Cuniform> frame : u32;\n\n@vertex\nfn vtx_main(@builtin(vertex_index) vertex_index : u32) -> @builtin(position) vec4f {\n  const pos = array(\n    vec2( 0.0,  1.0),\n    vec2( -1.0, -1.0),\n    vec2( 1.0, -1.0),\n  );\n\n  return vec4f(pos[vertex_index], 0, 1);\n}\n//https://thebookofshaders.com/08/\n// YUV to RGB matrix\nconst yuv2rgb = mat3x3f(1.0, 0.0, 1.13983,\n                    1.0, -0.39465, -0.58060,\n                    1.0, 2.03211, 0.0);\n\n@fragment\nfn frag_main() -> @location(0) vec4f {\n  var color : vec3f = yuv2rgb * vec3f(0.5, sin(f32(frame) / 128), 0.5);\n  return vec4(color, 1);\n}\n```\n\n**Multiplying colors by YUV matrix** turned from yellowish-greenish triangle into cyan, although it's not quite clear on the hero Image of the post, we all need to start somewhere=).\nOn my way there I also got a bit distracted with a [reminder how multiplying matrices works](https://mathinsight.org/matrix_vector_multiplication#:~:text=Matrix%2Dvector%20product&text=If%20we%20let%20Ax,na21a22%E2%80%A6), I had a vague memory from high school but wanted to understand the outcome better.\n\nThen I re-read Fundamentals articles again and I feel how it starts to layer in my head.\n\n#### Three.js Shading Language\n\nWhile trying to understand how WebGPU works, I wanted to make sure to understand the **difference between WGSL and TSL.** After scanning documentation and examples, I wasn't quite sure I understood it correctly. \nThe examples on three js website do contain TSL already used with WebGPU, but not shaders written in WGSl. Lately when I need to summarise new things I learned I ask ChatGpt to explain me concepts as an example of pizzeria:\n![TSL-pizza](/shaders-research-blog/images/TSL-pizza.png)\n\n\n#### Planning\nFor tomorrow I am planning to:\n- map out the roadmap;\n- start from [proper beginning of WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-fundamentals.html) again;\n- re-visit [this article about differences between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html), as it brings more clarity to me:\n![conspect-3](/shaders-research-blog/images/conspect-3.png)\nI also saved this comparison\n![conspect-2](/shaders-research-blog/images/conspect-2.png)\n\nBy the way, I just learned that actually its best practice **not to use ternary opeartors, or any flow controls,** when writing GLSL code as it's not good for performance, it might apply to WGSL as well.\n\n\n*My research might seem not linear right now, but I need to get the gist of the context and try out things to better understand the basics.*","src/content/blog/01-08-first-wgsl-shader.mdx","7e3330e00d2b632b","01-09-roadmap",{"id":40,"data":42,"body":47,"filePath":48,"digest":49,"deferredRender":19},{"title":43,"description":44,"pubDate":45,"heroImage":46},"Timeline","Timeplanning",["Date","2025-01-08T23:00:00.000Z"],"timeline.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Roadmap\n\nI tried to come up with a roadmap for my research, but for now I was able only to have very rough timeline:\n![timeline](/shaders-research-blog/images/timeline.png)\n\nI think of my features more of like happy accidents during research, but I will plan them more precisely this week for sure.\nMeanwhile, I also went back to the very beginning of WebGPU Fundamentals.\n\n#### WebGPU Fundamentals\n\nThe first thing that caught my eye was that **WebGPU is an asynchronous API** and is used inside of an **async function**.\nAnother thing was **requesting an adaper**:\n```javascript\nasync function main() {\n  const adapter = await navigator.gpu?.requestAdapter();\n  const device = await adapter?.requestDevice();\n  if (!device) {\n    fail('need a browser that supports WebGPU');\n    return;\n  }\n}\nmain();\n```\nThe adapter represents a specific GPU. Some devices have multiple GPUs.\n\nOkay, now an important takeaway:\n\u003CTakeaway>\n\"Shaders are written in a language called WebGPU Shading Language (WGSL) which is often pronounced wig-sil.\"\nI need to train it more: ***WIG-SIL***\n\u003C/Takeaway>\n\nBy the time I got to a red triangle on canvas, I had this conspect in Figma:\n\n![conspect](/shaders-research-blog/images/WebGPU-setup.png)\n\nNow it makes more sense, so we can move further to **running computations on the GPU.**","src/content/blog/01-09-roadmap.mdx","589fd2ba10444241","01-13-textures",{"id":50,"data":52,"body":57,"filePath":58,"digest":59,"deferredRender":19},{"title":53,"description":54,"pubDate":55,"heroImage":56},"Textures in WebGPU","Slowly going further",["Date","2025-01-12T23:00:00.000Z"],"textures-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Vertex and Storage buffers\nBefore going further to textures, I've went through storage & vertex buffers lessons.\nBuffer data is a new concept for me to use in coding, but it's one of essentials used in WGSL if you want to go further than triangles.\n\nI already used **Uniform Buffers** in previous lesson, now we're looking into **Storage** and **Vertex** buffers.\n\nOne of the main differences of Vertex buffer is how shaders access it. We need to explain to WebGPU what it is and how it's organized:\n\n![tell WebGPU how to supply data ](/shaders-research-blog/images/vertex-data-supply.png)\n\nTo the vertex entry of the pipeline descriptor we added a buffers array which is used to describe how to pull data out of 1 or more vertex buffers. For our first and only buffer, we set an arrayStride in number of bytes. A stride in this case is how many bytes to get from the data for one vertex in the buffer, to the next vertex in the buffer.\n\nAfter several examples I also understood how to use the stride offset when passing attributes to shader:\n```javascript\nbuffers: [\n                {\n                    arrayStride: 5 * 4, // 2 floats, 4 bytes each\n                    attributes: [\n                        { shaderLocation: 0, offset: 0, format: 'float32x2' },  // position\n                        { shaderLocation: 4, offset: 8, format: 'float32x3' },  // perVertexColor\n                    ],\n                },\n                ...\n]\n```\n![stride](/shaders-research-blog/images/stride.png)\n#### Index buffers\nFor optimizing vertices computaion, we can use index buffers to re-use existing vertices:\n![index buffers](/shaders-research-blog/images/index-buffers.png)\n\nThen the code turns into this, creating vertices and also **Index Data**:\n```javascript\nfunction createCircleVertices({\n    radius = 1,\n    numSubdivisions = 24,\n    innerRadius = 0,\n    startAngle = 0,\n    endAngle = Math.PI * 2,\n} = {}) {\n    // 2 vertices at each subdivision, + 1 to wrap around the circle.\n    const numVertices = (numSubdivisions + 1) * 2;\n    // const vertexData = new Float32Array(numSubdivisions * 2 * 3 * 2);\n    const vertexData = new Float32Array(numVertices * (2 + 3));\n\n    let offset = 0;\n    const addVertex = (x, y, r, g, b) => {\n        vertexData[offset++] = x;\n        vertexData[offset++] = y;\n        vertexData[offset++] = r;\n        vertexData[offset++] = g;\n        vertexData[offset++] = b;\n    };\n\n    const innerColor = [0.3, 0.3, 0.9];\n    const outerColor = [0.9, 0.9, 0.9];\n\n    // 2 triangles per subdivision\n    //\n    // 0  2  4  6  8 ...\n    //\n    // 1  3  5  7  9 ...\n    for (let i = 0; i \u003C= numSubdivisions; ++i) {\n        const angle = startAngle + (i + 0) * (endAngle - startAngle) / numSubdivisions;\n\n        const c1 = Math.cos(angle);\n        const s1 = Math.sin(angle);\n\n        addVertex(c1 * radius, s1 * radius, ...outerColor);\n        addVertex(c1 * innerRadius, s1 * innerRadius, ...innerColor);\n    }\n\n    const indexData = new Uint32Array(numSubdivisions * 6);\n    let ndx = 0;\n\n    // 1st tri  2nd tri  3rd tri  4th tri\n    // 0 1 2    2 1 3    2 3 4    4 3 5\n    //\n    // 0--2        2     2--4        4  .....\n    // | /        /|     | /        /|\n    // |/        / |     |/        / |\n    // 1        1--3     3        3--5  .....\n    for (let i = 0; i \u003C numSubdivisions; ++i) {\n        const ndxOffset = i * 2;\n\n        // first triangle\n        indexData[ndx++] = ndxOffset;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 2;\n\n        // second triangle\n        indexData[ndx++] = ndxOffset + 2;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 3;\n    }\n\n    return {\n        vertexData,\n        indexData,\n        numVertices: indexData.length,\n    };\n}\n```\nWe then set Index Buffer as well to pass our index data.\nWe also need to call different draw function:\n```javascript\n        pass.drawIndexed(numVertices, kNumObjects);\n```\nAnd we **saved 33% vertices**, slay!\nThis is what's we're getting by the way:\n![indexBuffer](/shaders-research-blog/images/indexBuffer.png)\n\nNow we're ready to go to **textures**.\n\n#### Textures\n\nOkay, we're finally at the last part of passing data into the WebGPU shaders!\n\nThe major difference for textures is that they are accessed by **sampler**, which can blend up to 16 values together in a texture.\nAs if there was not enough new information before yay....\n\nThe first thing I noticed when started following the lesson is **flipped textures,** which we faced a lot during Bachelor Thesis, and here was the eplanation for that:\n\n![indexBuffer](/shaders-research-blog/images/flipped-textures.png)\n\nIt was also nice finally to see familiar things about UV explanation and mixing, this is something I've encountered before and is easier for comprehension=)\nI think I finally understood meaning of **magFilter** and clamping textures to edge / repeating, which I saw in three js before very briefly:\n![magFilter](/shaders-research-blog/images/magFilter.png)\n\nIt's nice knowing that even if I am not writing shaders low-level daily, I understand how graphics api works in general better!","src/content/blog/01-13-textures.mdx","0cbdb7bcff64b522","01-10-computing-shaders",{"id":60,"data":62,"body":67,"filePath":68,"digest":69,"deferredRender":19},{"title":63,"description":64,"pubDate":65,"heroImage":66},"Computing shaders","Continiuing with WGSL",["Date","2025-01-09T23:00:00.000Z"],"vert-frag.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Computing Shaders\nToday I started with WebGPU Fundamentals again, going further to Computing shaders.\n\nThe first thing that is different from yesterdays **fragment** and **vertex** shaders is that we need **storage variable**:\n```javascript\n      @group(0) @binding(0) var\u003Cstorage, read_write> data: array\u003Cf32>;\n```\nThe thing that still confuses me a bit is *locations* in WGSL shaders:\n\"We tell it we’re going to specify this array on binding location 0 (the binding(0)) in bindGroup 0 (the @group(0)).\"\n\nThings get more interesting when we need to create separate buffers to **store** the data in and to **read** the data from:\n```javascript\n  //data for compute shader\n  const input = new Float32Array([1, 3, 5]);\n\n  //For WebGPU to use it, we need to make a buffer that exists on the GPU and copy the data to the buffer.\n\n  // create a buffer on the GPU to hold our computation\n  // input and output\n  const workBuffer = device.createBuffer({\n    label: 'work buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST,\n  });\n  // Copy our input data to that buffer\n  device.queue.writeBuffer(workBuffer, 0, input);\n\n  // create a buffer on the GPU to get a copy of the results\n  const resultBuffer = device.createBuffer({\n    label: 'result buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST\n  });\n\n  // Setup a bindGroup to tell the shader which\n  // buffer to use for the computation\n  const bindGroup = device.createBindGroup({\n    label: 'bindGroup for work buffer',\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [\n      { binding: 0, resource: { buffer: workBuffer } },\n    ],\n  });\n  ```\n\n  #### WebGPU Inter-stage Variables\n\n  Moving on to WebGPU Inter-stage Variables, in next chapter I learned how to pass structures (of which I think kind of like js classes) between 2 shaders:\n\n  ```javascript\n        struct OurVertexShaderOutput {\n        @builtin(position) position: vec4f,\n        @location(0) color: vec4f,\n      };\n      @vertex fn vs(\n        @builtin(vertex_index) vertexIndex : u32\n     ) -> OurVertexShaderOutput {\n        let pos = array(\n          vec2f( 0.0,  0.5),  // top center\n          vec2f(-0.5, -0.5),  // bottom left\n          vec2f( 0.5, -0.5)   // bottom right\n        );\n\n        let color = array(\n          vec4f(1, 0, 0, 1), // red\n          vec4f(0, 1, 0, 1), // green\n          vec4f(0, 0, 1, 1), // blue\n        );\n \n        var vsOutput: OurVertexShaderOutput;\n        vsOutput.position = vec4f(pos[vertexIndex], 0.0, 1.0);\n        vsOutput.color = color[vertexIndex];\n        return vsOutput;\n      }\n \n       @fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {\n        let red = vec4f(1, 0, 0, 1);\n        let colored = fsInput.color;\n\n        let grid = vec2u(fsInput.position.xy) / 16;\n        let checker = (grid.x + grid.y) % 2 == 1;\n\n        return select(red, colored, checker);\n      }\n  ```\nHere I combined 2 examples to both **pass color from vertex shader to frag shader**, and to add **condition** to use the **passed color or red** in frag shader.\nThis is the outcome:\n![vertex-to-frag](/shaders-research-blog/images/vert-frag.png)\n\n#### Uniforms\nUniforms look just like in GLSL with some differences:\nto create a Uniform, first we need to descripe its' \"class\" aka struct:\n\n```javascript\n    struct OurStruct {\n        color: vec4f,\n        scale: vec2f,\n        offset: vec2f,\n      };\n```\n\nThen, we need to declare a uniform with type of our struct:\n\n```javascript\n@group(0) @binding(0) var\u003Cuniform> ourStruct: OurStruct;\n```\nand after this we can use uniforms in our shader code.\nTo be able to se the from Javascript, we also need to create a buffer first, and to calculate it's size:\n```javascript  \nconst uniformBufferSize =\n    4 * 4 + // color is 4 32bit floats (4bytes each)\n    2 * 4 + // scale is 2 32bit floats (4bytes each)\n    2 * 4;  // offset is 2 32bit floats (4bytes each)\n  const uniformBuffer = device.createBuffer({\n    label: 'uniforms for triangle',\n    size: uniformBufferSize,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n  ```\n  after playing with setting uniforms a bit, I got this beautiful rotating triangle:\n\n![rotating-trinagle](/shaders-research-blog/images/wgsl-uniforms.gif)","src/content/blog/01-10-computing-shaders.mdx","78a51b5f49614566","01-14-planning",{"id":70,"data":72,"body":77,"filePath":78,"digest":79,"deferredRender":19},{"title":73,"description":74,"pubDate":75,"heroImage":76},"Planning","What can I use from what I learnt?",["Date","2025-01-13T23:00:00.000Z"],"Roadmap.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### WebGPU Fundamentals\n\nI am finishing going through textures lessons, including using pictures and videos as textures. I don't think I am going to use loading images on this low level, as I am limited in time, I need to be able to think of the **best usage for shaders**, something you would **not be able** to do on **Three js / React Three fiber** level.\nPer my planning I need to start working on the set up and the scene tomorrow, so there are some things I will go through / revisit while working on my experience.\n\nDuring this passion project I will not look into creating 3d Math on shading languages level, as Three js can do nearly everything much easier.\nSo my plan for WebGPU fundamentals is:\n- Finish **textures** lessons (today)\n- Deeper than fundamental **[Compute shader lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html)**\n- Check **[bind group layouts](https://webgpufundamentals.org/webgpu/lessons/webgpu-bind-group-layouts.html),** I have a feeling I might need them\n- **Particles** lessons – [WebGPU Points](https://webgpufundamentals.org/webgpu/lessons/webgpu-points.html)\n- Revsisting [How WebGPU works](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html) and [difference between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html)\n\n\nWhile doing so, there are things I know I will revisit anyway, such as [Memory layout](https://webgpufundamentals.org/webgpu/lessons/webgpu-memory-layout.html) and [Transparency and blending](https://webgpufundamentals.org/webgpu/lessons/webgpu-transparency.html)\n\n#### Planning usage\n\nEvery day that I read another article from WebGPU fundamentals, I kind of scroll through the whole website one more time to see what became clearer and if new knowledge filled some spots that were blank before=)\nIt's time to make decisions now! Based on what I learnt about WGSL and TSL, I mapped out my *features,* put short *explanation about each context* and specified which *shading language* I am thinking to use for that:\n\n![Roadmap](/shaders-research-blog/images/Roadmap.png)\n\nThe reason why I have **GLSL / WGSL** 3 times is because I am not sure what will be the better soultion at the moment and if I will be able to work with WGSL at the same level as GLSL. I am at least sure to highlight **compute shaders of WGSL** for my **room no. 3,** and to make use of **TSL** as more **textures / materials** for my **room no. 4.** As this whole project is a research proccess, these choices might change based on what I am going to learn on my way.\n\nThen I also added visual references for each location:\n\n**Intro and Corridor**\n![Corridor mood](/shaders-research-blog/images/corridor-mood.png)\n\n**Listening sphere**\n![Wobbly sphere mood](/shaders-research-blog/images/wobbly-sphere-mood.png)\n\n**Room with a conspiracy theory**\n![Conspiracy theory mood](/shaders-research-blog/images/conspiracy-theory-mood.png)\n\n**Distorted memories and outside outro**\n![Outro mood](/shaders-research-blog/images/memories-and-outro-mood.png)\n\n\n#### Back to today's programme\n\nI am finishing with textures, even though I don't think ia m goimng to load videos in WGSL, I went over the article, and the CubeMap article actually helped me to understand how normals abd environment map works.\nI also have better understanding how lighting works together with normals, this is quite helpful picture:\n\n![Cube normals](/shaders-research-blog/images/cube-normals.png)\n\nFollowing the lessons, I have created [several demos in my demo folder in Liminal Russia repo](https://github.com/eleonoradrykina/liminal-russia/tree/demos/demos).\nNow my brain is boiling with theory, and I want to switch to practice with some faster beautiful results:)","src/content/blog/01-14-planning.mdx","f853796bede9cc6b","01-15-gpgpu",{"id":80,"data":82,"body":87,"filePath":88,"digest":89,"deferredRender":19},{"title":83,"description":84,"pubDate":85,"heroImage":86},"GPGPU","Ping-pong shaders",["Date","2025-01-14T23:00:00.000Z"],"ping-pong.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### GPGPU\n\nSo to pick up where I left off, I want to understand how **Frame Buffers** and **Ping-ponging** works to be able:\n- compare it with WGSL compute shaders\n- work on my Memories Room no. 2\n\n\u003CTakeaway>\nSo, we have a texture in our **Frame Buffer Object** which we can use for our vertices position instead of simply rendering it, like we would do normally.\nBecause we **can not read and write** to the same Frame Buffer Object at the same time, we will update it and then exchange them to each other (*ping-ponging*):\n\u003C/Takeaway>\n\n![ping-pong](/shaders-research-blog/images/ping-pong.png)\n\nI feel that the most challenging thing will be to do the setup myself, as Bruno Simon's tutorial provided quite a ready solution with **GPUComputationRenderer**:\n```javascript\n/**\n * GPU Compute\n */\n//Setup\nconst gpgpu = {}\ngpgpu.size = Math.ceil(Math.sqrt(baseGeometry.count))\ngpgpu.computation = new GPUComputationRenderer(gpgpu.size, gpgpu.size, renderer) //square: width , height\n\n//Base particles\nconst baseParticlesTexture = gpgpu.computation.createTexture()\n\nfor (let i = 0; i \u003C baseGeometry.count; i++) {\n    const i3 = i * 3\n    const i4 = i * 4\n\n    // Position based on geometry\n    baseParticlesTexture.image.data[i4 + 0] = baseGeometry.instance.attributes.position.array[i3 + 0]\n    baseParticlesTexture.image.data[i4 + 1] = baseGeometry.instance.attributes.position.array[i3 + 1]\n    baseParticlesTexture.image.data[i4 + 2] = baseGeometry.instance.attributes.position.array[i3 + 2]\n    baseParticlesTexture.image.data[i4 + 3] = Math.random()\n\n}\n\nconsole.log(baseParticlesTexture)\n\n//Particles variable\ngpgpu.particlesVariable = gpgpu.computation.addVariable('uParticles', gpgpuParticlesShader, baseParticlesTexture)\ngpgpu.computation.setVariableDependencies(gpgpu.particlesVariable, [gpgpu.particlesVariable]) //you can pass more dependencies here\n\n// Uniforms\ngpgpu.particlesVariable.material.uniforms.uTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uDeltaTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uBase = new THREE.Uniform(baseParticlesTexture)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldInfluence = new THREE.Uniform(0.5)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldStrength = new THREE.Uniform(2.0)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldFrequency = new THREE.Uniform(0.5)\n\n\n//Init\ngpgpu.computation.init()\n\n```\nI will look into its [code](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js) more closely tomorrow, as Bruno mentioned, it's a good thing that it's [well-documented](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js).\n\nFollowing the tutorial, we saved all vertices for our particles positions in a texture which was update using shader, and then retrieved this texture in our *actual* vertex shader.\nAnother thing for me to keep in mind that I am kind of used to having a lot of built-in things in shaders such as *modelMatrix* and *viewMatrix* and I am not sure you have them in WGSL (will see soon!).\n\n#### Tweaks\n\nOkay, we have a nice result, let's play with it just a little bit for now, and then also get back to it later!\n\nFirst, I used an AI-generated model from [Trellis](https://trellis3d.github.io/), which was extremely cool, I definetely want to try it out for my final experience result. \nThe model is a **bit uneven,** which **does not matter at all** when you use it as a geometry base for **points.**\nAfter some hassle I managed to get the vertex colors inside of my model (apparently it can be hidden in different attributes, not only *color*, but sometimes *color_1* etc)\n\nIt's also nice, because I will be able to add my own lighting in Blender and then bake it.\nThis is the outcome:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-flowfield-overview.mp4\">\u003C/video>\n\nOkay, but because my main vibe is quite depressing, I want this \"memory\" of the building to decay at some point, so the points will be *falling down*.\nAt first I was a bit silly and just put minus on the whole FlowField:\n\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (-1.0)* abs(normalize(flowField));\n\n```\nAs you can see, this makes points fly away down, but also towards negative X:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-wrong.mp4\">\u003C/video>\n\nSo actually we need to make sure that we only have negative Y in the flowfield:\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            (-1.0)*abs(simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time))),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (normalize(flowField));\n```\n\nNow it seems right:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-right.mp4\">\u003C/video>\n\nAt this point I need to stop before I spend too much time tweaking this, and get back to it later when I will be working on the final result!\nNext step will switch to compute shaders abd particles in WGSL.","src/content/blog/01-15-gpgpu.mdx","a9a2000bfdd80432","01-16-compute-20",{"id":90,"data":92,"body":97,"filePath":98,"digest":99,"deferredRender":19},{"title":93,"description":94,"pubDate":95,"heroImage":96},"Compute shaders","More in-depth article about compute shaders in WGSL",["Date","2025-01-15T23:00:00.000Z"],"histogram-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Workgroups\n\nToday I am continuing on Compute Shaders more in depth.\nThe first thing I encountered was **workgroup**:\n\n![workgroup](/shaders-research-blog/images/workgroup.png)\n\nIf we then call pass.dispatchWorkgroups(4, 3, 2) we’re saying, execute a workgroup of 24 threads, 4 * 3 * 2 times (24)\n for a total of 576 threads.\n\n Compute shaders are better fit for GPGPU rather than GLSL with ping-pong buffers, but direct access \n to data buffers and organizing the computation that are used for compute shaders seems more complex at the first sight. (and at my second sight as well)\n\nMy first thought was to compare these **workgroups** to the **textures** that are used for data storage in ping-pong buffers (mostly because of the visual representation), but this is **wrong** comparison.\n\u003CTakeaway>\n    **Workgroups** is a way of organizing parallel computation. Data is stored in buffers, which we bind together and set to the pipeline. \n    **Textures** however, actually store data.\n\u003C/Takeaway>\n\nAnother important concept is **decoupling from the graphics pipeline**\n\u003CTakeaway>\n    WGSL execution model is independent of rendering. Compute shaders can work on **any kind of data**, not just pixels or vertices. Its outputs are stored in **storage buffers** or **textures** on GPU.\n\u003C/Takeaway>\n\nThese textures and storage buffers can be then fed to fragment or vertex shaders.\n\nThe next part of the article was coding the histogram using compute shader:\n![histogram](/shaders-research-blog/images/histogram.png)\n\nHere we loaded the image texture pixel by pixel in compute shader, counted its luminance and then saved into the histogram array, to be able to show it on another canvas:\n```javascript\n  @group(0) @binding(0) var\u003Cstorage, read_write> bins: array\u003Cu32>;\n      @group(0) @binding(1) var ourTexture: texture_2d\u003Cf32>;\n\n      // from: https://www.w3.org/WAI/GL/wiki/Relative_luminance\n      const kSRGBLuminanceFactors = vec3f(0.2126, 0.7152, 0.0722);\n      fn srgbLuminance(color: vec3f) -> f32 {\n        return saturate(dot(color, kSRGBLuminanceFactors));\n      }\n\n      @compute @workgroup_size(1) fn cs() {\n        let size = textureDimensions(ourTexture, 0);\n        let numBins = f32(arrayLength(&bins));\n        let lastBinIndex = u32(numBins - 1);\n        for (var y = 0u; y \u003C size.y; y++) {\n          for (var x = 0u; x \u003C size.x; x++) {\n            let position = vec2u(x, y);\n            let color = textureLoad(ourTexture, position, 0);\n            let v = srgbLuminance(color.rgb);\n            let bin = min(u32(v * numBins), lastBinIndex);\n            bins[bin] += 1;\n          }\n        }\n      }\n```\n\nInteresting thing is following:\n*\"Timing the results I found this is about 30x slower than the JavaScript version!!! 😱😱😱 (YMMV).\"*\nIt happens because we do not use *the full power* of GPU in this example as we compute this pixels one by one in 1 loop.\nBasically just like CPU, but seven slower as one GPU is slower than CPU.\n\nSo what we can do here is to use more GPU invocations, and this is very example of using workgroups.\nWe changed 1 workgroup per pixel, used position as ``global_invocation_id.xy`` instead of calculating it, and passed the amount of workgroups equal to amount of pixels:\n``pass.dispatchWorkgroups(texture.width, texture.height)``.\n\nThe result was that now histograms are always a bit different every time you re-calculate them...\nThis is actually very interesting because it's a very clear example of **race condition**, and finally I am starting to understand it yay=)\n\u003CTakeaway>\nBecause there are so many calculations happening in parallel, some of them are taking the same value in our bins array and try to write their values into the same position.\nDepending on which invocation \"wins\", the histogram changes all the time.\n\u003C/Takeaway>\n\nTo avoid this, we can use **atomicAdd**.\n\n*NOTE:* Atomic functions have the requirement that they only work on i32 or u32 and they require to data itself to be of type atomic.\n\nWhile previous ``bins[bin] += 1;`` was calling load->add->store while executing, **atomic** function will execute all 3 operations at once (probably like an atom consisting of smaller particles thus the naming?).\nSo I've changed my bins to type atomic and in the shader changed adding function to this:\n``atomicAdd(&bins[bin], 1u);``\n\n*every time it actually works I sigh in relief*\n\nBut now there's another problem –– apparently this atomicAdd **blocks** other invocations from adding their data to the bins.\nHere is the image to see this more clear:\n![locking-bin](/shaders-research-blog/images/locking-bin.png)\nThe code works fine, but it could be faster.\nTo achieve that, we need to break up our storage into workgroups' storage. This way we can make our bins to be shared only with invocations in the same workgroup.\nWhen each workgroup has chunks of their own data, we need to write a new shader to collect all this data into one place.\n\nIt then got interestin to the place where the setup changes to 2 pipelines for 2 compute shaders:\n\n```javascript\n  const histogramChunkPipeline = device.createComputePipeline({\n    label: 'histogram',\n    layout: 'auto',\n    compute: {\n      module: histogramChunkModule,\n    },\n  });\n \n  const chunkSumPipeline = device.createComputePipeline({\n    label: 'chunk sum',\n    layout: 'auto',\n    compute: {\n      module: chunkSumModule,\n    },\n  });\n```\n\nThen I have set to BindGroups:\n```javascript\n //BINDING ALL STORAGE BUFFERS AND TEXTURE\n // ONE BINDGROUP PER ONE PASS\n    const histogramBindGroup = device.createBindGroup({\n        label: 'histogram bindGroup',\n        layout: histogramChunkPipeline.getBindGroupLayout(0),\n        entries: [\n            { binding: 0, resource: { buffer: chunksBuffer } },\n            { binding: 1, resource: texture.createView() },\n        ],\n    });\n\n    const sumBindGroups = [];\n    const numSteps = Math.ceil(Math.log2(numChunks));\n    for (let i = 0; i \u003C numSteps; ++i) {\n        const stride = 2 ** i;\n        const uniformBuffer = device.createBuffer({\n            size: 4,\n            usage: GPUBufferUsage.UNIFORM,\n            mappedAtCreation: true,\n        });\n        new Uint32Array(uniformBuffer.getMappedRange()).set([stride]);\n        uniformBuffer.unmap();\n\n        const chunkSumBindGroup = device.createBindGroup({\n            layout: chunkSumPipeline.getBindGroupLayout(0),\n            entries: [\n                { binding: 0, resource: { buffer: chunksBuffer } },\n                { binding: 1, resource: { buffer: uniformBuffer } },\n            ],\n        });\n        sumBindGroups.push(chunkSumBindGroup);\n    }\n```\n\nand then we make 2 passes to the encoder:\n\n```javascript\n    //FIRST PASS\n    pass.setPipeline(histogramChunkPipeline);\n    pass.setBindGroup(0, histogramBindGroup);\n    pass.dispatchWorkgroups(chunksAcross, chunksDown);\n\n    //SECOND PASS\n    pass.setPipeline(chunkSumPipeline);\n    let chunksLeft = numChunks;\n    sumBindGroups.forEach(bindGroup => {\n        pass.setBindGroup(0, bindGroup);\n        const dispatchCount = Math.floor(chunksLeft / 2);\n        chunksLeft -= dispatchCount;\n        pass.dispatchWorkgroups(dispatchCount);\n    });\n    pass.end();\n```\nAnd it works!\n\nNow the speed is the most optimized, here's a table from the article conparing all options we tried:\n![speed-table](/shaders-research-blog/images/speed-table.png)\n\nThe thing is that while I understood pipelines and buffers much better now, there's a new thing with breaking chunks into workgroups and also calculating the optimal subdivisions...\nThere's the last part to this article left, and then I will try to use it in different scenarios.","src/content/blog/01-16-compute-20.mdx","000852924ec954e7","01-17-compute-lynch",{"id":100,"data":102,"body":107,"filePath":108,"digest":109,"deferredRender":19},{"title":103,"description":104,"pubDate":105,"heroImage":106},"Compute shaders with David Lynch","Going further with histogram",["Date","2025-01-16T23:00:00.000Z"],"twin-peaks-curtains.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n***Yesterday, 16 January 2025, the world lost a pure genius and a great visionary. David Lynch taught me to love the beauty of abstract and not to look for answers but for the right questions.\nHis art shaped me in so many different ways, and I am pretty sure this project is also partly inspired by his love of the absurd.\nToday's post will be a tribute to David Lynch who is always in my mind.***\n\n\n#### From Compute shader to Frag Shaders\nToday while working on histogram 2.0, we are finally passing the data from compute shaders to rendering shaders.\nThis is what I was waiting for as it's closer to real-life example!\n\nWe calculate the data for histogram in one compute shader and store the data:\n```javascript\nconst bindGroup = device.createBindGroup({\n    layout: drawHistogramPipeline.getBindGroupLayout(0),\n    entries: [\n        { binding: 0, resource: { buffer: chunksBuffer, size: chunkSize * 4 * 4 } },\n        { binding: 1, resource: { buffer: uniformBuffer } },\n        { binding: 2, resource: { buffer: scaleBuffer } },\n    ],\n});\n```\nWe also added one more compute shader to calculate heights of each histogram bar, which I am not going to put here otherwise it's too much code.\nThe most important part that we also get **scale** from there. \nAnd then we pass everyting into the Fragment shader:\n```javascript\n    const bindGroup = device.createBindGroup({\n    layout: drawHistogramPipeline.getBindGroupLayout(0),\n    entries: [\n        { binding: 0, resource: { buffer: chunksBuffer, size: chunkSize * 4 * 4 } },\n        { binding: 1, resource: { buffer: uniformBuffer } },\n        { binding: 2, resource: { buffer: scaleBuffer } },\n    ],\n});\n```\nThe outcome is the same histograms, but  drawn in GPU instead of javascript:\n![laura-dern-histogram](/shaders-research-blog/images/inland-empire-laura-histogram.png)\n*Laura Dern in Inland Empire, 2006*\n\nTo sum up: we have **3 compute pipelines** here: \n- `histogramChunkPipeline` where we calculate histogram chunk by chunk;\n- `chunkSumPipeline` where we gather all the histogram chunks from workroups into one chunk;\n- `scalePipeline` where we compute the height of each bar of histogram;\n\nand **1 render pipeline**\n- `drawHistogramPipeline`, where we read histogram data chunks and scale data and based on that render the histogram.\n\nNow can we do that on the same canvas on each frame?\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/twinpeaks-mirror-histogram.mp4\">\u003C/video>\n*Dale Cooper becomes possesed by Bob. Ending of Twin Peaks, Season 2, 1991*\n\nHere we call our render function on each frame using well-known `requestAnimationFrame(render)`.\nBindgroups are also now created on each frame inside of render function.\n\nAn inportant moment is that we also created a **separate render pipeline** to render a video:\n```javascript\n    const videoPipeline = device.createRenderPipeline({\n        label: 'hardcoded video textured quad pipeline',\n        layout: 'auto',\n        vertex: {\n            module: videoModule,\n        },\n        fragment: {\n            module: videoModule,\n            targets: [{ format: presentationFormat }],\n        },\n    });\n\n    const videoSampler = device.createSampler({\n        magFilter: 'linear',\n        minFilter: 'linear',\n    });\n    ```\nAnd this is the end of WebGPU Fundamentals compute shaders articles!\nNow lets try to tweak the given example ourselves.\n\nAt first, I got rid of histogram and tried to make half of the video black and white:\n![black and white](/shaders-research-blog/images/bw-twin-peaks.png)\n\nYes, I know, we don't need compute shader for that and this can be done in Frag shader, but we're getting there.\n\nNext, I changed the chunkSizes and the asked to make only red-ish pixels to become black and white (which in this case worked as blue segmentation).\nThis is my new compute shader:\n```javascript\nconst k = {\n        chunkWidth: 16,\n        chunkHeight: 16,\n    };\n    const chunkSize = k.chunkWidth * k.chunkHeight;\n    const sharedConstants = Object.entries(k).map(([k, v]) => `const ${k} = ${v};`).join('\\n');\n\n    const bwChunkModule = device.createShaderModule({\n        label: 'bw shader',\n        code: `\n        ${sharedConstants}\n\n        struct Chunk {\n            pixels: array\u003Cvec4f, ${chunkSize}>\n        };\n\n        @group(0) @binding(0) var ourTexture: texture_external;\n        @group(0) @binding(1) var outputTexture: texture_storage_2d\u003Crgba8unorm, write>;\n        @group(0) @binding(2) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n\n        const kSRGBLuminanceFactors = vec3f(0.2126, 0.7152, 0.0722);\n        fn srgbLuminance(color: vec3f) -> f32 {\n            return saturate(dot(color, kSRGBLuminanceFactors));\n        }\n\n        @compute @workgroup_size(chunkWidth, chunkHeight, 1)\n        fn cs(\n            @builtin(global_invocation_id) global_id: vec3u,\n            @builtin(workgroup_id) workgroup_id: vec3u,\n            @builtin(local_invocation_id) local_id: vec3u,\n        ) {\n            let size = textureDimensions(ourTexture);\n            let position = global_id.xy;\n\n            if (all(position \u003C size)) {\n            var color = textureLoad(ourTexture, position);\n\n        // Check if the pixel is in the left half of the video\n        // if (position.x \u003C size.x / 2u) {\n            //check if the pixel is red\n            if (color.r > color.g && color.r > color.b) {\n                let luminance = srgbLuminance(color.rgb);\n                color = vec4f(luminance, luminance, luminance, color.a);\n            }\n        //}\n\n        // Store in chunk\n        let chunk_idx = workgroup_id.y * (size.x / chunkWidth) + workgroup_id.x;\n        let pixel_idx = local_id.y * chunkWidth + local_id.x;\n        chunks[chunk_idx].pixels[pixel_idx] = color;\n\n\n        // Write to output texture\n            textureStore(outputTexture, position, color);\n        }\n         }\n        `,\n    });\n\n```\nand this is the outcome:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/blue-segmentation.mp4\">\u003C/video>\n\nOkay, but what's the point of compute shaders if we don't store anything between the frames?\nAs per Wouter's consult I will be looking into post-processing, it's a nice moment to creat a little post-processing myself.\n\nI think our Twin Peaks finale would benefit from a little **motion blur.** This is purely to get better understanding how wgsl compute shaders work, so I am not even googling how it actually works,\nmy guess is to store previous frames and blend them with the current one.\n\nI have started with storing 4 frames and did not see any result.\n\nMaybe we need to store more?\n\n```javascript\n struct Chunk {\n            frame0: array\u003Cvec4f, ${chunkSize}>,  // Most recent previous frame\n            frame1: array\u003Cvec4f, ${chunkSize}>,\n            frame2: array\u003Cvec4f, ${chunkSize}>,\n            frame3: array\u003Cvec4f, ${chunkSize}>,\n            frame4: array\u003Cvec4f, ${chunkSize}>,\n            frame5: array\u003Cvec4f, ${chunkSize}>,\n            frame6: array\u003Cvec4f, ${chunkSize}>,\n            frame7: array\u003Cvec4f, ${chunkSize}>,\n            frame8: array\u003Cvec4f, ${chunkSize}>,\n            frame9: array\u003Cvec4f, ${chunkSize}>   // Oldest frame\n        };\n```\n\nAt this point I got an error that my buffer is a little bit too big:\nBuffer size (279183360) exceeds the max buffer size limit (268435456).\n\nSo it means you probably can store around 9 pictures in one buffer which is quite a lot!\n\nThen I realized that the problem is not how many frames we take, but the **difference** between them.\nSo I want to check which frame is that and save every n-th frame:\n\n```javascript\n\n    const motionBlurModule = device.createShaderModule({\n        label: 'motion blur shader',\n        code: `\n        ${sharedConstants}\n\n        struct Settings {\n            frameInterval: u32,\n            currentFrameWeight: f32,\n            frame0Weight: f32,\n            frame1Weight: f32,\n            frame2Weight: f32,\n            frame3Weight: f32,\n        };\n\n        struct Chunk {\n            frame0: array\u003Cvec4f, ${chunkSize}>,  // Most recent saved frame\n            frame1: array\u003Cvec4f, ${chunkSize}>,\n            frame2: array\u003Cvec4f, ${chunkSize}>,\n            frame3: array\u003Cvec4f, ${chunkSize}>   // Oldest saved frame\n        };\n\n        @group(0) @binding(0) var ourTexture: texture_external;\n        @group(0) @binding(1) var outputTexture: texture_storage_2d\u003Crgba8unorm, write>;\n        @group(0) @binding(2) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n        @group(0) @binding(3) var\u003Cstorage, read_write> frameCounter: array\u003Cu32, 1>;\n        @group(0) @binding(4) var\u003Cuniform> settings: Settings;\n\n        @compute @workgroup_size(chunkWidth, chunkHeight, 1)\n        fn cs(\n            @builtin(global_invocation_id) global_id: vec3u,\n            @builtin(workgroup_id) workgroup_id: vec3u,\n            @builtin(local_invocation_id) local_id: vec3u,\n        ) {\n            let size = textureDimensions(ourTexture);\n            let position = global_id.xy;\n\n            if (all(position \u003C size)) {\n                let chunk_idx = workgroup_id.y * (size.x / chunkWidth) + workgroup_id.x;\n                let pixel_idx = local_id.y * chunkWidth + local_id.x;\n                \n                let currentColor = textureLoad(ourTexture, position);\n                \n                // Update frame counter with single thread\n                if (global_id.x == 0u && global_id.y == 0u) {\n                    frameCounter[0] = frameCounter[0] + 1u;\n                }\n\n                // Store frames every N frames for all pixels\n                if (frameCounter[0] % settings.frameInterval == 0u) {\n                    let prev0 = chunks[chunk_idx].frame0[pixel_idx];\n                    let prev1 = chunks[chunk_idx].frame1[pixel_idx];\n                    let prev2 = chunks[chunk_idx].frame2[pixel_idx];\n\n                    chunks[chunk_idx].frame3[pixel_idx] = prev2;\n                    chunks[chunk_idx].frame2[pixel_idx] = prev1;\n                    chunks[chunk_idx].frame1[pixel_idx] = prev0;\n                    chunks[chunk_idx].frame0[pixel_idx] = currentColor;\n                }\n                \n                let prev0 = chunks[chunk_idx].frame0[pixel_idx];\n                let prev1 = chunks[chunk_idx].frame1[pixel_idx];\n                let prev2 = chunks[chunk_idx].frame2[pixel_idx];\n                let prev3 = chunks[chunk_idx].frame3[pixel_idx];\n\n                let blendedColor = currentColor * settings.currentFrameWeight +\n                                  prev0 * settings.frame0Weight +\n                                  prev1 * settings.frame1Weight +\n                                  prev2 * settings.frame2Weight +\n                                  prev3 * settings.frame3Weight;\n\n                textureStore(outputTexture, position, blendedColor);\n            }\n        }\n        `,\n    });\n```\n\nI also added gui to be able to change the step between frames.\nOn the maximum threshold around 120 frames we get this beautiful trippy not-motion-blur:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/motion-blur.mp4\">\u003C/video>\n\nI am quite happy with the result! Next I will try to use WGSL in more complex setups and finally start working on the setting up the scene for the experience.","src/content/blog/01-17-compute-lynch.mdx","e842fb24cfe7d992","01-18-particles-wgsl",{"id":110,"data":112,"body":117,"filePath":118,"digest":119,"deferredRender":19},{"title":113,"description":114,"pubDate":115,"heroImage":116},"WGSL Particles","going towards particle system",["Date","2025-01-17T23:00:00.000Z"],"particles-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### WebGPU Points\n\nMy favorite thing about vertex and compute shaders is to create **particles systems that look alive.**\nFor my \"Conspiracy Theory\" room I want to have a mysterious particles system that will look like it has hidden answers to all the questions. It has its roots in \"kitchen talks\" in USSR era, where the only place where people could freely express their opinions were their kitchens.\nIt later has morphed into drunk kitchen talks \"for grown-ups\" for my generation, during which some of participants were claiming they have full understanding and explanation to the current political situation. Most of the times it was United States trying to sabotage the prosperity of Russians and hypnotize them into zombies.\n\nBut enough of history, let's code!\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-sphere.mp4\">\u003C/video>\nI followed particles article quite close, but added a twist with using **alha-map ![alpha-map](/shaders-research-blog/images/texture-1.png) in Frag shader** to make my particles fancier:\n\n\n```javascript\n        @fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {\n            // use texture as transparency mask\n            let mask = textureSample(t, s, vsOut.texcoord);\n            // Pre-multiply the RGB color by the alpha value\n            let alpha = mask.r * 1.0;  // here it can be even more transparent if needed\n            return vec4f(vec3f(0.4, 0.4, 0.9) * alpha, alpha);\n        }\n```\n\nCan we make things more interesting with a compute shader?\n\nAfter 1,5 hours of adding compute shader **just to achieve the same sphere**, I got my sphere a bit creased 😂😂:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-creased.mp4\">\u003C/video>\n\nI think it might has to do with messing up with my positions array indexation.\nAha! I was creating **vec3** for my initial particles, but then I went a little bit too fast and thinking about **life span** was treating them as vec4.\nFor now I have updated the function that creates sphere geometry to return **vec4.**\n\nSo now we have totally the same sphere (I've changed the color so it's more pleasant to look at), but with retrieving positions from compute shader:\n![same sphere but comoute shader](/shaders-research-blog/images/compute-sphere.png)\n\n\u003CTakeaway>\nNow instead of passing initial positios array, we can play with poistions first and pass them into the vertex.\nI keep track of distance (not sure if it's a good solution right now), and when particles are too different from initial, I reset them.\n\u003C/Takeaway>\n\nHere's code of my new compute shader:\n\n```javascript\n        ${sharedConstants}\n        struct Particle {\n            pos: vec4f,    // xyz for position, w for lifetime/age (not used yet)\n        }\n\n        struct Chunk {\n            particles: array\u003CParticle, 1000>,      // Current state\n            prevParticles: array\u003CParticle, 1000>,  // Previous state\n            initialPos: array\u003CParticle, 1000>,     // Initial/reset positions\n        };\n\n        struct Uniforms {\n            matrix: mat4x4f,\n            resolution: vec2f,\n            size: f32,\n            time: f32,\n        };\n\n        @group(0) @binding(0) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n        @group(0) @binding(1) var\u003Cuniform> uni: Uniforms;\n\n        @compute @workgroup_size(64)\n        fn cs(@builtin(global_invocation_id) id: vec3u) {\n            if (id.x >= 1000) {\n                return;\n            }\n\n            let i = id.x;\n            let initialPos = chunks[0].initialPos[i].pos.xyz;\n            let distance = length(chunks[0].particles[i].pos.xyz - initialPos);\n\n            if (distance \u003C 5.0) {\n                var newPos = chunks[0].particles[i].pos;\n                // Using time to create some distortion\n                newPos.x += sin(uni.time + f32(i) * 0.01) * 0.0001;\n                newPos.y += cos(uni.time * 0.5 + f32(i) * 0.01) * 0.0001;\n                chunks[0].particles[i].pos = newPos;\n            } else {\n                chunks[0].particles[i].pos = vec4f(initialPos, 1.0);\n            }\n\n            chunks[0].prevParticles[i] = chunks[0].particles[i];\n        }\n```\n\nThis creates this beautiful swirl:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/compute-particles.mp4\">\u003C/video>\n\nNext step will be to tune persistance and to get better control over understanding of vrtices positions, but basic setup is quite done!","src/content/blog/01-18-particles-wgsl.mdx","27b8b782eeb90c90","01-19-resources-update",{"id":120,"data":122,"body":127,"filePath":128,"digest":129,"deferredRender":19},{"title":123,"description":124,"pubDate":125,"heroImage":126},"Resources Update","dropping more interesting links here",["Date","2025-01-18T23:00:00.000Z"],"okay-dev.jpeg","On Sunday I combed through Twitter and fished out some links that *seem useful* at the first glance.\nI have not read them closely yet, only scanned through but I just want to dump everything I found here, just in case:\n\n| SL / API    | Links |\n| -------- | ------- |\n| WGSL  / WebGPU  | [Okay Dev article about WebGPU](https://okaydev.co/articles/dive-into-webgpu-part-1)   |\n| WGSL  / WebGPU  | [React Three Fiber WebGPU Post Processing](https://github.com/ektogamat/r3f-webgpu-starter) (probably just a set up with no WGSl involved)|\n| WebGPU   | [ Interacting with meshes in WebGPU ](https://codesandbox.io/p/sandbox/patient-lake-9sh577?file=%2Fsrc%2FApp.js)|\n| TSL    | [ Editor that generates TSL code](https://tsl-editor.vercel.app/) |\n| TSL  | [Youtube tutorial of Yuri Artiukh](https://www.youtube.com/live/Wgz3u-6tg28?si=cQtVlmS1eQDNip5B) (I actually follow him nearly everywhere but did not get a chance to use his tutorials)  |\n| TSL  | [ 2D Clouds in TSL]( https://niklever.com/get-to-grips-with-threejs-shading-language-tsl-part-13-2d-clouds/)  |\n\n\n*I noticed that Okay Dev use Megazoid font that I used for my NailBro project during first year, so I already trust them.*","src/content/blog/01-19-resources-update.mdx","edcb032b858fc4ea","01-20-building-up-scene",{"id":130,"data":132,"body":137,"filePath":138,"digest":139,"deferredRender":19},{"title":133,"description":134,"pubDate":135,"heroImage":136},"Building Up the Scene","Its time to build!",["Date","2025-01-19T23:00:00.000Z"],"corridor.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nOkay, it's time to set up the scene! Enough shaders for now, let's think ***spatially.***\nMy main concern was how to let user move inside of my corridor kid of freely, without coding camera path?\nBack then I imagined it as BackRooms where you experience everything from first person POV, so I was in terms of camera movement only:\n\n![Backrooms_model](/shaders-research-blog/images/Backrooms_model.jpg)\n\nThen I saw [this tutorial](https://threejs-journey.com/lessons/create-a-game-with-r3f#) of Bruno Simon where he had physics and an actual player:\n![Bruno Simon's game](/shaders-research-blog/images/bruno-simon-game.png)\n\n\u003CTakeaway>\nNot only it makes much more sense to put **physics colliders onto walls and floor** but also having an **actual player** helps to kind of \"see yourself\" in the environment.\nI think I was a little bit too fixated on that reference, anyway it is an inspiration and I am not building exact same copy, so why not have some fun?\n\u003C/Takeaway>\n\nI have changed the \"player\" to the capsule so fun and it's kind of entertaining how it's being pushed and thrown into sides, I was thiking maybe later I will add a model of some can instead of it.\n\nAlso I want to create a feeling of *impossible geometry*, so I'm going to move my player to another spot unnoticeably (hopefully) so that they just continue navigating thinking that they are in an infinite loop.\nAt first I drew a simple loop on paper:\n\n![corridor-sketch](/shaders-research-blog/images/corridor-sketch.jpg)\n\nSo after following Three.js Journey tutorial I was excited to start: I've put my own, longer corridor with the ceiling and basic colors for now and this is how it looks:\n![First iteration](/shaders-research-blog/images/corridor.png)\n\nI switch between normal OrbitControls to get the whole overview while building the \"architecture\" of the scene, and player POV to see how it feels.\nOf course, there are a lot of things that would be tweaked in future like textures and light, but I already see one big difference...\n\nFor some reason I always refered to this part of scene as \"corridor\" and imagined a corridor in my mind, but what does actually create this uncanny atmosphere in Backrooms?\nLet's see the reference again:\n![Backrooms_model](/shaders-research-blog/images/Backrooms_model.jpg)\n![Severance](/shaders-research-blog/images/Severance.webp)\n\nIt's not **just a hallway**, it's a structure that is off to a human proportion, big, empty and repetitive. If you see the corridor that just goes straight and then maybe turns, how scary it can be?\nOn the other hand, the more empty space you **don't see**, the more possibility there's something evil there.\n\nSo let's add more space and re-think the architecture:\n![architecture-iteration-2l](/shaders-research-blog/images/architecture-iteration-2.png)\n*Debug version with no ceiling and no player*\n\nNoow we're getting there!\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/gameplay-1.mp4\">\u003C/video>\n*Player POV*\n\nFrom here I also might try to make the geometry more complicated.\nNext step will be to think of **360 camera rotation**, adding **lights** and **textures**!","src/content/blog/01-20-building-up-scene.mdx","16db4c7b954addf7","01-21-backrooms",{"id":140,"data":142,"body":146,"filePath":147,"digest":148,"deferredRender":19},{"title":133,"description":143,"pubDate":144,"heroImage":145},"Working on the experience environment",["Date","2025-01-20T23:00:00.000Z"],"floor-plan-react.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Cleaning up the code\nFirst thing was to make my code a bit more clear and sustainable after all manual tweaks.\n\nI re-drew the plan and separated the walls into groups for convenience:\n![new floor plan](/shaders-research-blog/images/floor-plan-2.jpg)\n\nHere how it looks in debug mode: \n![new floor plan react](/shaders-research-blog/images/floor-plan-react.png)\n\n```javascript\n\n    export default function Corridor() {\n\n    return \u003C>\n        \u003CFloor size={ 16 }/>\n        {/* GROUP A*/}\n        \u003CWallHorizontal position={ [-9.7, 0.75, -4] } length = {12} />\n        \u003CWallVertical position={ [-3.85, 0.75, -8.0] }length = {8}/>\n\n        {/* GROUP B*/}\n        \u003CWallHorizontal position={ [-7.0, 0.75, -20] }length = {10 } />\n        \u003CWallVertical position={ [-6.6, 0.75, -25.0] }length = {9.75}/>\n\n        {/* GROUP C*/}\n        \u003CWallHorizontal position={ [12.7, 0.75, -22] } length = {6}/>\n        \u003CWallVertical position={ [6.6, 0.75, -20.65] }length = {13}/>\n        \u003CWallHorizontal position={ [8.7, 0.75, -14] } length = {14}/>\n\n        {/* GROUP D*/}\n        \u003CWallVertical position={ [6.6, 0.75, -2.15] }length = {8}/>\n\n        \u003CBoundsForward length={ 16} />\n        {/* \u003CCeiling size={ 16 }/> */}\n    \u003C/>\n}\n```\n\n#### Camera movement \nMy next question is how I move the camera?\nI am using Rapier with R3F for moving my capsule around. Physics engine and ``\u003CRigidBody/>`` prevents capsule from going into the walls so I don't have to think about coding the bounds myself.\nRight now the camera always follows the capsule and looks at it. This creates feeling of looking only straight ahead.\nBut my experience is very spatial, so we need to let the user to be able to look around.\nThis is the code for the camera I started with:\n```javascript\n...\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n        const cameraPosition = new THREE.Vector3()\n        cameraPosition.copy(bodyPosition)\n        cameraPosition.z += 2.25\n        cameraPosition.y += 0.65\n\n        const cameraTarget = new THREE.Vector3()\n        cameraTarget.copy(bodyPosition)\n        cameraTarget.y += 0.25 \n        cameraTarget.z -= 2.25\n\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n\n        state.camera.position.copy(smoothedCameraPosition)\n        state.camera.lookAt(smoothedCameraTarget)\n```\n\nAt first, I added looking to the sides while going sideways, adding CameraRotation together with the impulse:\n```javascript\n        /**\n         * Controls\n         */\n        const { forward, backward, left, right, jump } = getKeys()\n\n        const impulse = { x: 0, y: 0, z: 0 }\n        const cameraRotation = { x: 0, y: 0, z: 0 }\n\n\n        const impulseStrength = 0.1 * delta\n\n        if (forward) {\n                impulse.z -= impulseStrength\n            }\n        if (backward) {\n                impulse.z += impulseStrength\n            }\n        if (left) {\n                impulse.x -= impulseStrength\n                cameraRotation.x -= 1.5\n            }\n        if (right) {\n                impulse.x += impulseStrength\n                cameraRotation.x += 1.5\n            }\n            \n        body.current.applyImpulse(impulse)\n\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n        const cameraPosition = new THREE.Vector3()\n        cameraPosition.copy(bodyPosition)\n        cameraPosition.z += 2.25\n        cameraPosition.y += 0.65\n\n        const cameraTarget = new THREE.Vector3()\n        cameraTarget.copy(bodyPosition)\n        cameraTarget.y += 0.25 \n        cameraTarget.z -= 2.25\n        cameraTarget.x += cameraRotation.x\n\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n```\nBut this gave very limited field of view which also did not work smoothly with the movement.\n\nThen, I thought what if detach LookAt from the capsule for a second? But then the position is still updating each frame causing this camera shaking:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/camera-LAT-bug.mp4\">\u003C/video>\n\nThen I realised: we need to attach the camera to the capsule **only when we're moving** the capusle, letting user to look around when we're standing at the same place.\n\nSo I've added updating camera on Each frame only if we're moving:\n```javascript\n        if (forward || backward || left || right || jump) {\n            state.camera.position.copy(smoothedCameraPosition)\n            state.camera.lookAt(smoothedCameraTarget)\n        }\n```\n\nAnd set the bounds on the rbitControls because it is now enabled when the user does not move:\n```javascript\n        \u003COrbitControls \n            enablePan={ false }\n            enableZoom={ false }\n            maxPolarAngle={ Math.PI / 2 }\n            minPolarAngle={ Math.PI / 2 }\n            maxAzimuthAngle={ Math.PI / 2 }\n            minAzimuthAngle={ -Math.PI / 2 }\n            makeDefault \n        />\n```\nI also tweaked the inital camera position so that camera starts from the correct place. \nIt looks extremely weird, but I think it's closer to what I want:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/camera-movement-bug.mp4\">\u003C/video>\n\nThe problem is that my camera does not \"remember\" where we were before we stopped moving. I tried to fix that but I think I am missing something here.\nI compared states of camera while moving and after, but they look the same:\n\n![cameras comnparison](/shaders-research-blog/images/camera-state-comparison-1.png)\n![cameras comnparison](/shaders-research-blog/images/camera-state-comparison-2.png)\nI will think about it more later, for now I switched back to \"straight\" camera","src/content/blog/01-21-backrooms.mdx","81885dc104edffc3","01-22-wallpaper",{"id":149,"data":151,"body":156,"filePath":157,"digest":158,"deferredRender":19},{"title":152,"description":153,"pubDate":154,"heroImage":155},"Creating wallpapers with shaders"," ",["Date","2025-01-21T23:00:00.000Z"],"hero-22jan.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nAt first, I made sure that shader works and passed UV coordinates through it:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-1.png)\n\nThen I mixed it with our walls color, again, just to check that it works:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-2.png)\n\nNice! now we can write shader code.\nTo create wallpaper, we will need to divide our walls into grid and draw stripes between cells.\nFlowers are also easy as it's just sinusoid shape.\nThis what I got:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-3.png)\n\nAfter adding flowers, I wanted to create a pattern with a heart shape with flowers around it. \nSo we need to check what cell we're in and draw a shape depending on that:\n```javascript\n    // Place flowers every 2 cells\n    if (mod(cellId.x, 2.0) == 0.0 && mod(cellId.y, 2.0) == 0.0) {\n        vec2 center = (cellId + vec2(0.6)) / gridSize;\n        strength += flower(center, 0.0006, aspectCorrectedUV);\n    }\n\n    // Place shape every 4 cells with a 1 cell offset\n    if (mod(cellId.x, 4.0) == 1.0 && mod(cellId.y, 4.0) == 1.0) {\n        vec2 center = (cellId + vec2(0.6)) / gridSize;\n        strength += shape(center, 0.0006, aspectCorrectedUV);\n    }\n\n```\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-4.png)\n\nThen I wanted to add noise a little bit thinking that I only need to offset everything:\n```javascript\n    //add noise\n    float noise = cnoise(uv * 1000.0) * 0.25;\n    strength += noise;\n\n```\n\nAnd accidentally got this paper wallpaper texture instead!\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-5.png)\n\nthen I noticed that noise does not use my aspectRatio and id a bit too stretched, so I've put aspectRatio into nopise:\n\n```javascript\n    //add noise\n    float noise = cnoise(vec2(uv.x * 1000.0* aspectRatio, uv.y * 1000.0)) * 0.5;\n    strength += noise;\n\n```\nbut the perfect noise did not look like the paper anymore....\nSo I've saved the stretching factor a little bit:\n```javascript\n   float noise = cnoise(vec2(uv.x * 1000.0* aspectRatio * 0.25, uv.y * 1000.0)) * 0.3; \n```\nNow it's nice! At this point Ella also told me to make the heart \"normal\" because it is not mysterious at all but just looks weird...\n\nSo I flipped it back, and it's time to animate our flowers. I've played with tweaking, put time Uniform into sin and also into noise for the flower size and this is how it's gonna look in 10x speed:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-6.mp4\">\u003C/video>\n\nNice! but I want it to be extremely subtle, nearly not niticeable at all, just to create the feeling that something is off:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-7.mp4\">\u003C/video>\nThis is perfect!\n\nEarlier today I was looking for the floor carpet textures, but now I thought why do I need textures when I can code *all the textures* myself?? *evil laugh*\n\nWe already have the grid logic, now we need stripes for diffuse map **and emissive swuares for the light**.\nIt means we can use my favorite Bloom postprocessing effect! \nTo make it even more mysterious, I added flickering to the squares.\nTo randomize flickering I used their indexes inside of flickering itself and also to determine which square should flicker at what time:\n\n```javascript\n // Place light every 4 cells\n    if (mod(cellId.x, 4.0) == 0.0 && mod(cellId.y, 4.0) == 0.0) {\n        //minus bars width\n        if (cell.x > barWidth && cell.y > barWidth) {\n            //flcker every n seconds:\n            float n = (cellId.x + cellId.y);\n            float time = mod(uTime, n);\n            if (time > (n - 1.0)) {\n                flicker = sin(uTime * 5.0 * (cellId.x + cellId.y)); //from -1 to 1\n                flicker = abs(flicker);\n            }\n            emmissiveStrength = vec3(5.0, 5.0, 8.0) * flicker;\n        }\n    }\n```\nWoow now it looks really nice!😍😍😍\nYou also can notice how flowers are changing on the wallpaper here if you look closely:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-8.mp4\">\u003C/video>\nAfter that i changed my timer float n to multiply the indexes:\n```javascript\n       float n = (cellId.x * cellId.y);\n```\nso that the light flickers less often and more random.\n\nAt the end I also added noise to Diffuse and to Bump texture of the ceiling:\n![wallpaper-shader](/shaders-research-blog/images/hero-22jan.png)\nI am not sure if I remember correctly how bump texture works, I will look into it tomorrow, together with camera movement after Wouter's consult 👀","src/content/blog/01-22-wallpaper.mdx","30fcd052e998b1ee","01-25-interior-design",{"id":159,"data":161,"body":166,"filePath":167,"digest":168,"deferredRender":19},{"title":162,"description":163,"pubDate":164,"heroImage":165},"Interior design day","Tweaking colors, wallpapers and lighting scenarios",["Date","2025-01-24T23:00:00.000Z"],"interior-design.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nToday is Saturday and I want just a chill day making everything nice and pretty, and uncanny, of course.\n\nFirst of all I've noticed my liminal space is not yellow enough, so let's fix it.\nThis is more yellow, but it will also make more sense when we add real light:\n![colors](/shaders-research-blog/images/preview-color-1.png)\n\n#### Lights\nFirst, we can add a bit of yellow color into the ambient light. This will mix all the colors nicely and contrast with our blueish bloom.\n![colors](/shaders-research-blog/images/preview-color-2.png)\n\nSecondly, the bloom effect does not actually lights up our walls and floor and does not cast shadows. Let's see what we could add. \nI have **9 fake lights with Bloom effect,** but I don't want to put **9 lights** there because it is **not good for the performance.** Instead, I will try to fake the light with three js lights like it's coming from my ceiling lamps.\nBecaue the whole scene is surreal and a bit off, I think it can be okay if it is not 100% realistic.\n\nAt first, I thought maybe I could get away with **3 spotlights,** each spotlight for one row of lights:\n![spotlights](/shaders-research-blog/images/spotlights-1.png)\n![spotlights](/shaders-research-blog/images/spotlights.png)\n\nThis is not it at all, and the shadows mixing seems extremely weird and dirty.\n\nThen I thought maybe **directional light?** It gave **better shadows**, but the light itself did not make sense.\nWhat I want is **some rooms being lit up** and **some places being darker** so you're a bit scared to go in there.\n\nSo then I decided to place **pointlights** into my rooms near the fake lamps. Sometimes I've placed them right into the lamos, and sometimes a bit next to it to get a wider area lit:\n![point-light-1](/shaders-research-blog/images/point-light-1.png)\n![point-light-2](/shaders-research-blog/images/point-light-2.png)\n![point-light-3](/shaders-research-blog/images/point-light-3.png)\nCheck out the **bump map** on the ceiling! Now you can really see it, nice!\nSuch a nice result with just  reusing mixing of colors for bump texture:\n``csm_Bump = vec3((1.0 - strength) * 0.4);``\n\nI have only **1 ambient light and 4 pointlights** and I have quite a scary scene already hihihi\n\nBefore going further to the wallpapers, I also **clamped the camera position** so that camera can not go outside the room anymore.\n\n#### Wallpaper\nI spent a little bit more time than expected on lights, checking camera, and then scrolling through Shader Toy for inspiration and then not finding anything, so for now I added just simple shell-like ornaments:\n![wallpaper-seashell](/shaders-research-blog/images/wallpaper-seashell.png)\n\nI also noticed that my UV logic resolution does not work on the seashell turning them into croissants on another wall:\n![wallpaper-croissant](/shaders-research-blog/images/wallpaper-croissant.png)\n\nFor now its time to switch to other rooms & demos!","src/content/blog/01-25-interior-design.mdx","2531460b91f863ef","01-27-flow-field",{"id":169,"data":171,"body":176,"filePath":177,"digest":178,"deferredRender":19},{"title":172,"description":173,"pubDate":174,"heroImage":175},"Flow Field in GLSL","Going further with distorted memories room",["Date","2025-01-26T23:00:00.000Z"],"particles-khruschevka.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n \n\n##### *This post takes up 3 days – 27-29 Jan, because of slow pace work and lack of time*\n---\n\nSometimes it takes hours just to understand what's wrong with the .blend / .glb file and to show it in your project...\nAnd today is that day. Apart from quite silly \"oh I didn't realize the model is **100 times bigger**\", somehow the scene is not rotated(if you just add it to your three js scene), but the **geometry attributes** are...\n\n\nAfter some time we (Ella is saving me with Blender here) noticed that mesh is rotated in blender indeed, however fixing it in blender did not rotate it the correct way in three js. Baking vertex colors also did not go well from the beginning,\nSo when I at least saw correctly oriented system resembling the house I already was a bit relieved:\n![particles-khruschevka](/shaders-research-blog/images/particles-khruschevka.png)\n\nFinally, Ella managed to tweak the settings so that she sees texture in **\"no-texture\" preview mode in Blender** (that means it's gonna work for me):\n![particles-khruschevka](/shaders-research-blog/images/particles-khruschevka-2.png)\n\nBut colors are somehow weird, like extremely low quality texture, when the texture on our original model is quite detailed:\n\n![khruschevka](/shaders-research-blog/images/khruschevka-model.png)\nJust got these screenshots from Ella, this is hilarious:\n![khruschevka](/shaders-research-blog/images/original-texture.png)\n![khruschevka](/shaders-research-blog/images/ugly-texture.png)\n\nFinally we fixed the colors and while debugging I saw that I like the effect together with the normal model itself:\n![khruschevka](/shaders-research-blog/images/khruschevka-flow-field.png)\nFrom here I finally can work further with the shader interactions.\n\n---\n\nI started with organizing my code to free up some space.\nEvery scene I set up canvas, renderer, camera, add resizing eventlistener, so it does not really make sense to keep it with the rest of the logic.\nSo i've put all these \"default\" functions into utils.js, with the option to use or not to use OrbitControls and the color of clear canvas:\n![utils screenshot](/shaders-research-blog/images/utils-js.png)\n\nOkay, so to combine real model and particles, first I will write a shader for the model to hide its material on hover.\nI'm going to extend existing material using [custom shader material](https://www.npmjs.com/package/three-custom-shader-material).\n\nas always, first we just check that nothing broke:\n\n![khruschevka-uv](/shaders-research-blog/images/khruschevka-uv.png)\n\nThen I tried to get the starting color and noticed that if I call the initial color from shader, it's just grey....\n\n![grey-uv](/shaders-research-blog/images/grey-uv.png)\nfrag sgader:\n```javascript\nvarying vec2 vUv;\n\nvoid main() {\n    //retrievening the original color:\n    vec3 color = csm_DiffuseColor.rgb;\n    //NOT USING THIS FOR NOW:\n    //center:\n    vec2 center = vec2(0.5, 0.5);\n    //distance to center:\n    vec2 distance = (vUv - center);\n    float length = length(distance);\n    //alpha based on distance:\n    csm_DiffuseColor = vec4(color, 1.0);\n}\n\n```\n\nI thought maybe the problem was with not providing the color map, even though it should be baked into model:\n```javascript\n// Material\nconst material = new CustomShaderMaterial({\n    //CSM\n    baseMaterial: THREE.MeshStandardMaterial,\n    map: colorTexture, //I've put this both before and after the shaders to compare\n    vertexShader: houseVertexShader,\n    fragmentShader: houseFragmentShader,\n\n    // MeshBasicMaterial properties\n    transparent: true,\n    wireframe: false\n})\n```\nBut it did not fix the problem, I spent some time trying to understand what's wrong with the texture or the shader.\nThen I decided to check what my inital material contains and logged the house's mesh. The mesh had property map on it:\n```javascript\nconst map = gltf.scene.children[0].material.map\nconsole.log(map)\n```\n![property map](/shaders-research-blog/images/property-map.png)\n\nSo i tried just to reuse **this map** one more time passing **this particular map** into the shader:\n```javascript\n//setting shader material to the house\ngltf.scene.children[0].material = shaderMaterial\n//passing initial map as a uniform into the shader material\nshaderMaterial.uniforms.uMap.value = map\n```\nAnd it works!\nLet's try to make a hole in our material, static first:\n```javascript\nuniform sampler2D uMap;\nvarying vec2 vUv;\n\nvoid main() {\n    //retrievening the original color:\n    vec4 textureColor = texture2D(uMap, vUv);\n    //center:\n    vec2 center = vec2(0.5, 0.5);\n    //distance to center:\n    vec2 distance = (vUv - center);\n    float alpha = length(distance);\n    alpha = step(0.5, alpha);\n    //alpha based on distance:\n    csm_DiffuseColor = vec4(textureColor.rgb, alpha);\n}\n```\nAnd now I see another weird thing: it does not have just one hole in the middle, but many little holes as if there are multiple UVs that go from 0 to 1...\n![multiple uvs](/shaders-research-blog/images/many-uvs.png)\nIt actually was already visible on previous screenshot with just UV coordinates colors.\n\n\u003CTakeaway>\nSo we can not rely on **UV coordinates,** but we can retrieve **global position** from our **vertex shader:**\n\u003C/Takeaway>\n\n```javascript\nvarying vec2 vUv;\nvarying vec3 vPosition;\n\nvoid main() {\n  vUv = uv;\n  vPosition = position;\n}\n```\n\nand then we use it frag shader, both **vec2** and **vec3**\n```javascript\nuniform sampler2D uMap;\n\nvarying vec2 vUv;\nvarying vec3 vPosition;\n\nvoid main() {\n    //retrievening the original color:\n    vec4 textureColor = texture2D(uMap, vUv);\n    //center:\n    vec2 center = vec2(0.5, 0.0); //for 2d point\n    vec3 centerRaycastExample = vec3(0.25, 0.0, 0.1); //for 3d point\n    //distance to center:\n    //vec2 pos = vPosition.xz; //for 2d point\n    // float distance = length(pos - center); //for 2d point\n    float distance = length(vPosition - centerRaycastExample); //for 3d point\n    float alpha = step(1.0, distance);\n    //alpha based on distance:\n    csm_DiffuseColor = vec4(textureColor.rgb, alpha);\n    // csm_DiffuseColor = vec4(vUv, 0.0, 1.0);\n}\n\n```\nAnd we got this beautiful hole:\n![hole in the house](/shaders-research-blog/images/hole-in-the-house.png)\n\nHere I will stop for this post as it was more about just getting the basic setup, and continue with actual shader from another post!\n![hole with particles](/shaders-research-blog/images/particles-hole.png)","src/content/blog/01-27-flow-field.mdx","b6399d4c9d772f42","01-24-camera",{"id":179,"data":181,"body":186,"filePath":187,"digest":188,"deferredRender":19},{"title":182,"description":183,"pubDate":184,"heroImage":185},"Camera movement and more wallpapers","Fixing Orbit Controls and writing more shaders in glsl",["Date","2025-01-23T23:00:00.000Z"],"worley-noise.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Camera\n\nAfter taking a day off, I got back to fixing my \"Orbit Controls\" by going away from OrbitControls.\n\nThe main idea is:\n\u003CTakeaway>\n I want to always LookAt my object (let's look right at it for now), and the camera will be rotating around it with fixed radius.\n The user rotates camera around the object with Left and Right controls, and goes back and forward with Backward and Forward controls. \n I will use the angle of camera rotation to calculate absolute values of \"back\" and \"forward\" movement.\n\u003C/Takeaway>\n\nThis is the setup of the demo, still using physics here so that I don't have to change too much:\n```javascript\n useFrame((state, delta) => {\n\n        /**\n         * Controls\n         */\n        const { forward, backward, left, right } = getKeys()\n\n        //Empty statements for now:\n\n        if (forward) {\n\n            }\n        if (backward) {\n     \n            }\n        if (left) {\n\n            }\n        if (right) {\n\n            }\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n\n        const cameraPosition = new THREE.Vector3()\n\n        //Camera is following orbit with a set a radius around the body\n        cameraPosition.x = bodyPosition.x + Math.cos(angle) * radius //depends on rotation\n        cameraPosition.y = bodyPosition.y + 0.5 // always a bit higher than our body\n        cameraPosition.z = bodyPosition.z + Math.sin(angle) * radius //depends on rotation\n\n        const cameraTarget = new THREE.Vector3()\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n\n        state.camera.position.copy(smoothedCameraPosition)\n        state.camera.lookAt(smoothedCameraTarget)\n\n    })\n```\n\nI saw that we can get Vector from radius and angle:\n``cameraPosition.setFromSpherical(new THREE.Spherical(radius, angle, 0))``\nBut I want to do it myself so I have more control over the position and its set more explicitly:\n\n\nWith radius set to 3 this is how it looks:\n![camera-1](/shaders-research-blog/images/camera-demo-1.png)\n\nNow let's try to change the angle and orbit around the body:\n```javascript\n        if (forward) {\n            }\n        if (backward) {\n     \n            }\n        if (left) {\n            setAngle(angle + 0.1*delta)\n            }\n        if (right) {\n            setAngle(angle - 0.1*delta)\n            }\n        \n```\nAt first I have not noticed much so here I've added some random cubes to see the actual rotation and position change:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/orbit-controls-rotation-demo.mp4\">\u003C/video>\n\nIt works, but a little bit too slow. Let's speed it up and try to move the capsule on forward and backward:\n\n```javascript\n    useFrame((state, delta) => {\n\n        /**\n         * Controls\n         */\n        const { forward, backward, left, right } = getKeys()\n\n        const impulseStrength = 0.2 * delta\n\n        const direction = new THREE.Vector3()\n\n        if (forward) {\n            direction.x = -Math.cos(angle) * impulseStrength\n            direction.z = -Math.sin(angle) * impulseStrength\n            }\n        if (backward) {\n            direction.x = Math.cos(angle) * impulseStrength\n            direction.z = Math.sin(angle) * impulseStrength\n            }\n        if (left) {\n            setAngle(angle + 0.5*delta)\n            }\n        if (right) {\n            setAngle(angle - 0.5*delta)\n            }\n        \n        body.current.applyImpulse(direction)\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n\n        const cameraPosition = new THREE.Vector3()\n        cameraPosition.x = bodyPosition.x + Math.cos(angle) * radius\n        cameraPosition.y = bodyPosition.y + 0.5\n        cameraPosition.z = bodyPosition.z + Math.sin(angle) * radius\n\n        const cameraTarget = new THREE.Vector3()\n        cameraTarget.copy(bodyPosition)\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n\n        state.camera.position.copy(smoothedCameraPosition)\n        state.camera.lookAt(smoothedCameraTarget)\n\n    })\n    ```\nAnd it works like this:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/orbit-controls-demo.mp4\">\u003C/video>\n\nI think this is a good time for a confession: as I write the blog alongside with coding, sometimes I am changing snippets of code that I've just pasted here and don't understand why I can't see changes in my project...\nThis is why it's a good idea to take a break from time to time. \nThis is final result in my experience (I took the ceiling out for the moment):\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/controls-final.mp4\">\u003C/video>\n\nI also understood that even I am lost in my rooms from the capsule perspective and maybe there's no need to hide secret *\"transporting\"* areas for even greater confusion... will leave it for now.\n\nLet's add one more wallpaper texture and the floor texture!\n\n#### Floor\nAs usual, first let's check that our frag shader works:\n![check](/shaders-research-blog/images/check.png)\n\nI wanted to try different noise, and tried Worley, but I did not understand how to use it🤨🤨\n![check](/shaders-research-blog/images/worley-noise.png)\n\nPutting Perlin noise for now and will get back to Worley later one more time.\nI've also added Leva controls (I should have done it long time ago):\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/carpet-noise.mp4\">\u003C/video>\n\nI've got feedback from Ella that floor looks weirdly pixelated (I thought it were just my eyes at this point), putting step function to cut off blurred mixing helped:\n```javascript\n    float noise = step(threshold, cnoise(vUv * scale));\n```\n\nI might tweak it a bit later as well as I am not quite happy with this noise.\n\n#### Wallpaper \nI want to have mix of walls with wallpapers and plain walls. This time I want to create stripes with some more complex wavy simmetrical ornament.\nBut for today just to start with I added stripes and paper texture. The logic is quite similar to previous wallpaper, the most interesting part is ahead.\n\nThis is what I have for today, I also tweaked camera target a bit above our body to get a better angle while navigating the experience:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/24-jan-gameplay.mp4\">\u003C/video>\n\nFor tomorrow I am planning to work on the second wallpaper, tweaking the light for the scene, and after that I can start conecting my rooms!\nI already miss WGSL a little bit...","src/content/blog/01-24-camera.mdx","16223b4c82df6d8c","01-29-flow-field-2",{"id":189,"data":191,"body":196,"filePath":197,"digest":198,"deferredRender":19},{"title":192,"description":193,"pubDate":194,"heroImage":195},"Flow Field in GLSL – working with shader","Picking up from the setup",["Date","2025-01-28T23:00:00.000Z"],"hole-in-the-house.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nContinuing from before, now that we hole in the house, let's connect it to the mouse position:\n```javascript\n    const checkIntersects = (object) => {\n    const modelIntersects = raycaster.intersectObject(object)\n    if (modelIntersects.length) {\n        uniforms.uTouchPosition.value = modelIntersects[0].point\n    }\n}\n```\nthis is what we get:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/holes-in-the-house.mp4\">\u003C/video>\n\nBut I want this opacity gradient to go **less even** and more ***\"eased out\"***. For this I'm putting my alpha into power of 3:\n![hole-pow-3](/shaders-research-blog/images/hole-pow-3.png)\nor maybe even 4 ``float distance = pow(length(vPosition - uTouchPosition), 4.0);``:\n![hole-pow-4](/shaders-research-blog/images/hole-pow-4.png)\n\nNow I want particles to be shown where the hole in the house is, so let's pass the position into particles shader.\nBut how we gonna use it there? To make particles transparent just like the house would be weird. \n\n\nInstead, I want to use it for the **flow field strength,** especially that the model looks reall nice when its covered in particles that are not flying away:\n![particles-hole](/shaders-research-blog/images/particles-hole.png)\n\nSo what we need to do is go to gpgpu shader and check how close our **base position** is to **touch point**:\n```javascript\n     //flowfieldStrength depending on distance between base and touch position\n     float distance = length(base.xyz - uTouchPosition);\n     \n     //flowFieldStrength between 0 and 7\n     float flowFieldStrength = (1.0 - smoothstep(0.0, 1.0, distance)) * 7.0; \n\n     //apply flowfield to the particle\n     particle.xyz += flowField * uDeltaTime * strength * flowFieldStrength;\n```\nand finally we're getting beautiful result!\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/flow-field-khruschevka.mp4\">\u003C/video>\n\nNext step is to tweak uniforms and maybe area of influence of hover effect.","src/content/blog/01-29-flow-field-2.mdx","c13a4fe14ab87ab9","01-30-memories-room",{"id":199,"data":201,"body":206,"filePath":207,"digest":208,"deferredRender":19},{"title":202,"description":203,"pubDate":204,"heroImage":205},"Memories room","Finishing the memories room",["Date","2025-01-29T23:00:00.000Z"],"particles-flow-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nToday I am working towards the finish the ***distorted memories*** part of my experience. I am planning to add more to the environment around the house and to make shader more interactive.\n\n#### environment\nAt the consult Wouter suggested that I try to use **3d piece of an actual map** as an anvironment around my **[khruschevka](https://en.wikipedia.org/wiki/Khrushchevka)**.\nWith Ella's help we downloaded the [Blosm addon](https://github.com/vvoovv/blosm), but the thing is that of course, Russia did not give permission to Google to scan buildings in 3d.\n\"You never know what these Google spies are up to\" I thought, and checked Russian maps sources, but you can not get scans out of there.\n\nThen I thought okay, the post-soviet urban areas are not so exclusive and they don't solely exist in Russia, but then I found out that post-soviet european countries have very little city center scanned in 3d or no scans at all:\n![flat map](/shaders-research-blog/images/flat-map-scan.jpg)\nSo it's quite impossible to find the scanned part that fits the looks.\n\nIf I use the flat mesh it does not look nice at all, so I will just add some planes to the scene to anchor it in the space.\n\nAs I'm doing this in vanilla js and I'm trying to keep main script readable, I created module environment js just to create the group and then add it in the main script:\n```javascript\n/**\n * Scene's environment\n */\nconst environmentGroup = environment()\nscene.add(environmentGroup)\n```\n\nAfter trying several asphalt maps and several setups, I had a crisis and Ella suggested to extrude my flat map of actual Russian neighbourhood.\n![house-setup](/shaders-research-blog/images/house-setup.png)\n\nmeanwhile, I will work on flow field shader\n\n#### gpgpu shader\nSo as we are using a texture as a buffer between our \"compute\" shader and actual shader, we have **n pixels** in it, where we store data about our vertices positions.\nWe got ***n*** by taking a square root of our geometry attributes array size and ceiling the number. So unless we our array size is a perfect power of 2, we have some extra space.\n\nLet's console our array and our texture size:\n![console array size](/shaders-research-blog/images/console-array-size.png)\n\nOur **array count equals 460043** , and our **texture size is 679**.\n\nWe can count the amount of pixels in our computing texture:\n679 x 679 = 461041\n\nit means that we have **998 pixels left** to store the data!\n\n\u003CTakeaway>\nWe don't even need that much. Instead, I want to save previous **touch position** and previous **FlowFieldInfluence.** Then, in my gpgpu shader I will calculate the distance between current touch point and previous touch point.\nSo if the user moves mouse faster, the distance will be bigger. Then, I will take that into account when calculating new FlowFieldInfluence and lerp between previous and current values.\n\u003C/Takeaway>\n\nOkay, so first I thought I will store my **touch point from Raycaster** in the texture as well, but then I realised as we are changing it **within javascript**, we need to use **uniform** for that:\n```javascript\n/**\n * Raycasting\n */\nconst raycaster = new THREE.Raycaster()\n\nconst checkIntersects = (object) => {\n    const modelIntersects = raycaster.intersectObject(object)\n    if (modelIntersects.length) {\n        // Store current position as previous before updating current\n        gpgpu.particlesVariable.material.uniforms.uPreviousTouchPosition.value.copy(\n            gpgpu.particlesVariable.material.uniforms.uTouchPosition.value\n        )\n\n        // Update current touch position\n        uniforms.uTouchPosition.value = modelIntersects[0].point\n        gpgpu.particlesVariable.material.uniforms.uTouchPosition.value = modelIntersects[0].point\n    }\n}\n```\n\nthen we can get distance between them in gpgpu shader:\n```javascript\n    // Calculate movement speed based on distance between current and previous touch positions\n    float touchMovementDistance = length(uTouchPosition - uPreviousTouchPosition);\n    float speedFactor = clamp(touchMovementDistance * 4.0, 0.0, 1.0); // still tweaking sensitivity factor\n    float dynamicInfluence = (speedFactor - 0.5) * (-2.0); \n```\nnow, if we base our flowField influence only on that, it will jump every frame in a weird way.\n\nWhat we could instead is to save flowField influence as a part of a texture of gpgpu shader:\n```javascript\n// Store values in the last pixel\nconst lastPixelIndex = (baseGeometry.count) * 4\n//flowFieldInfluence 0.0\nbaseParticlesTexture.image.data[lastPixelIndex + 0] = 0.0 //start with 0.0\n```\n\nAnd then we retrieve our initial flowfield inside of the shader:\n```javascript\n    // Get the initial flow field influence from the last pixel\n    vec4 lastPixel = texture(uParticles, vec2(1.0));\n    float initialInfluence = lastPixel.x;\n```\nand then instead of using dynamicInfluence directly, we lerp between initial influence and target influence:\n\n```javascript\n    targetInfluence = smoothstep(dynamicInfluence, 1.0, targetInfluence);\n    float flowFieldInfluence = mix(initialInfluence, targetInfluence, 0.5);\n```\nand then we need to save this influence for the next frame:\n\n```javascript\n        //are we in the last pixel?\n        if (gl_FragCoord.x >= resolution.x - 1.0 && gl_FragCoord.y >= resolution.y - 1.0) {\n            gl_FragColor = vec4(flowFieldInfluence, 0.0, 0.0, 0.0);\n            return;\n        }\n```\nAnd with this we have this smooth result of flowField influence depending on a the mouse velocity:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/flow-field-khruschevka-2.mp4\">\u003C/video>\n\n\nthis might be too sensitive, also now when we don't move mouse and the position is 0 nothing hapens. I want the time to accumulate the intensity of flow field as well.\nAlso here you can see some experimenting with the setup instead of the map scene, at the end it got a bit of Lynch vibes...","src/content/blog/01-30-memories-room.mdx","edb423d59adbe27a","01-31-memories-space",{"id":209,"data":211,"body":215,"filePath":216,"digest":217,"deferredRender":19},{"title":212,"description":203,"pubDate":213,"heroImage":214},"Memories space",["Date","2025-01-30T23:00:00.000Z"],"memories-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nFirst things first, I've deployed current versions of corridor and memories spaces:\n\n--> https://corridor-iota.vercel.app/\n\n--> https://memories-jet-xi.vercel.app/\n\n#### Optimization experiments\nAfter yesterday's consult I've been thinking about the GPGPU setup where we use a model's geometry to position our particles.\nAs its a particle per vertex, we need many subdivisions for the final result to look nice.\n\nBut do we really need to load the whole heavy model every time, or do *we need only **geometry** and **vertex color** atributes?*...\n\nI'm not going to try to generate more positions based on given vertexes, but I want to try to extract only these attributes and see what happens.\n![google search](/shaders-research-blog/images/google-attributes.png)\n\nApparently it's not something you can do in Blender by default.\nSo I've created a separate demo with kinda the same minimal setup and uploaded my model.\nI've changed the geometry just like I use it in my main project:\n```javascript\n/**\n * Base Geometry\n */\n\nconst baseGeometry = {}\nconsole.log(gltf.scene.children[0])\n\nbaseGeometry.instance = gltf.scene.children[0].geometry\n//set the scale\nbaseGeometry.instance.scale(0.25, 0.25, 0.25)\n//rotate\nbaseGeometry.instance.rotateX(Math.PI / 2)\n\n//console.log instance attributes\nconsole.log(baseGeometry.instance.attributes)\n```\nFrom this we will only use position attribute and color_1 (this the one that contains vector color data).\nI don't really need anything else, right?\n\nSo what if I just export these particular attributes in some form and use only them?\n```javascript\n//Reduce precision to 3 decimal places\nconst positions = baseGeometry.instance.attributes.position.array;\nconst positionsText = Array.from(positions).map(n => n.toFixed(3)).join('\\n');\n\n// Export position data as text file\nconst blob = new Blob([positionsText], { type: 'text/plain' });\nconst url = URL.createObjectURL(blob);\nconst link = document.createElement('a');\nlink.href = url;\nlink.download = 'positions.txt';\nlink.click();\nURL.revokeObjectURL(url);\n```\nand this is what I get:\n\n![txt screenshot](/shaders-research-blog/images/txt-screenshot.png)\n\nI tried different options for colors and positions attributes:\n\n***Positions Atribute array***\n| Format  | Size |\n| -------- | ------- |\n| .json | 44.4 mb |\n| .txt   | 27 mb |\n| .txt - 3 decimal precision| 9 mb |\n| .txt - 2 decimal precision | 7.6 mb |\n| .bin | 5.5mb |\n\n***Colors Atribute array***\n| Format  | Size |\n| -------- | ------- |\n| .json | 34.2 mb |\n| .txt - 2 decimal precision | 6.9 mb |\n| .bin | 5.5 mb |\n\nIt's also too big to expect that Vite will compress it while building unless I break it up into chunks...\n\nAnd then I compressed the .glb of the house using DRACO compression, keeping the big amount of vertices that I need, and it went down **from 43.7mb to 1.2mb**, and result looks exactly the same...\n\nWell, that was an interesting little experiment, let's get back to our shader.\n\n#### Adding time of standby to the shader\n\nSo, yesterday I stopped at adding strength to the flowFieldInfluence together with the speed of the cursor.\nNow when the cursor does not move at all, nothing really happens. I want to change that.\n\n\u003CTakeaway>\nI also already added **time of standby** value already to my gpgpu texture. This value grows when the distance between previous touch point and current touch point is less than 0.01, meaning we have not moved our cursor.\nI use this value as dynamic factor in my flowFieldInfluence value, just like speed factor. \n\u003C/Takeaway>\n\nso here I retrieve the value from the texture:\n```javascript\n     // Get the data from the last pixel\n    vec4 lastPixel = texture(uParticles, vec2(1.0));\n    //Get the initial flow field influence\n    float initialInfluence = lastPixel.x;\n    //get the time of standby\n    float timeStandby = lastPixel.y;\n```\n\nthen I would count my \"base\" random flowfield influence and add speed factor from cursor:\n```javascript\n    //strength with speed influence\n    float baseInfluence = simplexNoise4d(vec4(base.xyz * 0.2, time + 1.0));\n    //desired uFlowFieldInfluence is between 0.75 and 1\n    float dynamicInfluence = (speedFactor - 0.5) * (-2.0); //-0.2 to -1\n```\n\nand then here is where I got lost yesterday, as you can't console.log values it's extremely hard to remember which approx constraints every factor has. \nThis is why I have so many comments and sometimes leave something like ``//-0.2 to -1`` to help myself with approximate calculations.\n\nSo my problem was that I was clamping dynamicInfluence between 0.0 and 1.0, so it did not really do anything. Instead, it should be negative and have a bit higher amplitude:\n```javascript\n        // if we're in standby, use time of standby instead of speedFactor\n        if (touchMovementDistance \u003C 0.01) {\n            dynamicInfluence = -timeStandby;\n            //clamp dynamicInfluence between -5.0 and 0.0\n            dynamicInfluence = clamp(dynamicInfluence, -5.0, 0.0);\n            timeStandby += uDeltaTime*0.1;\n        }  else {\n            timeStandby = 0.0;  // Reset the timer when there's movement\n        }\n\n        float targetInfluence = smoothstep(dynamicInfluence, 1.0, baseInfluence);\n\n        // Gradually interpolate between initial and target influence\n        float flowFieldInfluence = mix(initialInfluence, targetInfluence, 0.5);\n```\n\nAnd now it works!\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/gpgpu-final.mp4\">\u003C/video>\n\n\nI also experimented with standby time sensitivity tweaking multiplier `` timeStandby += uDeltaTime*0.1;``.\nAnd now we have not just an interactive shader, but an interactive \"compute\" shader, cool!\n\n#### Little extra\n\nAnother thing i tried today was finally creating my own .hdr environment map from a 360 HDR photo taken on Insta 360 camera.\nI should've tried it earlier, but I always pushed it back. Using Insta360 Studio and certain Photoshop settings, I managed to export my 360 in a correct format and add it to my scene:\n\n![env map](/shaders-research-blog/images/env-map.png)\n\nIt looks a bit weird on this screenshot, but it actually works and you don't even see any seams from the tripod where the picture was taken!\nAs I took this 360 picture at my friend's room which was quite small, it looks weird. If I widen my camera angle with bigger camera's Field Of View, I can see more of the environment.\n\nI think it might have a potential so I will try to use this with an outside photo.","src/content/blog/01-31-memories-space.mdx","3fd2fe08f231084d","02-01-clean-up",{"id":218,"data":220,"body":225,"filePath":226,"digest":227,"deferredRender":19},{"title":221,"description":222,"pubDate":223,"heroImage":224},"Clean up","No more coats and no more home",["Date","2025-01-31T21:00:00.000Z"],"no-more-coats.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Clean up and final tweaks\nSo I am happy with my flow field shader logic, so I am taking out tweaks and debugging parts. \nWhen I started working on this, I limited flow field direction to only go down to create \"decaying\" image, but at the end my house is standing on the ground so you can't see all these beautiful swirls really.\nI tweaked the frequency to make this beautifil swirls as long as possible, but also to keep them \"swirly\" enough. \nHere how it looks with final particles size and frequency:\n![final-frequency](/shaders-research-blog/images/final-frequency.png)\n\n#### No more coats and no more home\nAt the end my khrushchevka is floating in \"memories\" space, surrounded by ghosts of many others khrushchevkas, just like real-life built-up neighborhoods.\nI wanted to create a very clear feeling of absence: you see this familiar \"token\" of your old life as it ceases to exist. You know that of course it is still standing somewhere, but this space is no longer actually real for you.\n\nMany years ago I saw this painting by Christopher Wool:\n\n![christopher wool](/shaders-research-blog/images/wool.webp)\n\nThese words resonated with me, they got stuck in my head forever. I immediately thought about the context from which these words were taken, I think I googled it right at Pompidou Centre where I saw it for the first time. \nWool took this quote from \"The Revolution of Everyday Life\" by Belgian writer Raoul Vaneigem, who in his turn quoted Russian philosopher Vasily Rozanov. Vasily Rozanov in his \"The Apocalypse of Our Time\", 1918, was trying to explain the essense of nihilism during revolution period in Russia. \n\n*\"\nFor nihilism is a European not an American invention. Gagosian’s title comes from Vasily Rozanov’s essay “The Apocalypse of Our Time”, written just before the philosopher starved to death in 1919. “With a clang, a creak and a scream the iron curtain drops on Russian history,” Rozanov predicted. “The show is over. The audience get up to leave their seats. Time to collect their coats and go home. They turn round. No more coats and no more home.”\n\"* – from [Financial Times article](https://www.ft.com/content/a594f7d2-3022-11e3-9eec-00144feab7de)\n\n\nI feel that it also fits with the current situation, and while of course it is just a metaphor, I think it is also a very accurate depiction of the impotency of the Russian crowd.\n\n![no more coats ](/shaders-research-blog/images/no-more-coats.png)\n\nI think because at least half of the string of text is nearly always is closed by house material, sometimes it disappears because it's occluded by the house. For now I disabled **depthWrite** on the house material and **depthTest** on the string material. I also will tweak the layout and the font later.\n\nI also started working on \"impossible\" geometry for my corridor, but it's going to be another post.","src/content/blog/02-01-clean-up.mdx","150fbcee4cfbb171","02-04-curtains",{"id":228,"data":230,"body":235,"filePath":236,"digest":237,"deferredRender":19},{"title":231,"description":232,"pubDate":233,"heroImage":234},"Curtains js","Using library for WebGPU set up",["Date","2025-02-03T23:00:00.000Z"],"curtains-demo.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nToday I am following this [Okay Dev article](https://okaydev.co/articles/dive-into-webgpu-part-1). The author, Martin\nLaxenaire also created javascript library for turning html elements into WebGLcanvases, [curtains js](https://www.curtainsjs.com/). And with WebGPU release he refactored his code to be able to work with WebGPU – [gpu curtains](https://github.com/martinlaxenaire/gpu-curtains).\nI am particularly interested in this because one of the trickiest part with WebGPU is the setup, and I at least want to see this approach to understand it from another perspective.\n\nHe also works a lot with GSAP, and together with GSAP he uses another library for called [Lenis](https://lenis.darkroom.engineering/). Lenis is a library for smoothing scrolling, which I have never heard of before! \nI have used smooth scroll by GSAP, it was quite limited for phones, but quite cool on desktop. I am not sure if Lenis is more performant (they seem to state so) and if they have solved the issue with touch devices, but I am glad to try something new and at least is free! Unlike smoothscrolling in GSAP.\n\nI followed through the first demo, introducing curtians js setup using **Demo.js, DemoScene.js and IntroScene.js classes,** and thought to myself that I should use **object-oriented** programming much more...\n![curtains-demo](/shaders-research-blog/images/curtains-demo.png)\n I also scrolled through 2nd and 3rd demo, but the most interesting is compute shaders one, the 4th one \"for particles lovers\" – that's me!\n I'm also lucky beacuse it's been published just today =)\n\nFollowing the Martin's article, the first nice example:\nUnlike the example from WebGPU fundamentals example, this shader positions points in 3d, uses *billboarding* method, and uses normals for further lighting in frag shader:\n```javascript\n  @vertex fn main(\n    attributes: Attributes,\n  ) -> VSOutput {    \n    var vsOutput : VSOutput;\n    \n    let instanceIndex: f32 = f32(attributes.instanceIndex);\n    const PI: f32 = 3.14159265359;\n    \n    var position: vec3f;\n    \n    // random radius in the [0, params.radius] range\n    let radius: f32 = rand11(cos(instanceIndex)) * params.radius;\n    \n    let phi: f32 = (rand11(sin(instanceIndex)) - 0.5) * PI;\n    \n    let theta: f32 = rand11(sin(cos(instanceIndex) * PI)) * PI * 2;\n\n    position.x = radius * cos(theta) * cos(phi);\n    position.y = radius * sin(phi);\n    position.z = radius * sin(theta) * cos(phi);\n    \n    // billboarding\n    var mvPosition: vec4f = matrices.modelView * vec4(position, 1.0);\n    mvPosition += vec4(attributes.position, 0.0);\n    vsOutput.position = camera.projection * mvPosition;\n    \n    vsOutput.uv = attributes.uv;\n    \n    // normals in view space to follow billboarding\n    vsOutput.normal = getViewNormal(attributes.normal);\n    \n    return vsOutput;\n  }\n```\n\nOne more important thing that I might have missed while describing difference between WebGL and WebGPU when it comes to compute shaders:\nI've always compared compute shaders with using ping-ponging textures in GLSL, but compute shaders is not **just replacement for this**.\n\nThe cool thing is that you also get control on **computing threads**, meaning that you determine how much calculations are running **in parallel**. \nThis was one of the concepts I've mentioned as confusing when starting working with compute shaders. I'm just mentioning it here again to keep in mind for my presentation as well.\n\n\nOkay, so for the compute shaders with our particles here, at first we change the positioning from vertex to compute shader:\n```javascript\n\n  // https://gist.github.com/munrocket/236ed5ba7e409b8bdf1ff6eca5dcdc39\n  // On generating random numbers, with help of y= [(a+x)sin(bx)] mod 1\", W.J.J. Rey, 22nd European Meeting of Statisticians 1998\n  fn rand11(n: f32) -> f32 { return fract(sin(n) * 43758.5453123); }\n  \n  fn getInitLife(index: f32) -> f32 {\n    return round(rand11(cos(index)) * params.maxLife * 0.95) + params.maxLife * 0.05;\n  }\n  \n  const PI: f32 = 3.14159265359;\n  \n  // set initial positions and data\n  @compute @workgroup_size(256) fn setInitData(\n    @builtin(global_invocation_id) GlobalInvocationID: vec3\u003Cu32>\n  ) {\n    let index = GlobalInvocationID.x;\n    \n    if(index \u003C arrayLength(&particles)) {\n      let fIndex: f32 = f32(index);\n      \n      // calculate a random particle init life, in number of frames\n      var initLife: f32 = getInitLife(fIndex);\n      \n      initParticles[index].position.w = initLife;\n      particles[index].position.w = initLife;\n      \n      // now the positions\n      // calculate an initial random position inside a sphere of a defined radius\n      var position: vec3f;\n      \n      // random radius in the [0.5 * params.radius, params.radius] range\n      let radius: f32 = (0.5 + rand11(cos(fIndex)) * 0.5) * params.radius;\n      let phi: f32 = (rand11(sin(fIndex)) - 0.5) * PI;\n      let theta: f32 = rand11(sin(cos(fIndex) * PI)) * PI * 2;\n\n      position.x = radius * cos(theta) * cos(phi);\n      position.y = radius * sin(phi);\n      position.z = radius * sin(theta) * cos(phi);\n\n      // initial velocity\n      var velocity: vec3f = vec3(0.0);\n      particles[index].velocity = vec4(velocity, initLife);\n            \n      // write positions\n      particles[index].position.x = position.x;\n      particles[index].position.y = position.y;\n      particles[index].position.z = position.z;\n\n      initParticles[index].position.x = position.x;\n      initParticles[index].position.y = position.y;\n      initParticles[index].position.z = position.z;\n    }\n  }\n\n```\n\nthen we pass particlesPosition into our vertex shader:\n```javascript\n  createParticles() {\n    const geometry = new PlaneGeometry({\n      instancesCount: this.nbInstances,\n      vertexBuffers: [\n        {\n          // use instancing\n          stepMode: 'instance',\n          name: 'instanceAttributes',\n          buffer: this.updateComputeBuffer.buffer, // pass the compute buffer right away\n          attributes: [\n            {\n              name: 'particlePosition',\n              type: 'vec4f',\n              bufferFormat: 'float32x4',\n              size: 4,\n            },\n            {\n              name: 'particleVelocity',\n              type: 'vec4f',\n              bufferFormat: 'float32x4',\n              size: 4,\n            },\n          ],\n        },\n      ],\n    })\n    ...\n    }\n```\nand retrieve this attribute in our vertex shader instead of calculating positions there:\n```javascript\n @vertex fn main(\n    attributes: Attributes,\n  ) -> VSOutput {    \n    var vsOutput : VSOutput;\n    \n    // billboarding\n    var mvPosition: vec4f = matrices.modelView * vec4(attributes.particlePosition.xyz, 1.0);\n    mvPosition += vec4(attributes.position, 0.0);\n    vsOutput.position = camera.projection * mvPosition;\n    \n    vsOutput.uv = attributes.uv;\n    \n    // normals in view space to follow billboarding\n    vsOutput.normal = getViewNormal(attributes.normal);\n    \n    return vsOutput;\n  }\n```\n\nAnd we still get the sphere:\n![curtains-demo](/shaders-research-blog/images/curtains-sphere-1.png)\n\nAs we initiate compute shader, we pass our compute shader, bindGroups, dispatchSize which will determine how many calcualtions will be executed in parallel. Heere's another nice explanation about workthreads:\n\n\u003CTakeaway>\nThe dispatchSize is equal to our number of instances divided by **256** since this is the workgroup size we’ve set in our shader (this is also the maximum allowed for the X and Y dimensions). \nThis means the compute shader will run a little less than **400 times (100,000 / 256) on 256 threads** simultaneously. \nThis is much, much faster than our previous vertex shader example!\n\u003C/Takeaway>\n\nAfter including curl noise inside of our compute shader, we can use it for the velocity:\n```javascript\n      var vVel: vec3f = particles[index].velocity.xyz; //retrieving initial velocity\n\n      vVel += curlNoise(vPos * 0.02, 0.0, 0.05);\n      vVel *= 0.4;\n```\nThen we also added fragment shader for mixing colors based on it's position to give it more volume.\nAnd we get noisy particles \"sphere\":\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wgsl-okay-dev.mp4\">\u003C/video>\n\nThen following the article we also add shadow mapping on particles and the box around it:\n\n![curtains-demo](/shaders-research-blog/images/curtains-sphere-2.png)\n\nI will not include snippets from shadow mapping here otherwise it would be too long and so far I am not sure how I will use it in my project.\nThis is something that I will try to do next time!","src/content/blog/02-04-curtains.mdx","c8304b2873d512f6","02-07-conspiracy",{"id":238,"data":240,"body":244,"filePath":245,"digest":246,"deferredRender":19},{"title":241,"description":232,"pubDate":242,"heroImage":243},"Working on Conspiracy Theory",["Date","2025-02-06T23:00:00.000Z"],"wgsl-curtains-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nToday I am working on connecting the example from Okay Dev article and my compute shader.\nI am using the same structure as in the article, exporting classes per scene.\n\nFirst important tweak is that I am pretty sure that it looks different on my laptop. It happens beacuse it runs all calculations per frame and not per delta time.\n\nSo what I did is introduced uniform delta time:\n```javascript\n    onRender() {\n        const currentTime = performance.now() / 1000\n\n        this.timeDelta.current = currentTime - this.timeLast.current\n        this.timeLast.current = currentTime\n\n        this.mouse.lerped.lerp(this.mouse.current, 0.5)\n    }\n```\nand passed it to the compute shader too:\n```javascript\n this.computeBindGroup = new BindGroup(this.renderer, {\n            label: 'Compute instances bind group',\n            bindings: [this.initComputeBuffer, this.updateComputeBuffer],\n            uniforms: {\n                params: {\n                    visibility: ['compute'],\n                    struct: {\n                        radius: {\n                            type: 'f32',\n                            value: this.radius,\n                        },\n                        maxLife: {\n                            type: 'f32',\n                            value: 100.0,\n                        },\n                        deltaTime: {\n                            type: 'f32',\n                            value: this.timeDelta.current,\n                        },\n                        mouse: {\n                            type: 'vec2f',\n                            value: this.mouse.lerped,\n                        },\n                    },\n                },\n            },\n })\n```\n\nI thought it will follow the same logic as the following mouse logic, but it behaved weirdly. It got stuck on the first frame or was unpredictably different:\n![curtains-demo](/shaders-research-blog/images/curtains-deltaTime.png)\n\nAfter some debugging I realized that deltaTime is not being updated correctly in uniform buffer, and my compute shader does not actually use it correctly.\n\nAs I mentoined before, the big issue nearly all of the time is to understand actual set up before even diving into the shaders, and while I hoped that curtains library can help me with that, it's again a new logic to get used to.\nI am extremely not sure about this, but the only way I made it work is to explixitly update the uniforms of the bind group of compute shader with this:\n\n```javascript\n    onRender() {\n        const currentTime = performance.now() / 1000\n\n        this.timeDelta.current = currentTime - this.timeLast.current\n        this.timeLast.current = currentTime\n\n        //updating compute shader bind group\n        this.computeBindGroup.bindings[2].inputs.deltaTime._value = this.timeDelta.current\n\n        this.mouse.lerped.lerp(this.mouse.current, 0.5)\n    }\n```\n\nAnd now it works!\nI've played with the speed and the frequency, and then I thought – how far can I go with the performance without changing much?\n\n\u003CTakeaway>\nUnlike ping-ponging example, I am not loading any **model** or **positioning** logic on cpu, so why not crack up the number maybe to **1 million** particles? or maybe even 2?\nAt number of *1 million* it looks extremely smooth which is nice.\n\u003C/Takeaway>\nI did not look for solutions like ``\u003CPerf/>`` and simply logged my deltaTime which is supposed to be time per frame.\nOf course, consoling itself is extremely not performant, but here you can see that the framerate is very okay, being around **100 frames per second** most of the time:\n![curtains-demo](/shaders-research-blog/images/framerate.png)\n\nParticles also start look like a fluid or a cloth at this number, very soothing effect and I literally caught myself just staring at them several times=)\n\nFor next step I added a bit more interactivity not only to position, but to the color in frag shader:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/million-particles-frag.mp4\">\u003C/video>\n\nAnd then also to the compute shader for the frequency. Now I use both **absolute** and **normalised** mouse positions: absolute to position the center of particle system, and normalised to let it influence the movement of the particles.\n\nMy idea is to create a simple scene with a couple of common conspiracy theories, and then particle systems will work as a representation of these common thoughts bouncing between them while being transformed a little bit:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/conspiracy.mp4\">\u003C/video>\n\nI will add more content and deploy it tomorrow.","src/content/blog/02-07-conspiracy.mdx","05c3a2ea78ebe5a2"]