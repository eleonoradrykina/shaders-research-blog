[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.0.3","config-digest","8d1b7566f4d3c391","blog",["Map",9,10,20,21,30,31,40,41,50,51,60,61,70,71,80,81,90,91,100,101,110,111,120,121,130,131,140,141,149,150],"01-06-start",{"id":9,"data":11,"body":16,"filePath":17,"digest":18,"deferredRender":19},{"title":12,"description":13,"pubDate":14,"heroImage":15},"Starting with resources","What we going to read and watch",["Date","2025-01-05T23:00:00.000Z"],"blog-placeholder-3.jpg","import Takeaway from \"../../components/Takeaway.astro\";\n\nFirst things first: before starting this project, I have several links and tabs hanging open in my Google for weeks.\nToday I sorted them into more or less subtopics of my research resources:\n\n| SHADING LANGUAGES    | Links |\n| -------- | ------- |\n| TSL  | [Specification](https://github.com/mrdoob/three.js/wiki/Three.js-Shading-Language)   |\n| TSL | [Medium Article and video from @gianluca.lomarco](https://medium.com/@gianluca.lomarco/three-js-shading-language-a-new-era-for-shaders-cd48de8b22b0)   |\n| WGSL    | [Damien Seguin: comparing WGSL and GLSL](https://dmnsgn.me/blog/from-glsl-to-wgsl-the-future-of-shaders-on-the-web/)   |\n| WGSL    | [WebGPU best Practices pdf from Khronos Group](https://www.khronos.org/assets/uploads/developers/presentations/WebGPU_Best_Practices_Google.pdf)|\n| WGSL    | [Podcast with Gregg Tavares (author of WebGL/WebGPU Fundamentals)](https://changelog.com/jsparty/304)|\n| GLSL    | [All shaders lessons of Bruno Simon](https://threejs-journey.com/lessons/shader-patterns) |\n| GLSL    | [Non-Figurativ \"Entanglement\" piece source code](https://github.com/bgstaal/gpuparticles)|\n\n**Extra:** \n[Introduction to Signed Distance Fields](https://www.youtube.com/watch?v=pEdlZ9W2Xs0)\n\n\n| More on WebGPU   | Links |\n| -------- | ------- |\n| WebGPU Fundamentals    | [Github Repo](https://github.com/webgpu/webgpufundamentals) |\n| WebGPU Fundamentals    | [Webgpufundamentals.org Lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-wgsl.html)|\n| WebGL & WebGPU Meetup - November 2024  | [Youtube video](https://www.youtube.com/watch?v=koY-kDb50VI) |\n\n| Working with Shaders and Materials | Links |\n| -------- | ------- |\n| Nodetoy   | https://nodetoy.co/|\n| Material X    | https://materialx.org/ |\n\n\nI started with reading Medium Article and watching Youtube video from @gianluca.lomarco about what Threejs Shading Language is (finally italian skills come in use!).\nTo plan what I am going to do, I wanted to get a better understanding of what **TSL** is.\nMain takeaways are:\n\u003CTakeaway header=\"Nodes\">\nTSL facilitates shader development by breaking down the shaders into a series of nodes, each applying a specific effect. These nodes can be combined to generate the final shader.\n\u003C/Takeaway>\n\n\u003CTakeaway header=\"WEBGL & WEBGPU\">\nTSL will automatically handle the adaptation for the appropriate API, whether GLSL for WebGL or WGSL for WebGPU.\n\u003C/Takeaway>\n\nNext steps for me will be to get more comofrtable with writing shading langugaes and plan out the roadmap.","src/content/blog/01-06-start.mdx","7619c5767a952c82",true,"01-07-writing-shaders",{"id":20,"data":22,"body":27,"filePath":28,"digest":29,"deferredRender":19},{"title":23,"description":24,"pubDate":25,"heroImage":26},"GLSL tutorials","Remembering how to write shaders",["Date","2025-01-06T23:00:00.000Z"],"shader-1.png","I started with Bruno Simon's tutorials to remember basics of GLSL, and already found out interesting combinations:\n``` javascript\n   //BLINGS MEETING EACH OTHER\n    vec2 rotatedUv = rotate(vUV, PI/4.0, vec2(0.5, 0.5));\n\n    vec2 lightUv = vec2(\n        rotatedUv.x*0.2 + 0.45,\n        rotatedUv.y \n    );\n\n    float strengthX = 0.125 / distance(lightUv, vec2(0.5, 0.5));\n\n    vec2 lightUv2 = vec2(\n        rotatedUv.x,\n        rotatedUv.y*0.01 + 0.35\n    );\n\n     float strengthY = 0.125 / distance(lightUv2, vec2(0.5, 0.5));\n     float strength = strengthX * strengthY;\n\n     //CLAMP STRNGTH\n    strength = clamp (strength, 0.0, 1.0);\n\n    //COLORED\n    vec3 firstColor = vec3(0.0345, 0.07, 0.11);\n    vec3 secondColor = vec3(vUV, 1.0);\n\n    vec3 color = mix(firstColor, secondColor, strength);\n\n\n    gl_FragColor = vec4(vec3(color), 1.0);\n    ```\n\n    This produces gradients that look like light flare like at the cover of this post.","src/content/blog/01-07-writing-shaders.mdx","f9e144d10fd9d0a3","01-08-first-wgsl-shader",{"id":30,"data":32,"body":37,"filePath":38,"digest":39,"deferredRender":19},{"title":33,"description":34,"pubDate":35,"heroImage":36},"My first WGSL shader","Understanding WGSL semantics",["Date","2025-01-07T23:00:00.000Z"],"shader-2.png","#### Meeting\n\nToday started with the first coach meeting. Wouter suggested to start with [WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html#toc), where I accidentally opened a lesson from the middle instead of the beginning.\n\n#### WebGPU Fundamentals && Tour of WGSL\nSeeing snippets of WGSL code I was quite lost, but then I saw [Tour of WGSL](https://google.github.io/tour-of-wgsl/).\nTo understand the context better, I have decided to try out WGSL directly, so I went through the half of the tour and saved some memroy anchors to my figma conspect:\n![WGSL Figma concept](/shaders-research-blog/images/conspect.png)\n\nTo get more comfortable with WGSL, I decided to use a turn a little example from **Book of Shaders into WGSL:**\n```javascript\n@binding(0) @group(0) var\u003Cuniform> frame : u32;\n\n@vertex\nfn vtx_main(@builtin(vertex_index) vertex_index : u32) -> @builtin(position) vec4f {\n  const pos = array(\n    vec2( 0.0,  1.0),\n    vec2( -1.0, -1.0),\n    vec2( 1.0, -1.0),\n  );\n\n  return vec4f(pos[vertex_index], 0, 1);\n}\n//https://thebookofshaders.com/08/\n// YUV to RGB matrix\nconst yuv2rgb = mat3x3f(1.0, 0.0, 1.13983,\n                    1.0, -0.39465, -0.58060,\n                    1.0, 2.03211, 0.0);\n\n@fragment\nfn frag_main() -> @location(0) vec4f {\n  var color : vec3f = yuv2rgb * vec3f(0.5, sin(f32(frame) / 128), 0.5);\n  return vec4(color, 1);\n}\n```\n\n**Multiplying colors by YUV matrix** turned from yellowish-greenish triangle into cyan, although it's not quite clear on the hero Image of the post, we all need to start somewhere=).\nOn my way there I also got a bit distracted with a [reminder how multiplying matrices works](https://mathinsight.org/matrix_vector_multiplication#:~:text=Matrix%2Dvector%20product&text=If%20we%20let%20Ax,na21a22%E2%80%A6), I had a vague memory from high school but wanted to understand the outcome better.\n\nThen I re-read Fundamentals articles again and I feel how it starts to layer in my head.\n\n#### Three.js Shading Language\n\nWhile trying to understand how WebGPU works, I wanted to make sure to understand the **difference between WGSL and TSL.** After scanning documentation and examples, I wasn't quite sure I understood it correctly. \nThe examples on three js website do contain TSL already used with WebGPU, but not shaders written in WGSl. Lately when I need to summarise new things I learned I ask ChatGpt to explain me concepts as an example of pizzeria:\n![TSL-pizza](/shaders-research-blog/images/TSL-pizza.png)\n\n\n#### Planning\nFor tomorrow I am planning to:\n- map out the roadmap;\n- start from [proper beginning of WebGPU Fundamentals](https://webgpufundamentals.org/webgpu/lessons/webgpu-fundamentals.html) again;\n- re-visit [this article about differences between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html), as it brings more clarity to me:\n![conspect-3](/shaders-research-blog/images/conspect-3.png)\nI also saved this comparison\n![conspect-2](/shaders-research-blog/images/conspect-2.png)\n\nBy the way, I just learned that actually its best practice **not to use ternary opeartors, or any flow controls,** when writing GLSL code as it's not good for performance, it might apply to WGSL as well.\n\n\n*My research might seem not linear right now, but I need to get the gist of the context and try out things to better understand the basics.*","src/content/blog/01-08-first-wgsl-shader.mdx","7e3330e00d2b632b","01-09-roadmap",{"id":40,"data":42,"body":47,"filePath":48,"digest":49,"deferredRender":19},{"title":43,"description":44,"pubDate":45,"heroImage":46},"Timeline","Timeplanning",["Date","2025-01-08T23:00:00.000Z"],"timeline.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Roadmap\n\nI tried to come up with a roadmap for my research, but for now I was able only to have very rough timeline:\n![timeline](/shaders-research-blog/images/timeline.png)\n\nI think of my features more of like happy accidents during research, but I will plan them more precisely this week for sure.\nMeanwhile, I also went back to the very beginning of WebGPU Fundamentals.\n\n#### WebGPU Fundamentals\n\nThe first thing that caught my eye was that **WebGPU is an asynchronous API** and is used inside of an **async function**.\nAnother thing was **requesting an adaper**:\n```javascript\nasync function main() {\n  const adapter = await navigator.gpu?.requestAdapter();\n  const device = await adapter?.requestDevice();\n  if (!device) {\n    fail('need a browser that supports WebGPU');\n    return;\n  }\n}\nmain();\n```\nThe adapter represents a specific GPU. Some devices have multiple GPUs.\n\nOkay, now an important takeaway:\n\u003CTakeaway>\n\"Shaders are written in a language called WebGPU Shading Language (WGSL) which is often pronounced wig-sil.\"\nI need to train it more: ***WIG-SIL***\n\u003C/Takeaway>\n\nBy the time I got to a red triangle on canvas, I had this conspect in Figma:\n\n![conspect](/shaders-research-blog/images/WebGPU-setup.png)\n\nNow it makes more sense, so we can move further to **running computations on the GPU.**","src/content/blog/01-09-roadmap.mdx","589fd2ba10444241","01-13-textures",{"id":50,"data":52,"body":57,"filePath":58,"digest":59,"deferredRender":19},{"title":53,"description":54,"pubDate":55,"heroImage":56},"Textures in WebGPU","Slowly going further",["Date","2025-01-12T23:00:00.000Z"],"textures-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Vertex and Storage buffers\nBefore going further to textures, I've went through storage & vertex buffers lessons.\nBuffer data is a new concept for me to use in coding, but it's one of essentials used in WGSL if you want to go further than triangles.\n\nI already used **Uniform Buffers** in previous lesson, now we're looking into **Storage** and **Vertex** buffers.\n\nOne of the main differences of Vertex buffer is how shaders access it. We need to explain to WebGPU what it is and how it's organized:\n\n![tell WebGPU how to supply data ](/shaders-research-blog/images/vertex-data-supply.png)\n\nTo the vertex entry of the pipeline descriptor we added a buffers array which is used to describe how to pull data out of 1 or more vertex buffers. For our first and only buffer, we set an arrayStride in number of bytes. A stride in this case is how many bytes to get from the data for one vertex in the buffer, to the next vertex in the buffer.\n\nAfter several examples I also understood how to use the stride offset when passing attributes to shader:\n```javascript\nbuffers: [\n                {\n                    arrayStride: 5 * 4, // 2 floats, 4 bytes each\n                    attributes: [\n                        { shaderLocation: 0, offset: 0, format: 'float32x2' },  // position\n                        { shaderLocation: 4, offset: 8, format: 'float32x3' },  // perVertexColor\n                    ],\n                },\n                ...\n]\n```\n![stride](/shaders-research-blog/images/stride.png)\n#### Index buffers\nFor optimizing vertices computaion, we can use index buffers to re-use existing vertices:\n![index buffers](/shaders-research-blog/images/index-buffers.png)\n\nThen the code turns into this, creating vertices and also **Index Data**:\n```javascript\nfunction createCircleVertices({\n    radius = 1,\n    numSubdivisions = 24,\n    innerRadius = 0,\n    startAngle = 0,\n    endAngle = Math.PI * 2,\n} = {}) {\n    // 2 vertices at each subdivision, + 1 to wrap around the circle.\n    const numVertices = (numSubdivisions + 1) * 2;\n    // const vertexData = new Float32Array(numSubdivisions * 2 * 3 * 2);\n    const vertexData = new Float32Array(numVertices * (2 + 3));\n\n    let offset = 0;\n    const addVertex = (x, y, r, g, b) => {\n        vertexData[offset++] = x;\n        vertexData[offset++] = y;\n        vertexData[offset++] = r;\n        vertexData[offset++] = g;\n        vertexData[offset++] = b;\n    };\n\n    const innerColor = [0.3, 0.3, 0.9];\n    const outerColor = [0.9, 0.9, 0.9];\n\n    // 2 triangles per subdivision\n    //\n    // 0  2  4  6  8 ...\n    //\n    // 1  3  5  7  9 ...\n    for (let i = 0; i \u003C= numSubdivisions; ++i) {\n        const angle = startAngle + (i + 0) * (endAngle - startAngle) / numSubdivisions;\n\n        const c1 = Math.cos(angle);\n        const s1 = Math.sin(angle);\n\n        addVertex(c1 * radius, s1 * radius, ...outerColor);\n        addVertex(c1 * innerRadius, s1 * innerRadius, ...innerColor);\n    }\n\n    const indexData = new Uint32Array(numSubdivisions * 6);\n    let ndx = 0;\n\n    // 1st tri  2nd tri  3rd tri  4th tri\n    // 0 1 2    2 1 3    2 3 4    4 3 5\n    //\n    // 0--2        2     2--4        4  .....\n    // | /        /|     | /        /|\n    // |/        / |     |/        / |\n    // 1        1--3     3        3--5  .....\n    for (let i = 0; i \u003C numSubdivisions; ++i) {\n        const ndxOffset = i * 2;\n\n        // first triangle\n        indexData[ndx++] = ndxOffset;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 2;\n\n        // second triangle\n        indexData[ndx++] = ndxOffset + 2;\n        indexData[ndx++] = ndxOffset + 1;\n        indexData[ndx++] = ndxOffset + 3;\n    }\n\n    return {\n        vertexData,\n        indexData,\n        numVertices: indexData.length,\n    };\n}\n```\nWe then set Index Buffer as well to pass our index data.\nWe also need to call different draw function:\n```javascript\n        pass.drawIndexed(numVertices, kNumObjects);\n```\nAnd we **saved 33% vertices**, slay!\nThis is what's we're getting by the way:\n![indexBuffer](/shaders-research-blog/images/indexBuffer.png)\n\nNow we're ready to go to **textures**.\n\n#### Textures\n\nOkay, we're finally at the last part of passing data into the WebGPU shaders!\n\nThe major difference for textures is that they are accessed by **sampler**, which can blend up to 16 values together in a texture.\nAs if there was not enough new information before yay....\n\nThe first thing I noticed when started following the lesson is **flipped textures,** which we faced a lot during Bachelor Thesis, and here was the eplanation for that:\n\n![indexBuffer](/shaders-research-blog/images/flipped-textures.png)\n\nIt was also nice finally to see familiar things about UV explanation and mixing, this is something I've encountered before and is easier for comprehension=)\nI think I finally understood meaning of **magFilter** and clamping textures to edge / repeating, which I saw in three js before very briefly:\n![magFilter](/shaders-research-blog/images/magFilter.png)\n\nIt's nice knowing that even if I am not writing shaders low-level daily, I understand how graphics api works in general better!","src/content/blog/01-13-textures.mdx","0cbdb7bcff64b522","01-10-computing-shaders",{"id":60,"data":62,"body":67,"filePath":68,"digest":69,"deferredRender":19},{"title":63,"description":64,"pubDate":65,"heroImage":66},"Computing shaders","Continiuing with WGSL",["Date","2025-01-09T23:00:00.000Z"],"vert-frag.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Computing Shaders\nToday I started with WebGPU Fundamentals again, going further to Computing shaders.\n\nThe first thing that is different from yesterdays **fragment** and **vertex** shaders is that we need **storage variable**:\n```javascript\n      @group(0) @binding(0) var\u003Cstorage, read_write> data: array\u003Cf32>;\n```\nThe thing that still confuses me a bit is *locations* in WGSL shaders:\n\"We tell it we’re going to specify this array on binding location 0 (the binding(0)) in bindGroup 0 (the @group(0)).\"\n\nThings get more interesting when we need to create separate buffers to **store** the data in and to **read** the data from:\n```javascript\n  //data for compute shader\n  const input = new Float32Array([1, 3, 5]);\n\n  //For WebGPU to use it, we need to make a buffer that exists on the GPU and copy the data to the buffer.\n\n  // create a buffer on the GPU to hold our computation\n  // input and output\n  const workBuffer = device.createBuffer({\n    label: 'work buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST,\n  });\n  // Copy our input data to that buffer\n  device.queue.writeBuffer(workBuffer, 0, input);\n\n  // create a buffer on the GPU to get a copy of the results\n  const resultBuffer = device.createBuffer({\n    label: 'result buffer',\n    size: input.byteLength,\n    usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST\n  });\n\n  // Setup a bindGroup to tell the shader which\n  // buffer to use for the computation\n  const bindGroup = device.createBindGroup({\n    label: 'bindGroup for work buffer',\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [\n      { binding: 0, resource: { buffer: workBuffer } },\n    ],\n  });\n  ```\n\n  #### WebGPU Inter-stage Variables\n\n  Moving on to WebGPU Inter-stage Variables, in next chapter I learned how to pass structures (of which I think kind of like js classes) between 2 shaders:\n\n  ```javascript\n        struct OurVertexShaderOutput {\n        @builtin(position) position: vec4f,\n        @location(0) color: vec4f,\n      };\n      @vertex fn vs(\n        @builtin(vertex_index) vertexIndex : u32\n     ) -> OurVertexShaderOutput {\n        let pos = array(\n          vec2f( 0.0,  0.5),  // top center\n          vec2f(-0.5, -0.5),  // bottom left\n          vec2f( 0.5, -0.5)   // bottom right\n        );\n\n        let color = array(\n          vec4f(1, 0, 0, 1), // red\n          vec4f(0, 1, 0, 1), // green\n          vec4f(0, 0, 1, 1), // blue\n        );\n \n        var vsOutput: OurVertexShaderOutput;\n        vsOutput.position = vec4f(pos[vertexIndex], 0.0, 1.0);\n        vsOutput.color = color[vertexIndex];\n        return vsOutput;\n      }\n \n       @fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {\n        let red = vec4f(1, 0, 0, 1);\n        let colored = fsInput.color;\n\n        let grid = vec2u(fsInput.position.xy) / 16;\n        let checker = (grid.x + grid.y) % 2 == 1;\n\n        return select(red, colored, checker);\n      }\n  ```\nHere I combined 2 examples to both **pass color from vertex shader to frag shader**, and to add **condition** to use the **passed color or red** in frag shader.\nThis is the outcome:\n![vertex-to-frag](/shaders-research-blog/images/vert-frag.png)\n\n#### Uniforms\nUniforms look just like in GLSL with some differences:\nto create a Uniform, first we need to descripe its' \"class\" aka struct:\n\n```javascript\n    struct OurStruct {\n        color: vec4f,\n        scale: vec2f,\n        offset: vec2f,\n      };\n```\n\nThen, we need to declare a uniform with type of our struct:\n\n```javascript\n@group(0) @binding(0) var\u003Cuniform> ourStruct: OurStruct;\n```\nand after this we can use uniforms in our shader code.\nTo be able to se the from Javascript, we also need to create a buffer first, and to calculate it's size:\n```javascript  \nconst uniformBufferSize =\n    4 * 4 + // color is 4 32bit floats (4bytes each)\n    2 * 4 + // scale is 2 32bit floats (4bytes each)\n    2 * 4;  // offset is 2 32bit floats (4bytes each)\n  const uniformBuffer = device.createBuffer({\n    label: 'uniforms for triangle',\n    size: uniformBufferSize,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n  ```\n  after playing with setting uniforms a bit, I got this beautiful rotating triangle:\n\n![rotating-trinagle](/shaders-research-blog/images/wgsl-uniforms.gif)","src/content/blog/01-10-computing-shaders.mdx","78a51b5f49614566","01-14-planning",{"id":70,"data":72,"body":77,"filePath":78,"digest":79,"deferredRender":19},{"title":73,"description":74,"pubDate":75,"heroImage":76},"Planning","What can I use from what I learnt?",["Date","2025-01-13T23:00:00.000Z"],"Roadmap.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### WebGPU Fundamentals\n\nI am finishing going through textures lessons, including using pictures and videos as textures. I don't think I am going to use loading images on this low level, as I am limited in time, I need to be able to think of the **best usage for shaders**, something you would **not be able** to do on **Three js / React Three fiber** level.\nPer my planning I need to start working on the set up and the scene tomorrow, so there are some things I will go through / revisit while working on my experience.\n\nDuring this passion project I will not look into creating 3d Math on shading languages level, as Three js can do nearly everything much easier.\nSo my plan for WebGPU fundamentals is:\n- Finish **textures** lessons (today)\n- Deeper than fundamental **[Compute shader lessons](https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html)**\n- Check **[bind group layouts](https://webgpufundamentals.org/webgpu/lessons/webgpu-bind-group-layouts.html),** I have a feeling I might need them\n- **Particles** lessons – [WebGPU Points](https://webgpufundamentals.org/webgpu/lessons/webgpu-points.html)\n- Revsisting [How WebGPU works](https://webgpufundamentals.org/webgpu/lessons/webgpu-how-it-works.html) and [difference between WebGL and WebGPU](https://webgpufundamentals.org/webgpu/lessons/webgpu-from-webgl.html)\n\n\nWhile doing so, there are things I know I will revisit anyway, such as [Memory layout](https://webgpufundamentals.org/webgpu/lessons/webgpu-memory-layout.html) and [Transparency and blending](https://webgpufundamentals.org/webgpu/lessons/webgpu-transparency.html)\n\n#### Planning usage\n\nEvery day that I read another article from WebGPU fundamentals, I kind of scroll through the whole website one more time to see what became clearer and if new knowledge filled some spots that were blank before=)\nIt's time to make decisions now! Based on what I learnt about WGSL and TSL, I mapped out my *features,* put short *explanation about each context* and specified which *shading language* I am thinking to use for that:\n\n![Roadmap](/shaders-research-blog/images/Roadmap.png)\n\nThe reason why I have **GLSL / WGSL** 3 times is because I am not sure what will be the better soultion at the moment and if I will be able to work with WGSL at the same level as GLSL. I am at least sure to highlight **compute shaders of WGSL** for my **room no. 3,** and to make use of **TSL** as more **textures / materials** for my **room no. 4.** As this whole project is a research proccess, these choices might change based on what I am going to learn on my way.\n\nThen I also added visual references for each location:\n\n**Intro and Corridor**\n![Corridor mood](/shaders-research-blog/images/corridor-mood.png)\n\n**Listening sphere**\n![Wobbly sphere mood](/shaders-research-blog/images/wobbly-sphere-mood.png)\n\n**Room with a conspiracy theory**\n![Conspiracy theory mood](/shaders-research-blog/images/conspiracy-theory-mood.png)\n\n**Distorted memories and outside outro**\n![Outro mood](/shaders-research-blog/images/memories-and-outro-mood.png)\n\n\n#### Back to today's programme\n\nI am finishing with textures, even though I don't think ia m goimng to load videos in WGSL, I went over the article, and the CubeMap article actually helped me to understand how normals abd environment map works.\nI also have better understanding how lighting works together with normals, this is quite helpful picture:\n\n![Cube normals](/shaders-research-blog/images/cube-normals.png)\n\nFollowing the lessons, I have created [several demos in my demo folder in Liminal Russia repo](https://github.com/eleonoradrykina/liminal-russia/tree/demos/demos).\nNow my brain is boiling with theory, and I want to switch to practice with some faster beautiful results:)","src/content/blog/01-14-planning.mdx","f853796bede9cc6b","01-15-gpgpu",{"id":80,"data":82,"body":87,"filePath":88,"digest":89,"deferredRender":19},{"title":83,"description":84,"pubDate":85,"heroImage":86},"GPGPU","Ping-pong shaders",["Date","2025-01-14T23:00:00.000Z"],"ping-pong.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### GPGPU\n\nSo to pick up where I left off, I want to understand how **Frame Buffers** and **Ping-ponging** works to be able:\n- compare it with WGSL compute shaders\n- work on my Memories Room no. 2\n\n\u003CTakeaway>\nSo, we have a texture in our **Frame Buffer Object** which we can use for our vertices position instead of simply rendering it, like we would do normally.\nBecause we **can not read and write** to the same Frame Buffer Object at the same time, we will update it and then exchange them to each other (*ping-ponging*):\n\u003C/Takeaway>\n\n![ping-pong](/shaders-research-blog/images/ping-pong.png)\n\nI feel that the most challenging thing will be to do the setup myself, as Bruno Simon's tutorial provided quite a ready solution with **GPUComputationRenderer**:\n```javascript\n/**\n * GPU Compute\n */\n//Setup\nconst gpgpu = {}\ngpgpu.size = Math.ceil(Math.sqrt(baseGeometry.count))\ngpgpu.computation = new GPUComputationRenderer(gpgpu.size, gpgpu.size, renderer) //square: width , height\n\n//Base particles\nconst baseParticlesTexture = gpgpu.computation.createTexture()\n\nfor (let i = 0; i \u003C baseGeometry.count; i++) {\n    const i3 = i * 3\n    const i4 = i * 4\n\n    // Position based on geometry\n    baseParticlesTexture.image.data[i4 + 0] = baseGeometry.instance.attributes.position.array[i3 + 0]\n    baseParticlesTexture.image.data[i4 + 1] = baseGeometry.instance.attributes.position.array[i3 + 1]\n    baseParticlesTexture.image.data[i4 + 2] = baseGeometry.instance.attributes.position.array[i3 + 2]\n    baseParticlesTexture.image.data[i4 + 3] = Math.random()\n\n}\n\nconsole.log(baseParticlesTexture)\n\n//Particles variable\ngpgpu.particlesVariable = gpgpu.computation.addVariable('uParticles', gpgpuParticlesShader, baseParticlesTexture)\ngpgpu.computation.setVariableDependencies(gpgpu.particlesVariable, [gpgpu.particlesVariable]) //you can pass more dependencies here\n\n// Uniforms\ngpgpu.particlesVariable.material.uniforms.uTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uDeltaTime = new THREE.Uniform(0)\ngpgpu.particlesVariable.material.uniforms.uBase = new THREE.Uniform(baseParticlesTexture)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldInfluence = new THREE.Uniform(0.5)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldStrength = new THREE.Uniform(2.0)\ngpgpu.particlesVariable.material.uniforms.uFlowFieldFrequency = new THREE.Uniform(0.5)\n\n\n//Init\ngpgpu.computation.init()\n\n```\nI will look into its [code](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js) more closely tomorrow, as Bruno mentioned, it's a good thing that it's [well-documented](https://github.com/mrdoob/three.js/blob/8286a475fd8ee00ef07d1049db9bb1965960057b/examples/jsm/misc/GPUComputationRenderer.js).\n\nFollowing the tutorial, we saved all vertices for our particles positions in a texture which was update using shader, and then retrieved this texture in our *actual* vertex shader.\nAnother thing for me to keep in mind that I am kind of used to having a lot of built-in things in shaders such as *modelMatrix* and *viewMatrix* and I am not sure you have them in WGSL (will see soon!).\n\n#### Tweaks\n\nOkay, we have a nice result, let's play with it just a little bit for now, and then also get back to it later!\n\nFirst, I used an AI-generated model from [Trellis](https://trellis3d.github.io/), which was extremely cool, I definetely want to try it out for my final experience result. \nThe model is a **bit uneven,** which **does not matter at all** when you use it as a geometry base for **points.**\nAfter some hassle I managed to get the vertex colors inside of my model (apparently it can be hidden in different attributes, not only *color*, but sometimes *color_1* etc)\n\nIt's also nice, because I will be able to add my own lighting in Blender and then bake it.\nThis is the outcome:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-flowfield-overview.mp4\">\u003C/video>\n\nOkay, but because my main vibe is quite depressing, I want this \"memory\" of the building to decay at some point, so the points will be *falling down*.\nAt first I was a bit silly and just put minus on the whole FlowField:\n\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time)),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (-1.0)* abs(normalize(flowField));\n\n```\nAs you can see, this makes points fly away down, but also towards negative X:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-wrong.mp4\">\u003C/video>\n\nSo actually we need to make sure that we only have negative Y in the flowfield:\n```javascript\n        //Flow field\n        vec3 flowField = vec3(\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 0.0, time)),\n            (-1.0)*abs(simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency+ 1.0, time))),\n            simplexNoise4d(vec4(particle.xyz * uFlowFieldFrequency + 2.0, time))\n        );\n\n    //normalize direction\n     flowField = (normalize(flowField));\n```\n\nNow it seems right:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/disintegration-right.mp4\">\u003C/video>\n\nAt this point I need to stop before I spend too much time tweaking this, and get back to it later when I will be working on the final result!\nNext step will switch to compute shaders abd particles in WGSL.","src/content/blog/01-15-gpgpu.mdx","a9a2000bfdd80432","01-16-compute-20",{"id":90,"data":92,"body":97,"filePath":98,"digest":99,"deferredRender":19},{"title":93,"description":94,"pubDate":95,"heroImage":96},"Compute shaders","More in-depth article about compute shaders in WGSL",["Date","2025-01-15T23:00:00.000Z"],"histogram-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Workgroups\n\nToday I am continuing on Compute Shaders more in depth.\nThe first thing I encountered was **workgroup**:\n\n![workgroup](/shaders-research-blog/images/workgroup.png)\n\nIf we then call pass.dispatchWorkgroups(4, 3, 2) we’re saying, execute a workgroup of 24 threads, 4 * 3 * 2 times (24)\n for a total of 576 threads.\n\n Compute shaders are better fit for GPGPU rather than GLSL with ping-pong buffers, but direct access \n to data buffers and organizing the computation that are used for compute shaders seems more complex at the first sight. (and at my second sight as well)\n\nMy first thought was to compare these **workgroups** to the **textures** that are used for data storage in ping-pong buffers (mostly because of the visual representation), but this is **wrong** comparison.\n\u003CTakeaway>\n    **Workgroups** is a way of organizing parallel computation. Data is stored in buffers, which we bind together and set to the pipeline. \n    **Textures** however, actually store data.\n\u003C/Takeaway>\n\nAnother important concept is **decoupling from the graphics pipeline**\n\u003CTakeaway>\n    WGSL execution model is independent of rendering. Compute shaders can work on **any kind of data**, not just pixels or vertices. Its outputs are stored in **storage buffers** or **textures** on GPU.\n\u003C/Takeaway>\n\nThese textures and storage buffers can be then fed to fragment or vertex shaders.\n\nThe next part of the article was coding the histogram using compute shader:\n![histogram](/shaders-research-blog/images/histogram.png)\n\nHere we loaded the image texture pixel by pixel in compute shader, counted its luminance and then saved into the histogram array, to be able to show it on another canvas:\n```javascript\n  @group(0) @binding(0) var\u003Cstorage, read_write> bins: array\u003Cu32>;\n      @group(0) @binding(1) var ourTexture: texture_2d\u003Cf32>;\n\n      // from: https://www.w3.org/WAI/GL/wiki/Relative_luminance\n      const kSRGBLuminanceFactors = vec3f(0.2126, 0.7152, 0.0722);\n      fn srgbLuminance(color: vec3f) -> f32 {\n        return saturate(dot(color, kSRGBLuminanceFactors));\n      }\n\n      @compute @workgroup_size(1) fn cs() {\n        let size = textureDimensions(ourTexture, 0);\n        let numBins = f32(arrayLength(&bins));\n        let lastBinIndex = u32(numBins - 1);\n        for (var y = 0u; y \u003C size.y; y++) {\n          for (var x = 0u; x \u003C size.x; x++) {\n            let position = vec2u(x, y);\n            let color = textureLoad(ourTexture, position, 0);\n            let v = srgbLuminance(color.rgb);\n            let bin = min(u32(v * numBins), lastBinIndex);\n            bins[bin] += 1;\n          }\n        }\n      }\n```\n\nInteresting thing is following:\n*\"Timing the results I found this is about 30x slower than the JavaScript version!!! 😱😱😱 (YMMV).\"*\nIt happens because we do not use *the full power* of GPU in this example as we compute this pixels one by one in 1 loop.\nBasically just like CPU, but seven slower as one GPU is slower than CPU.\n\nSo what we can do here is to use more GPU invocations, and this is very example of using workgroups.\nWe changed 1 workgroup per pixel, used position as ``global_invocation_id.xy`` instead of calculating it, and passed the amount of workgroups equal to amount of pixels:\n``pass.dispatchWorkgroups(texture.width, texture.height)``.\n\nThe result was that now histograms are always a bit different every time you re-calculate them...\nThis is actually very interesting because it's a very clear example of **race condition**, and finally I am starting to understand it yay=)\n\u003CTakeaway>\nBecause there are so many calculations happening in parallel, some of them are taking the same value in our bins array and try to write their values into the same position.\nDepending on which invocation \"wins\", the histogram changes all the time.\n\u003C/Takeaway>\n\nTo avoid this, we can use **atomicAdd**.\n\n*NOTE:* Atomic functions have the requirement that they only work on i32 or u32 and they require to data itself to be of type atomic.\n\nWhile previous ``bins[bin] += 1;`` was calling load->add->store while executing, **atomic** function will execute all 3 operations at once (probably like an atom consisting of smaller particles thus the naming?).\nSo I've changed my bins to type atomic and in the shader changed adding function to this:\n``atomicAdd(&bins[bin], 1u);``\n\n*every time it actually works I sigh in relief*\n\nBut now there's another problem –– apparently this atomicAdd **blocks** other invocations from adding their data to the bins.\nHere is the image to see this more clear:\n![locking-bin](/shaders-research-blog/images/locking-bin.png)\nThe code works fine, but it could be faster.\nTo achieve that, we need to break up our storage into workgroups' storage. This way we can make our bins to be shared only with invocations in the same workgroup.\nWhen each workgroup has chunks of their own data, we need to write a new shader to collect all this data into one place.\n\nIt then got interestin to the place where the setup changes to 2 pipelines for 2 compute shaders:\n\n```javascript\n  const histogramChunkPipeline = device.createComputePipeline({\n    label: 'histogram',\n    layout: 'auto',\n    compute: {\n      module: histogramChunkModule,\n    },\n  });\n \n  const chunkSumPipeline = device.createComputePipeline({\n    label: 'chunk sum',\n    layout: 'auto',\n    compute: {\n      module: chunkSumModule,\n    },\n  });\n```\n\nThen I have set to BindGroups:\n```javascript\n //BINDING ALL STORAGE BUFFERS AND TEXTURE\n // ONE BINDGROUP PER ONE PASS\n    const histogramBindGroup = device.createBindGroup({\n        label: 'histogram bindGroup',\n        layout: histogramChunkPipeline.getBindGroupLayout(0),\n        entries: [\n            { binding: 0, resource: { buffer: chunksBuffer } },\n            { binding: 1, resource: texture.createView() },\n        ],\n    });\n\n    const sumBindGroups = [];\n    const numSteps = Math.ceil(Math.log2(numChunks));\n    for (let i = 0; i \u003C numSteps; ++i) {\n        const stride = 2 ** i;\n        const uniformBuffer = device.createBuffer({\n            size: 4,\n            usage: GPUBufferUsage.UNIFORM,\n            mappedAtCreation: true,\n        });\n        new Uint32Array(uniformBuffer.getMappedRange()).set([stride]);\n        uniformBuffer.unmap();\n\n        const chunkSumBindGroup = device.createBindGroup({\n            layout: chunkSumPipeline.getBindGroupLayout(0),\n            entries: [\n                { binding: 0, resource: { buffer: chunksBuffer } },\n                { binding: 1, resource: { buffer: uniformBuffer } },\n            ],\n        });\n        sumBindGroups.push(chunkSumBindGroup);\n    }\n```\n\nand then we make 2 passes to the encoder:\n\n```javascript\n    //FIRST PASS\n    pass.setPipeline(histogramChunkPipeline);\n    pass.setBindGroup(0, histogramBindGroup);\n    pass.dispatchWorkgroups(chunksAcross, chunksDown);\n\n    //SECOND PASS\n    pass.setPipeline(chunkSumPipeline);\n    let chunksLeft = numChunks;\n    sumBindGroups.forEach(bindGroup => {\n        pass.setBindGroup(0, bindGroup);\n        const dispatchCount = Math.floor(chunksLeft / 2);\n        chunksLeft -= dispatchCount;\n        pass.dispatchWorkgroups(dispatchCount);\n    });\n    pass.end();\n```\nAnd it works!\n\nNow the speed is the most optimized, here's a table from the article conparing all options we tried:\n![speed-table](/shaders-research-blog/images/speed-table.png)\n\nThe thing is that while I understood pipelines and buffers much better now, there's a new thing with breaking chunks into workgroups and also calculating the optimal subdivisions...\nThere's the last part to this article left, and then I will try to use it in different scenarios.","src/content/blog/01-16-compute-20.mdx","000852924ec954e7","01-17-compute-lynch",{"id":100,"data":102,"body":107,"filePath":108,"digest":109,"deferredRender":19},{"title":103,"description":104,"pubDate":105,"heroImage":106},"Compute shaders with David Lynch","Going further with histogram",["Date","2025-01-16T23:00:00.000Z"],"twin-peaks-curtains.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n***Yesterday, 16 January 2025, the world lost a pure genius and a great visionary. David Lynch taught me to love the beauty of abstract and not to look for answers but for the right questions.\nHis art shaped me in so many different ways, and I am pretty sure this project is also partly inspired by his love of the absurd.\nToday's post will be a tribute to David Lynch who is always in my mind.***\n\n\n#### From Compute shader to Frag Shaders\nToday while working on histogram 2.0, we are finally passing the data from compute shaders to rendering shaders.\nThis is what I was waiting for as it's closer to real-life example!\n\nWe calculate the data for histogram in one compute shader and store the data:\n```javascript\nconst bindGroup = device.createBindGroup({\n    layout: drawHistogramPipeline.getBindGroupLayout(0),\n    entries: [\n        { binding: 0, resource: { buffer: chunksBuffer, size: chunkSize * 4 * 4 } },\n        { binding: 1, resource: { buffer: uniformBuffer } },\n        { binding: 2, resource: { buffer: scaleBuffer } },\n    ],\n});\n```\nWe also added one more compute shader to calculate heights of each histogram bar, which I am not going to put here otherwise it's too much code.\nThe most important part that we also get **scale** from there. \nAnd then we pass everyting into the Fragment shader:\n```javascript\n    const bindGroup = device.createBindGroup({\n    layout: drawHistogramPipeline.getBindGroupLayout(0),\n    entries: [\n        { binding: 0, resource: { buffer: chunksBuffer, size: chunkSize * 4 * 4 } },\n        { binding: 1, resource: { buffer: uniformBuffer } },\n        { binding: 2, resource: { buffer: scaleBuffer } },\n    ],\n});\n```\nThe outcome is the same histograms, but  drawn in GPU instead of javascript:\n![laura-dern-histogram](/shaders-research-blog/images/inland-empire-laura-histogram.png)\n*Laura Dern in Inland Empire, 2006*\n\nTo sum up: we have **3 compute pipelines** here: \n- `histogramChunkPipeline` where we calculate histogram chunk by chunk;\n- `chunkSumPipeline` where we gather all the histogram chunks from workroups into one chunk;\n- `scalePipeline` where we compute the height of each bar of histogram;\n\nand **1 render pipeline**\n- `drawHistogramPipeline`, where we read histogram data chunks and scale data and based on that render the histogram.\n\nNow can we do that on the same canvas on each frame?\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/twinpeaks-mirror-histogram.mp4\">\u003C/video>\n*Dale Cooper becomes possesed by Bob. Ending of Twin Peaks, Season 2, 1991*\n\nHere we call our render function on each frame using well-known `requestAnimationFrame(render)`.\nBindgroups are also now created on each frame inside of render function.\n\nAn inportant moment is that we also created a **separate render pipeline** to render a video:\n```javascript\n    const videoPipeline = device.createRenderPipeline({\n        label: 'hardcoded video textured quad pipeline',\n        layout: 'auto',\n        vertex: {\n            module: videoModule,\n        },\n        fragment: {\n            module: videoModule,\n            targets: [{ format: presentationFormat }],\n        },\n    });\n\n    const videoSampler = device.createSampler({\n        magFilter: 'linear',\n        minFilter: 'linear',\n    });\n    ```\nAnd this is the end of WebGPU Fundamentals compute shaders articles!\nNow lets try to tweak the given example ourselves.\n\nAt first, I got rid of histogram and tried to make half of the video black and white:\n![black and white](/shaders-research-blog/images/bw-twin-peaks.png)\n\nYes, I know, we don't need compute shader for that and this can be done in Frag shader, but we're getting there.\n\nNext, I changed the chunkSizes and the asked to make only red-ish pixels to become black and white (which in this case worked as blue segmentation).\nThis is my new compute shader:\n```javascript\nconst k = {\n        chunkWidth: 16,\n        chunkHeight: 16,\n    };\n    const chunkSize = k.chunkWidth * k.chunkHeight;\n    const sharedConstants = Object.entries(k).map(([k, v]) => `const ${k} = ${v};`).join('\\n');\n\n    const bwChunkModule = device.createShaderModule({\n        label: 'bw shader',\n        code: `\n        ${sharedConstants}\n\n        struct Chunk {\n            pixels: array\u003Cvec4f, ${chunkSize}>\n        };\n\n        @group(0) @binding(0) var ourTexture: texture_external;\n        @group(0) @binding(1) var outputTexture: texture_storage_2d\u003Crgba8unorm, write>;\n        @group(0) @binding(2) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n\n        const kSRGBLuminanceFactors = vec3f(0.2126, 0.7152, 0.0722);\n        fn srgbLuminance(color: vec3f) -> f32 {\n            return saturate(dot(color, kSRGBLuminanceFactors));\n        }\n\n        @compute @workgroup_size(chunkWidth, chunkHeight, 1)\n        fn cs(\n            @builtin(global_invocation_id) global_id: vec3u,\n            @builtin(workgroup_id) workgroup_id: vec3u,\n            @builtin(local_invocation_id) local_id: vec3u,\n        ) {\n            let size = textureDimensions(ourTexture);\n            let position = global_id.xy;\n\n            if (all(position \u003C size)) {\n            var color = textureLoad(ourTexture, position);\n\n        // Check if the pixel is in the left half of the video\n        // if (position.x \u003C size.x / 2u) {\n            //check if the pixel is red\n            if (color.r > color.g && color.r > color.b) {\n                let luminance = srgbLuminance(color.rgb);\n                color = vec4f(luminance, luminance, luminance, color.a);\n            }\n        //}\n\n        // Store in chunk\n        let chunk_idx = workgroup_id.y * (size.x / chunkWidth) + workgroup_id.x;\n        let pixel_idx = local_id.y * chunkWidth + local_id.x;\n        chunks[chunk_idx].pixels[pixel_idx] = color;\n\n\n        // Write to output texture\n            textureStore(outputTexture, position, color);\n        }\n         }\n        `,\n    });\n\n```\nand this is the outcome:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/blue-segmentation.mp4\">\u003C/video>\n\nOkay, but what's the point of compute shaders if we don't store anything between the frames?\nAs per Wouter's consult I will be looking into post-processing, it's a nice moment to creat a little post-processing myself.\n\nI think our Twin Peaks finale would benefit from a little **motion blur.** This is purely to get better understanding how wgsl compute shaders work, so I am not even googling how it actually works,\nmy guess is to store previous frames and blend them with the current one.\n\nI have started with storing 4 frames and did not see any result.\n\nMaybe we need to store more?\n\n```javascript\n struct Chunk {\n            frame0: array\u003Cvec4f, ${chunkSize}>,  // Most recent previous frame\n            frame1: array\u003Cvec4f, ${chunkSize}>,\n            frame2: array\u003Cvec4f, ${chunkSize}>,\n            frame3: array\u003Cvec4f, ${chunkSize}>,\n            frame4: array\u003Cvec4f, ${chunkSize}>,\n            frame5: array\u003Cvec4f, ${chunkSize}>,\n            frame6: array\u003Cvec4f, ${chunkSize}>,\n            frame7: array\u003Cvec4f, ${chunkSize}>,\n            frame8: array\u003Cvec4f, ${chunkSize}>,\n            frame9: array\u003Cvec4f, ${chunkSize}>   // Oldest frame\n        };\n```\n\nAt this point I got an error that my buffer is a little bit too big:\nBuffer size (279183360) exceeds the max buffer size limit (268435456).\n\nSo it means you probably can store around 9 pictures in one buffer which is quite a lot!\n\nThen I realized that the problem is not how many frames we take, but the **difference** between them.\nSo I want to check which frame is that and save every n-th frame:\n\n```javascript\n\n    const motionBlurModule = device.createShaderModule({\n        label: 'motion blur shader',\n        code: `\n        ${sharedConstants}\n\n        struct Settings {\n            frameInterval: u32,\n            currentFrameWeight: f32,\n            frame0Weight: f32,\n            frame1Weight: f32,\n            frame2Weight: f32,\n            frame3Weight: f32,\n        };\n\n        struct Chunk {\n            frame0: array\u003Cvec4f, ${chunkSize}>,  // Most recent saved frame\n            frame1: array\u003Cvec4f, ${chunkSize}>,\n            frame2: array\u003Cvec4f, ${chunkSize}>,\n            frame3: array\u003Cvec4f, ${chunkSize}>   // Oldest saved frame\n        };\n\n        @group(0) @binding(0) var ourTexture: texture_external;\n        @group(0) @binding(1) var outputTexture: texture_storage_2d\u003Crgba8unorm, write>;\n        @group(0) @binding(2) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n        @group(0) @binding(3) var\u003Cstorage, read_write> frameCounter: array\u003Cu32, 1>;\n        @group(0) @binding(4) var\u003Cuniform> settings: Settings;\n\n        @compute @workgroup_size(chunkWidth, chunkHeight, 1)\n        fn cs(\n            @builtin(global_invocation_id) global_id: vec3u,\n            @builtin(workgroup_id) workgroup_id: vec3u,\n            @builtin(local_invocation_id) local_id: vec3u,\n        ) {\n            let size = textureDimensions(ourTexture);\n            let position = global_id.xy;\n\n            if (all(position \u003C size)) {\n                let chunk_idx = workgroup_id.y * (size.x / chunkWidth) + workgroup_id.x;\n                let pixel_idx = local_id.y * chunkWidth + local_id.x;\n                \n                let currentColor = textureLoad(ourTexture, position);\n                \n                // Update frame counter with single thread\n                if (global_id.x == 0u && global_id.y == 0u) {\n                    frameCounter[0] = frameCounter[0] + 1u;\n                }\n\n                // Store frames every N frames for all pixels\n                if (frameCounter[0] % settings.frameInterval == 0u) {\n                    let prev0 = chunks[chunk_idx].frame0[pixel_idx];\n                    let prev1 = chunks[chunk_idx].frame1[pixel_idx];\n                    let prev2 = chunks[chunk_idx].frame2[pixel_idx];\n\n                    chunks[chunk_idx].frame3[pixel_idx] = prev2;\n                    chunks[chunk_idx].frame2[pixel_idx] = prev1;\n                    chunks[chunk_idx].frame1[pixel_idx] = prev0;\n                    chunks[chunk_idx].frame0[pixel_idx] = currentColor;\n                }\n                \n                let prev0 = chunks[chunk_idx].frame0[pixel_idx];\n                let prev1 = chunks[chunk_idx].frame1[pixel_idx];\n                let prev2 = chunks[chunk_idx].frame2[pixel_idx];\n                let prev3 = chunks[chunk_idx].frame3[pixel_idx];\n\n                let blendedColor = currentColor * settings.currentFrameWeight +\n                                  prev0 * settings.frame0Weight +\n                                  prev1 * settings.frame1Weight +\n                                  prev2 * settings.frame2Weight +\n                                  prev3 * settings.frame3Weight;\n\n                textureStore(outputTexture, position, blendedColor);\n            }\n        }\n        `,\n    });\n```\n\nI also added gui to be able to change the step between frames.\nOn the maximum threshold around 120 frames we get this beautiful trippy not-motion-blur:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/motion-blur.mp4\">\u003C/video>\n\nI am quite happy with the result! Next I will try to use WGSL in more complex setups and finally start working on the setting up the scene for the experience.","src/content/blog/01-17-compute-lynch.mdx","e842fb24cfe7d992","01-18-particles-wgsl",{"id":110,"data":112,"body":117,"filePath":118,"digest":119,"deferredRender":19},{"title":113,"description":114,"pubDate":115,"heroImage":116},"WGSL Particles","going towards particle system",["Date","2025-01-17T23:00:00.000Z"],"particles-hero.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### WebGPU Points\n\nMy favorite thing about vertex and compute shaders is to create **particles systems that look alive.**\nFor my \"Conspiracy Theory\" room I want to have a mysterious particles system that will look like it has hidden answers to all the questions. It has its roots in \"kitchen talks\" in USSR era, where the only place where people could freely express their opinions were their kitchens.\nIt later has morphed into drunk kitchen talks \"for grown-ups\" for my generation, during which some of participants were claiming they have full understanding and explanation to the current political situation. Most of the times it was United States trying to sabotage the prosperity of Russians and hypnotize them into zombies.\n\nBut enough of history, let's code!\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-sphere.mp4\">\u003C/video>\nI followed particles article quite close, but added a twist with using **alha-map ![alpha-map](/shaders-research-blog/images/texture-1.png) in Frag shader** to make my particles fancier:\n\n\n```javascript\n        @fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {\n            // use texture as transparency mask\n            let mask = textureSample(t, s, vsOut.texcoord);\n            // Pre-multiply the RGB color by the alpha value\n            let alpha = mask.r * 1.0;  // here it can be even more transparent if needed\n            return vec4f(vec3f(0.4, 0.4, 0.9) * alpha, alpha);\n        }\n```\n\nCan we make things more interesting with a compute shader?\n\nAfter 1,5 hours of adding compute shader **just to achieve the same sphere**, I got my sphere a bit creased 😂😂:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/particles-creased.mp4\">\u003C/video>\n\nI think it might has to do with messing up with my positions array indexation.\nAha! I was creating **vec3** for my initial particles, but then I went a little bit too fast and thinking about **life span** was treating them as vec4.\nFor now I have updated the function that creates sphere geometry to return **vec4.**\n\nSo now we have totally the same sphere (I've changed the color so it's more pleasant to look at), but with retrieving positions from compute shader:\n![same sphere but comoute shader](/shaders-research-blog/images/compute-sphere.png)\n\n\u003CTakeaway>\nNow instead of passing initial positios array, we can play with poistions first and pass them into the vertex.\nI keep track of distance (not sure if it's a good solution right now), and when particles are too different from initial, I reset them.\n\u003C/Takeaway>\n\nHere's code of my new compute shader:\n\n```javascript\n        ${sharedConstants}\n        struct Particle {\n            pos: vec4f,    // xyz for position, w for lifetime/age (not used yet)\n        }\n\n        struct Chunk {\n            particles: array\u003CParticle, 1000>,      // Current state\n            prevParticles: array\u003CParticle, 1000>,  // Previous state\n            initialPos: array\u003CParticle, 1000>,     // Initial/reset positions\n        };\n\n        struct Uniforms {\n            matrix: mat4x4f,\n            resolution: vec2f,\n            size: f32,\n            time: f32,\n        };\n\n        @group(0) @binding(0) var\u003Cstorage, read_write> chunks: array\u003CChunk>;\n        @group(0) @binding(1) var\u003Cuniform> uni: Uniforms;\n\n        @compute @workgroup_size(64)\n        fn cs(@builtin(global_invocation_id) id: vec3u) {\n            if (id.x >= 1000) {\n                return;\n            }\n\n            let i = id.x;\n            let initialPos = chunks[0].initialPos[i].pos.xyz;\n            let distance = length(chunks[0].particles[i].pos.xyz - initialPos);\n\n            if (distance \u003C 5.0) {\n                var newPos = chunks[0].particles[i].pos;\n                // Using time to create some distortion\n                newPos.x += sin(uni.time + f32(i) * 0.01) * 0.0001;\n                newPos.y += cos(uni.time * 0.5 + f32(i) * 0.01) * 0.0001;\n                chunks[0].particles[i].pos = newPos;\n            } else {\n                chunks[0].particles[i].pos = vec4f(initialPos, 1.0);\n            }\n\n            chunks[0].prevParticles[i] = chunks[0].particles[i];\n        }\n```\n\nThis creates this beautiful swirl:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/compute-particles.mp4\">\u003C/video>\n\nNext step will be to tune persistance and to get better control over understanding of vrtices positions, but basic setup is quite done!","src/content/blog/01-18-particles-wgsl.mdx","27b8b782eeb90c90","01-19-resources-update",{"id":120,"data":122,"body":127,"filePath":128,"digest":129,"deferredRender":19},{"title":123,"description":124,"pubDate":125,"heroImage":126},"Resources Update","dropping more interesting links here",["Date","2025-01-18T23:00:00.000Z"],"okay-dev.jpeg","On Sunday I combed through Twitter and fished out some links that *seem useful* at the first glance.\nI have not read them closely yet, only scanned through but I just want to dump everything I found here, just in case:\n\n| SL / API    | Links |\n| -------- | ------- |\n| WGSL  / WebGPU  | [Okay Dev article about WebGPU](https://okaydev.co/articles/dive-into-webgpu-part-1)   |\n| WGSL  / WebGPU  | [React Three Fiber WebGPU Post Processing](https://github.com/ektogamat/r3f-webgpu-starter) (probably just a set up with no WGSl involved)|\n| WebGPU   | [ Interacting with meshes in WebGPU ](https://codesandbox.io/p/sandbox/patient-lake-9sh577?file=%2Fsrc%2FApp.js)|\n| TSL    | [ Editor that generates TSL code](https://tsl-editor.vercel.app/) |\n| TSL  | [Youtube tutorial of Yuri Artiukh](https://www.youtube.com/live/Wgz3u-6tg28?si=cQtVlmS1eQDNip5B) (I actually follow him nearly everywhere but did not get a chance to use his tutorials)  |\n| TSL  | [ 2D Clouds in TSL]( https://niklever.com/get-to-grips-with-threejs-shading-language-tsl-part-13-2d-clouds/)  |\n\n\n*I noticed that Okay Dev use Megazoid font that I used for my NailBro project during first year, so I already trust them.*","src/content/blog/01-19-resources-update.mdx","edcb032b858fc4ea","01-20-building-up-scene",{"id":130,"data":132,"body":137,"filePath":138,"digest":139,"deferredRender":19},{"title":133,"description":134,"pubDate":135,"heroImage":136},"Building Up the Scene","Its time to build!",["Date","2025-01-19T23:00:00.000Z"],"corridor.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nOkay, it's time to set up the scene! Enough shaders for now, let's think ***spatially.***\nMy main concern was how to let user move inside of my corridor kid of freely, without coding camera path?\nBack then I imagined it as BackRooms where you experience everything from first person POV, so I was in terms of camera movement only:\n\n![Backrooms_model](/shaders-research-blog/images/Backrooms_model.jpg)\n\nThen I saw [this tutorial](https://threejs-journey.com/lessons/create-a-game-with-r3f#) of Bruno Simon where he had physics and an actual player:\n![Bruno Simon's game](/shaders-research-blog/images/bruno-simon-game.png)\n\n\u003CTakeaway>\nNot only it makes much more sense to put **physics colliders onto walls and floor** but also having an **actual player** helps to kind of \"see yourself\" in the environment.\nI think I was a little bit too fixated on that reference, anyway it is an inspiration and I am not building exact same copy, so why not have some fun?\n\u003C/Takeaway>\n\nI have changed the \"player\" to the capsule so fun and it's kind of entertaining how it's being pushed and thrown into sides, I was thiking maybe later I will add a model of some can instead of it.\n\nAlso I want to create a feeling of *impossible geometry*, so I'm going to move my player to another spot unnoticeably (hopefully) so that they just continue navigating thinking that they are in an infinite loop.\nAt first I drew a simple loop on paper:\n\n![corridor-sketch](/shaders-research-blog/images/corridor-sketch.jpg)\n\nSo after following Three.js Journey tutorial I was excited to start: I've put my own, longer corridor with the ceiling and basic colors for now and this is how it looks:\n![First iteration](/shaders-research-blog/images/corridor.png)\n\nI switch between normal OrbitControls to get the whole overview while building the \"architecture\" of the scene, and player POV to see how it feels.\nOf course, there are a lot of things that would be tweaked in future like textures and light, but I already see one big difference...\n\nFor some reason I always refered to this part of scene as \"corridor\" and imagined a corridor in my mind, but what does actually create this uncanny atmosphere in Backrooms?\nLet's see the reference again:\n![Backrooms_model](/shaders-research-blog/images/Backrooms_model.jpg)\n![Severance](/shaders-research-blog/images/Severance.webp)\n\nIt's not **just a hallway**, it's a structure that is off to a human proportion, big, empty and repetitive. If you see the corridor that just goes straight and then maybe turns, how scary it can be?\nOn the other hand, the more empty space you **don't see**, the more possibility there's something evil there.\n\nSo let's add more space and re-think the architecture:\n![architecture-iteration-2l](/shaders-research-blog/images/architecture-iteration-2.png)\n*Debug version with no ceiling and no player*\n\nNoow we're getting there!\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/gameplay-1.mp4\">\u003C/video>\n*Player POV*\n\nFrom here I also might try to make the geometry more complicated.\nNext step will be to think of **360 camera rotation**, adding **lights** and **textures**!","src/content/blog/01-20-building-up-scene.mdx","16db4c7b954addf7","01-21-backrooms",{"id":140,"data":142,"body":146,"filePath":147,"digest":148,"deferredRender":19},{"title":133,"description":143,"pubDate":144,"heroImage":145},"Working on the experience environment",["Date","2025-01-20T23:00:00.000Z"],"floor-plan-react.png","import Takeaway from \"../../components/Takeaway.astro\";\n\n#### Cleaning up the code\nFirst thing was to make my code a bit more clear and sustainable after all manual tweaks.\n\nI re-drew the plan and separated the walls into groups for convenience:\n![new floor plan](/shaders-research-blog/images/floor-plan-2.jpg)\n\nHere how it looks in debug mode: \n![new floor plan react](/shaders-research-blog/images/floor-plan-react.png)\n\n```javascript\n\n    export default function Corridor() {\n\n    return \u003C>\n        \u003CFloor size={ 16 }/>\n        {/* GROUP A*/}\n        \u003CWallHorizontal position={ [-9.7, 0.75, -4] } length = {12} />\n        \u003CWallVertical position={ [-3.85, 0.75, -8.0] }length = {8}/>\n\n        {/* GROUP B*/}\n        \u003CWallHorizontal position={ [-7.0, 0.75, -20] }length = {10 } />\n        \u003CWallVertical position={ [-6.6, 0.75, -25.0] }length = {9.75}/>\n\n        {/* GROUP C*/}\n        \u003CWallHorizontal position={ [12.7, 0.75, -22] } length = {6}/>\n        \u003CWallVertical position={ [6.6, 0.75, -20.65] }length = {13}/>\n        \u003CWallHorizontal position={ [8.7, 0.75, -14] } length = {14}/>\n\n        {/* GROUP D*/}\n        \u003CWallVertical position={ [6.6, 0.75, -2.15] }length = {8}/>\n\n        \u003CBoundsForward length={ 16} />\n        {/* \u003CCeiling size={ 16 }/> */}\n    \u003C/>\n}\n```\n\n#### Camera movement \nMy next question is how I move the camera?\nI am using Rapier with R3F for moving my capsule around. Physics engine and ``\u003CRigidBody/>`` prevents capsule from going into the walls so I don't have to think about coding the bounds myself.\nRight now the camera always follows the capsule and looks at it. This creates feeling of looking only straight ahead.\nBut my experience is very spatial, so we need to let the user to be able to look around.\nThis is the code for the camera I started with:\n```javascript\n...\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n        const cameraPosition = new THREE.Vector3()\n        cameraPosition.copy(bodyPosition)\n        cameraPosition.z += 2.25\n        cameraPosition.y += 0.65\n\n        const cameraTarget = new THREE.Vector3()\n        cameraTarget.copy(bodyPosition)\n        cameraTarget.y += 0.25 \n        cameraTarget.z -= 2.25\n\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n\n        state.camera.position.copy(smoothedCameraPosition)\n        state.camera.lookAt(smoothedCameraTarget)\n```\n\nAt first, I added looking to the sides while going sideways, adding CameraRotation together with the impulse:\n```javascript\n        /**\n         * Controls\n         */\n        const { forward, backward, left, right, jump } = getKeys()\n\n        const impulse = { x: 0, y: 0, z: 0 }\n        const cameraRotation = { x: 0, y: 0, z: 0 }\n\n\n        const impulseStrength = 0.1 * delta\n\n        if (forward) {\n                impulse.z -= impulseStrength\n            }\n        if (backward) {\n                impulse.z += impulseStrength\n            }\n        if (left) {\n                impulse.x -= impulseStrength\n                cameraRotation.x -= 1.5\n            }\n        if (right) {\n                impulse.x += impulseStrength\n                cameraRotation.x += 1.5\n            }\n            \n        body.current.applyImpulse(impulse)\n\n\n        /**\n         * Camera\n         */\n        const bodyPosition = body.current.translation()\n\n        const cameraPosition = new THREE.Vector3()\n        cameraPosition.copy(bodyPosition)\n        cameraPosition.z += 2.25\n        cameraPosition.y += 0.65\n\n        const cameraTarget = new THREE.Vector3()\n        cameraTarget.copy(bodyPosition)\n        cameraTarget.y += 0.25 \n        cameraTarget.z -= 2.25\n        cameraTarget.x += cameraRotation.x\n\n        \n        smoothedCameraPosition.lerp(cameraPosition, 5 * delta)\n        smoothedCameraTarget.lerp(cameraTarget, 5 * delta )\n```\nBut this gave very limited field of view which also did not work smoothly with the movement.\n\nThen, I thought what if detach LookAt from the capsule for a second? But then the position is still updating each frame causing this camera shaking:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/camera-LAT-bug.mp4\">\u003C/video>\n\nThen I realised: we need to attach the camera to the capsule **only when we're moving** the capusle, letting user to look around when we're standing at the same place.\n\nSo I've added updating camera on Each frame only if we're moving:\n```javascript\n        if (forward || backward || left || right || jump) {\n            state.camera.position.copy(smoothedCameraPosition)\n            state.camera.lookAt(smoothedCameraTarget)\n        }\n```\n\nAnd set the bounds on the rbitControls because it is now enabled when the user does not move:\n```javascript\n        \u003COrbitControls \n            enablePan={ false }\n            enableZoom={ false }\n            maxPolarAngle={ Math.PI / 2 }\n            minPolarAngle={ Math.PI / 2 }\n            maxAzimuthAngle={ Math.PI / 2 }\n            minAzimuthAngle={ -Math.PI / 2 }\n            makeDefault \n        />\n```\nI also tweaked the inital camera position so that camera starts from the correct place. \nIt looks extremely weird, but I think it's closer to what I want:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/camera-movement-bug.mp4\">\u003C/video>\n\nThe problem is that my camera does not \"remember\" where we were before we stopped moving. I tried to fix that but I think I am missing something here.\nI compared states of camera while moving and after, but they look the same:\n\n![cameras comnparison](/shaders-research-blog/images/camera-state-comparison-1.png)\n![cameras comnparison](/shaders-research-blog/images/camera-state-comparison-2.png)\nI will think about it more later, for now I switched back to \"straight\" camera","src/content/blog/01-21-backrooms.mdx","81885dc104edffc3","01-22-wallpaper",{"id":149,"data":151,"body":156,"filePath":157,"digest":158,"deferredRender":19},{"title":152,"description":153,"pubDate":154,"heroImage":155},"Creating wallpapers with shaders"," ",["Date","2025-01-21T23:00:00.000Z"],"hero-22jan.png","import Takeaway from \"../../components/Takeaway.astro\";\n\nAt first, I made sure that shader works and passed UV coordinates through it:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-1.png)\n\nThen I mixed it with our walls color, again, just to check that it works:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-2.png)\n\nNice! now we can write shader code.\nTo create wallpaper, we will need to divide our walls into grid and draw stripes between cells.\nFlowers are also easy as it's just sinusoid shape.\nThis what I got:\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-3.png)\n\nAfter adding flowers, I wanted to create a pattern with a heart shape with flowers around it. \nSo we need to check what cell we're in and draw a shape depending on that:\n```javascript\n    // Place flowers every 2 cells\n    if (mod(cellId.x, 2.0) == 0.0 && mod(cellId.y, 2.0) == 0.0) {\n        vec2 center = (cellId + vec2(0.6)) / gridSize;\n        strength += flower(center, 0.0006, aspectCorrectedUV);\n    }\n\n    // Place shape every 4 cells with a 1 cell offset\n    if (mod(cellId.x, 4.0) == 1.0 && mod(cellId.y, 4.0) == 1.0) {\n        vec2 center = (cellId + vec2(0.6)) / gridSize;\n        strength += shape(center, 0.0006, aspectCorrectedUV);\n    }\n\n```\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-4.png)\n\nThen I wanted to add noise a little bit thinking that I only need to offset everything:\n```javascript\n    //add noise\n    float noise = cnoise(uv * 1000.0) * 0.25;\n    strength += noise;\n\n```\n\nAnd accidentally got this paper wallpaper texture instead!\n![wallpaper-shader](/shaders-research-blog/images/wallpaper-shader-5.png)\n\nthen I noticed that noise does not use my aspectRatio and id a bit too stretched, so I've put aspectRatio into nopise:\n\n```javascript\n    //add noise\n    float noise = cnoise(vec2(uv.x * 1000.0* aspectRatio, uv.y * 1000.0)) * 0.5;\n    strength += noise;\n\n```\nbut the perfect noise did not look like the paper anymore....\nSo I've saved the stretching factor a little bit:\n```javascript\n   float noise = cnoise(vec2(uv.x * 1000.0* aspectRatio * 0.25, uv.y * 1000.0)) * 0.3; \n```\nNow it's nice! At this point Ella also told me to make the heart \"normal\" because it is not mysterious at all but just looks weird...\n\nSo I flipped it back, and it's time to animate our flowers. I've played with tweaking, put time Uniform into sin and also into noise for the flower size and this is how it's gonna look in 10x speed:\n\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-6.mp4\">\u003C/video>\n\nNice! but I want it to be extremely subtle, nearly not niticeable at all, just to create the feeling that something is off:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-7.mp4\">\u003C/video>\nThis\n\nEarlier today I was looking for the floor carpet textures, but now I thought why do I need textures when I can code *all the textures* myself?? *evil laugh*\n\nWe already have the grid logic, now we need stripes for diffuse map **and emissive swuares for the light**.\nIt means we can use my favorite Bloom postprocessing effect! \nTo make it even more mysterious, I added flickering to the squares.\nTo randomize flickering I used their indexes inside of flickering itself and also to determine which square should flicker at what time:\n\n```javascript\n // Place light every 4 cells\n    if (mod(cellId.x, 4.0) == 0.0 && mod(cellId.y, 4.0) == 0.0) {\n        //minus bars width\n        if (cell.x > barWidth && cell.y > barWidth) {\n            //flcker every n seconds:\n            float n = (cellId.x + cellId.y);\n            float time = mod(uTime, n);\n            if (time > (n - 1.0)) {\n                flicker = sin(uTime * 5.0 * (cellId.x + cellId.y)); //from -1 to 1\n                flicker = abs(flicker);\n            }\n            emmissiveStrength = vec3(5.0, 5.0, 8.0) * flicker;\n        }\n    }\n```\nWoow now it looks really nice!😍😍😍\nYou also can notice how flowers are changing on the wallpaper here if you look closely:\n\u003Cvideo controls muted autopolay loop width=\"800\" src=\"/shaders-research-blog/images/wallpaper-shader-8.mp4\">\u003C/video>\nAfter that i changed my timer float n to multiply the indexes:\n```javascript\n       float n = (cellId.x * cellId.y);\n```\nso that the light flickers less often and more random.\n\nAt the end I also added noise to Diffuse and to Bump texture of the ceiling:\n![wallpaper-shader](/shaders-research-blog/images/hero-22jan.png)\nI am not sure if I remember correctly how bump texture works, I will look into it tomorrow, together with camera movement after Wouter's consult 👀","src/content/blog/01-22-wallpaper.mdx","ba0877214142aa28"]